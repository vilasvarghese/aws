https://docs.ogle.com/spreadsheets/d/1UCocQ2Bq98HFT4zHRdf42J_hMeb0eJX58mQxSOttZRM/edit#gid=1450392222f

Good references: 
	https://github.com/open-guides/og-aws (Not managed over time)
	https://github.com/mikeroyal/AWS-Guide (maintained)
	https://www.youtube.com/@SrceCde	(Some of the how to?)
	https://github.com/ronaldbradford/aws-tutorial

Loadbalancer

Suraj's note 
https://docs.google.com/spreadsheets/d/1lzdgbGHAX6H-C4vLVGr3oy8pmw5sSzsacbBtG-W8mNA/edit#gid=0


[09:14] Su (Guest)
B-Tech ECE-2023,Karunya University
[09:15] Rohindh Kishore (Guest)
Electronics & Communication Kgisl 2023
[09:15] aditya trivedi  (Guest)
computer science eng, from PSIT Kanpur(U.P), 2023 Passout
[09:15] sandeepkumar.venkatesan@verticurl.com
B.Tech -- IT - Hindusthan college of engineering and technology- 2023
[09:15] Vanshgarg.Vikas@verticurl.com
Information Technology
Maharaja Agrasen Institute Of Technology (Delhi)
2022
[09:15] Mayur R S (Guest)
M.Sc CS 
Hindustan College of Arts & Science , Coimbatore
2023
[09:15] Jila Rasnat B (Guest)
BE CSE
KGiSL Institute of Technology
2023
[09:15] SINDHU (Guest)
MSc Software Systems- kgisl (20230

[09:15] Ayush singhal  (Guest)

B-Tech CsE,psit kanpur 2023

[09:15] SINDHU (Guest)

2023

[09:15] Suraj Karthic (Guest)

B.E CSE - KGISL 2023

[09:16] Stephen Adithya (Guest)

MCA , Hindusthan college- 2023



Breaks:
	1:00 pm to 2:00 pm (Lunch)
	4:00 pm to 4:20 pm (20 min)
	
1. Hypervisor 
2. Virtualization
3. Infastructure as a Service 
	e.g. storage, compute etc., network 
	
4. Platform as a service 
	e.g. Beanstalk 
5. Software as a service 
	ogle drive, photos, zoom 

6. Function as a service
	Lambda, micro services , 
7. Containers as a service 


1. EC2
2. s3 for storage 
3. lambda for functions 
4. 


Deployment models
-----------------
Public 
	accessible for everyone in the internet 
		based on whom you gave access
Private
	belogs to org.
	only the org. can access
	Limited access
	
Hybrid 
	



3 networks 
	192.168.0.0/24		192.168.0.0 - 192.168.0.255
	192.168.0.0/16		192.168.0.0 - 192.168.255.255
	192.0.0.0/8			192.0.0.0 - 192.255.255.255
	0.0.0.0/0			0.0.0.0 - 255.255.255.255
	
	192.168.0.0/23		192.168.0.0	- 192.168.1.255	
	
	192.168.0.0/20		192.168.0.0	- 192.168.15.255
	
	
192.168.0.0/8	- Type A
192.168.0.0/16	- Type B
192.168.0.0/24	- Type C

192.168.0.0/28

	
	
	
------------------------------------------------------------------------------------------------------------------	
Network 	1		2		4		8		16		32		64		128		256
Host		256		128		64		32		16		8		4		2		1
Subnet 		/24		/25		/26		/27		/28		/29		/30		/31		/32
------------------------------------------------------------------------------------------------------------------
4096/256	= 16
256 * 256

/23	512
/22	1024
/21	2048
/20	4096
/19	8192
/18	16384
/17	32768
/16	65536
		16777216	
			
		4294967296	
			
0 - 255.0 - 255.0-255- 0- 255
0000 0000

000
001
010
011
100
101
110
111

VPC	- Virtual private cloud
VNet	Virtual network 

													IP (Internet protocol)
													-----------------------------------------
192.168.0.0/24													
													Network ID 			Broadcast ID
Engineering		192.168.0.1		- 192.168.0.62		192.168.0.0			192.168.0.63	
HR				192.168.0.65	- 192.168.0.126     192.168.0.64	    192.168.0.127
Reception		192.168.0.129	- 192.168.0.190     192.168.0.128       192.168.0.191
Vilas			192.168.0.193 	- 192.168.0.254     192.168.0.192       192.168.0.255




IaC (Infrastructure as Code)
	Infrastructure provisioning tools
		Terraform
		CloudFormation
		Azure ARM Templates 
	Configuration management tools 
		Pull 
			Puppet
			Chef
		Push 
			Ansible 
				ssh and scp - defult in linux 






EBS
	Instance type 
	EBS 
	
		Bootable 
		Attach volume to instance 
	Instance 	
		Instance fails 
			data lost 
			temporary data 
			store persistant data 

Storage types 
	Block Store 
		aws: Elastic block store 
		azure: azure disk 
		typically one machine connects/edits 
			other machines if allowed to connect to connect would read.
		Bootable 
			install o/s 

		
	File store 
		aws: Elastic file system
		azure: azure files
		allow multiple machines to connect 
			can edit 
		like network storage 	
		not bootable 
		
	







Week 1		Foundations of Cloud Computing & Basic Operations
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	1	Introduction to Cloud Computing (refresher from previous intro and more hands on)
-------------------------------------------------------------------------------------------------------------------------


Cloud computing is a transformative technology that has revolutionized the way businesses and individuals access and manage computing resources. It involves the delivery of various computing services, including servers, storage, databases, networking, software, analytics, and intelligence, over the internet. Instead of owning and maintaining physical hardware and software, users can rent or lease these resources from cloud service providers. This model offers numerous advantages, making it a fundamental part of modern IT infrastructure.



1. What is Cloud Computing?
Cloud computing refers to the delivery of computing services, including servers, storage, databases, networking, software, and more, over the internet. These services are hosted and managed by cloud service providers, and users access them remotely, typically on a pay-as-you- basis. Cloud computing offers several advantages, such as scalability, cost-efficiency, and flexibility.

2. Key Components of Cloud Computing:

    Hypervisor: A hypervisor is a crucial component in cloud computing that allows multiple virtual machines (VMs) to run on a single physical server. It acts as a layer of virtualization between the hardware and the VMs, enabling efficient resource allocation and isolation.

    Virtualization: Virtualization technology abstracts physical resources (such as CPU, memory, and storage) and creates virtualized instances of these resources. This allows for the creation of multiple VMs on a single physical server, each running its own operating system and applications.

    Infrastructure as a Service (IaaS): IaaS provides virtualized computing resources over the internet. Users can rent virtual machines, storage, and networking infrastructure. Hypervisors play a critical role in managing these virtual resources.

    Platform as a Service (PaaS): PaaS offers a platform that includes development tools, databases, and middleware. Developers can build, deploy, and manage applications without worrying about the underlying infrastructure, as the cloud provider manages the hypervisors and hardware.

    Software as a Service (SaaS): SaaS delivers software applications over the internet. Users access these applications through web browsers, and all underlying infrastructure and hypervisor management are handled by the SaaS provider.

3. Types of Hypervisors:

    Type 1 Hypervisor (Bare-Metal Hypervisor): Type 1 hypervisors run directly on the physical hardware without the need for an underlying operating system. They are highly efficient and are typically used in enterprise environments and data centers. Examples include VMware vSphere/ESXi, Microsoft Hyper-V, and Xen.

    Type 2 Hypervisor (Hosted Hypervisor): Type 2 hypervisors run on top of an existing operating system. They are often used for development and testing purposes or on personal computers. Examples include Oracle VirtualBox and VMware Workstation.

4. Benefits of Hypervisors in Cloud Computing:

    Resource Isolation: Hypervisors ensure that each virtual machine is isolated from others, preventing one VM from affecting the performance or stability of others on the same physical server.

    Resource Allocation: Hypervisors allocate CPU, memory, and storage resources to virtual machines as needed, allowing for efficient utilization of hardware.

    Flexibility: Virtual machines can be easily created, cloned, and moved between physical servers, providing flexibility in managing workloads.

    Cost Savings: Hypervisors enable server consolidation, reducing the need for multiple physical servers, which can lead to significant cost savings in terms of hardware and maintenance.

    High Availability: Hypervisors often include features for load balancing and failover, ensuring that workloads remain available even in the event of hardware failures.

In summary, cloud computing relies on hypervisors, which are essential for virtualization and efficient resource management in the cloud. They enable the creation and management of virtual machines, making it possible to deliver cloud services such as IaaS, PaaS, and SaaS. Understanding hypervisors is fundamental to comprehending the foundational technology behind cloud computing.


Here are key components and concepts of cloud computing:



    Common Cloud Computing Use Cases:

        Web Hosting: Hosting websites and web applications.

        Data Backup and Recovery: Storing and protecting data in case of disasters.

        Big Data and Analytics: Processing and analyzing large datasets.

        IoT (Internet of Things): Collecting and processing data from IoT devices.

        Machine Learning and AI: Training and deploying machine learning models.

        Content Delivery: Distributing content globally to users with low latency.

Cloud computing has transformed the IT landscape, enabling organizations to innovate faster, reduce costs, and focus on their core competencies. As technology continues to evolve, cloud computing is likely to play an even more central role in the digital transformation of businesses and the delivery of services to individuals and communities.


-------------------------------------------------------------------------------------------------------------------------
				
		Overview of cloud computing: Benefits, challenges, and service models.
-------------------------------------------------------------------------------------------------------------------------
Benefits
https://cloud.ogle.com/learn/advantages-of-cloud-computing

    Benefits of Cloud Computing:

        Scalability: Cloud resources can be easily scaled up or down to meet changing demands, ensuring efficient resource utilization.

        Cost-Efficiency: Users pay only for the resources they consume, eliminating the need for large upfront investments in hardware and reducing operational costs.

        Flexibility and Agility: Cloud services can be provisioned and deployed rapidly, enabling organizations to respond quickly to market changes and opportunities.

        Reliability and Availability: Leading cloud providers offer high levels of service uptime and redundancy, reducing the risk of downtime.

        Security: Cloud providers invest in robust security measures and compliance certifications to protect data and infrastructure.


challenges
	
While cloud computing offers numerous benefits, it also presents several challenges that organizations need to consider when adopting and managing cloud services. Here are some of the key challenges of cloud computing:

    Security and Privacy:
        Data Security: Storing sensitive data in the cloud raises concerns about unauthorized access or data breaches. Ensuring the security of data at rest and in transit is a top priority.
        Compliance: Meeting regulatory requirements, such as GDPR or HIPAA, can be challenging when data is stored in the cloud. Cloud providers often offer compliance certifications, but organizations must also take steps to ensure their own compliance.

    Data Management:
        Data vernance: Managing data across multiple cloud services and on-premises systems can be complex. Organizations need effective data vernance strategies to maintain data quality, consistency, and security.
        Data Transfer Costs: Moving large volumes of data in and out of the cloud can result in significant data transfer costs.

    Downtime and Reliability:
        Downtime: While cloud providers offer high availability, no service is immune to outages. Organizations need to plan for downtime and implement strategies for business continuity and disaster recovery.
        Dependence on Providers: Relying on third-party providers means that organizations have limited control over the underlying infrastructure. They must trust their providers' reliability and security measures.

    Cost Management:
        Uncontrolled Costs: Cloud resources are typically billed on a pay-as-you- basis, and costs can spiral out of control if not monitored carefully. Organizations need to implement cost management and optimization practices to control expenses.
        Hidden Costs: Some cloud services have hidden or unexpected costs, such as data transfer fees, support fees, and licensing costs for certain software.

    Vendor Lock-In:
        Proprietary Services: Using proprietary cloud services and APIs can lead to vendor lock-in, making it challenging to migrate to a different cloud provider or bring services back in-house.
        Interoperability: Ensuring interoperability between different cloud services and on-premises systems can be complex.

    Performance and Scalability:
        Resource Contention: In a multi-tenant environment, resource contention can occur, leading to performance bottlenecks. Organizations need to monitor and manage resource usage to maintain consistent performance.
        Scalability Challenges: Scaling applications and workloads in the cloud may require redesigning or refactoring applications to take full advantage of cloud resources.

    Compliance and Legal Issues:
        Data Jurisdiction: Determining where data is physically stored and which jurisdiction's laws apply can be complex in a global cloud environment.
        Legal and Contractual Issues: Understanding and netiating cloud service contracts, service level agreements (SLAs), and liability terms is critical to protecting the organization's interests.

    Skills and Training:
        Skills Gap: Managing cloud resources effectively requires a different skill set compared to traditional IT. Organizations may need to invest in training and hiring to bridge the skills gap.

    Data Loss and Recovery:
        Data Loss: While cloud providers offer data redundancy, data loss can still occur due to various factors, including human error. Organizations must have robust backup and recovery strategies.

    Network and Connectivity:
        Network Reliability: Cloud services depend on internet connectivity. Organizations may experience performance issues or downtime if their network connections are unreliable.
        Latency: Latency can be a concern for applications that require real-time responsiveness.

	Cost:
	
	
	Despite these challenges, many organizations find that the benefits of cloud computing outweigh the drawbacks. Effective planning, vernance, and risk management are essential to navigate these challenges successfully and harness the full potential of cloud technology.


service models

	    Cloud Service Models:

        IaaS Services: Virtual machines, storage, load balancers, and network resources.

        PaaS Services: Development platforms, databases, and application hosting environments.

        SaaS Services: Email, productivity software, customer relationship management (CRM), and other applications.
    Service Models:

        Infrastructure as a Service (IaaS): Provides virtualized computing resources over the internet. Users can rent virtual machines, storage, and networking infrastructure.

        Platform as a Service (PaaS): Offers a platform that includes development tools, databases, and middleware. Developers can build, deploy, and manage applications without worrying about the underlying infrastructure.

        Software as a Service (SaaS): Delivers software applications over the internet. Users access these applications through web browsers, eliminating the need for local installations.

	Infrastructure as a Service (IaaS)
	----------------------------------
Infrastructure as a Service (IaaS) 
	basic building blocks for cloud IT and 
	access to (virtual)
		networking features, 
		computers 
			(virtual or on dedicated hardware), and 
		data storage space. 
	
	IaaS provides 
		highest level of flexibility 
		management control on IT resources 
		most similar to existing IT resources 
			
e.g. 
	AWS EC2.
	Rackspace.
	ogle Compute Engine (GCE).
	Digital Ocean.
	Microsoft Azure.
	Magento 1 Enterprise Edition*.


	Platform as a Service (PaaS)
	----------------------------
Platform as a Service (PaaS) 
	manage the underlying infrastructure 
		(usually hardware and operating systems) and 
		we can focus on 
			deployment and 
			management 
				of your applications. 
			
		we don’t need to worry about 
			resource procurement, 
			capacity planning, 
			software maintenance, 
			patching, 
		or 
			any of the other undifferentiated heavy lifting involved in running your application.
e.g.
	AWS Elastic Beanstalk.
	Heroku.
	Windows Azure (mainly used as PaaS).
	Force.com.
	ogle App Engine.
	OpenShift.
	Apache Stratos.
	Adobe Magento Commerce Cloud.

	Software as a Service (SaaS)
	----------------------------
	Software as a Service (SaaS) 
		provides you with a completed product 
			run and 
			managed 
				by the service provider. 
	mostly end-user applications. 
	e.g.
		web-based email 
			send and receive email 



------------------------------

    Deployment Models:

        Public Cloud: 
			Services are provided by third-party cloud providers and are available to anyone over the internet. Examples include Amazon Web Services (AWS), Microsoft Azure, and ogle Cloud Platform (GCP).

        Private Cloud: 
			Resources are dedicated to a single organization and can be hosted on-premises or by a third-party provider. Private clouds offer greater control and security.

        Hybrid Cloud: 
			Combines both public and private cloud resources, allowing data and applications to move between them. This model provides flexibility and scalability.




Deployment Models 
The cloud deployment model 
	identifies the specific type of cloud environment 
	based on 
		ownership, 
		scale, and 
		access, as 
		cloud’s nature and purpose. 
		location of the servers 
		who controls them 
			defined by a cloud deployment model. It specifies how your cloud infrastructure will look, what you can change, and whether you will be given services or will have to create everything yourself. Relationships between the infrastructure and your users are also defined by cloud deployment types. 

Different types of cloud computing deployment models are:
	Public cloud 
	Private cloud
	Hybrid cloud
	Community cloud
	Multi-cloud 




Public Cloud 
	anybody can access services. 
	[arguably] may be less secure 
		as it is open to everyone. 
	cloud infrastructure services provided over the internet 
		to general people or 
		major industry groups. 
	
	infrastructure 
		owned by the cloud provider
		not by the consumer. 
		allows customers and users 
			to easily access systems and services. 
		
		storage backup and retrieval services are 
			given for free
			as a subscription, 
			or on a per-user basis. 
			Example: ogle App Engine etc.

	Advantages of Public Cloud Model:

		Minimal Investment: 
		No setup cost: 
		No Infrastructure Management 
		No maintenance: 
		Dynamic Scalability:  

	Disadvantages of Public Cloud Model:
		Arguably Less secure: 
			Public cloud is less secure as resources are public so there is no guarantee of high-level security.
		Low customization: 
			It is accessed by many public so it can’t be customized according to personal requirements. 


Private Cloud 

	exact opposite of the public cloud deployment model. 
	one-on-one environment 
		for a single user (customer). 
	don't share your hardware 
	access systems and services 
		within a given border or organization. 
	The cloud platform 
		implemented in a cloud-based secure environment 
		protected by powerful firewalls and 
		under the supervision of an organization’s IT department. 
		greater flexibility of control over cloud resources.

	Advantages of Private Cloud Model:
		Better Control: 
			sole owner of the property. 
			complete command over 
				service integration, 
				IT operations, 
				policies, and 
				user behavior. 
		Data Security and Privacy: 
			suitable for storing critical corporate information 
				only authorized staff have access. 
			segment resources 
				improved access and 
				security can be achieved.
		Supports Legacy Systems: 
			designed to work with legacy systems 
				that are unable to access the public cloud. 
		Customization: 
			Unlike a public cloud deployment, 
			private cloud allows a company to tailor its solution to meet its specific needs.
	Disadvantages of Private Cloud Model:
		Less scalable: 
			scaled in a range as 
				less number of clients.
		Costly: 
			costly 
			as you provide personalized facilities.
 
Hybrid Cloud 
	bridge 
		public and private cloud 
	gives the best of both worlds. 
	host the app in a safe environment 
		save cost using public cloud’s . 
	move data and applications 
		between different clouds 
			using a combination of 
				two or more cloud deployment methods.
	Advantages of  Hybrid Cloud Model:
		Flexibility and control: 
			Businesses with more flexibility 
			can design personalized solutions 
				that meet their particular needs.
		Cost: 
			public clouds provide scalability
		Security: 
			data is properly separated
			chances of data theft 
				reduced. 
	Disadvantages of Hybrid Cloud Model:
		Difficult to manage: 
			combination of both 
			So, it is complex.
		Slow data transmission: 
			Data transmission through the public cloud 
			so latency occurs.
		Cost: 	
			pay for extra capacity 
			and continous data transfer

Community Cloud
	allows systems and services 
		to be accessible by a 
			group of organizations. 
	distributed system 
		created by integrating the services of 
			different clouds 
				to address the specific needs of a 
					community, 
					industry, or 
					business. 
	infrastructure of the community 
		could be shared between 
			organization which has shared concerns or tasks. 
	generally managed by 
		third party 
	or 
		combination of multiple organizations in the community. 

	Advantages of Community Cloud Model:
		Cost Effective: 
			shared by multiple organizations or communities.
		Security: 
			Community cloud provides better security.
		Shared resources: 
			share 
				resources, 
				infrastructure, 
				etc. with 
					multiple organizations.
		Collaboration and data sharing: 
			suitable for both collaboration and data sharing.
	Disadvantages of Community Cloud Model:
		Limited Scalability: 
			relatively less scalable 
				as many organizations share the same resources 
					according to their collaborative interests. 
		Rigid in customization: 
			everybody should agree
			
Multi-cloud 
	Use multiple cloud providers 
		at the same time 
	similar to hybrid cloud 
	public cloud providers 
		provide numerous tools 
			improve the reliability of their 
				services, 
				mishaps still occur. 
	Highly unlikely 
		two distinct clouds 
		would have an incident at the same moment. 
		improves the high availability of your services even more. 

	Advantages of a Multi-Cloud Model:
		mix and match 
			the best features  
		Reduced Latency: 
			To reduce latency 
				improve user experience
				choose cloud regions and zones 
					that are close to your clients. 
		High availability of service: 
			two distinct clouds 
				would not have an incident 
					at the same moment. 
	Disadvantages of Multi-Cloud Model:
		Complex: 
			
		Security issue: 
			complex structure
				there may be loopholes 
			hacker may take advantage 

-------------------------------------------------------------------------------------------------------------------------
		
		Introduction to AWS and Azure: Key services and differences.
-------------------------------------------------------------------------------------------------------------------------
Amazon Web Services (AWS) and Microsoft Azure are two of the most prominent and widely used cloud computing platforms in the world. They offer a vast array of cloud services and solutions to individuals, businesses, and organizations. Let's provide a brief introduction to each of them:

Amazon Web Services (AWS):

    Overview: AWS is a comprehensive cloud computing platform offered by Amazon.com. It was launched in 2006 and has since become one of the leading cloud service providers globally.

    Services: AWS offers a wide range of cloud services organized into several cateries, including:
        Compute: 
			Services for virtual servers (EC2), containers (ECS and EKS), and serverless computing (AWS Lambda).
        Storage: 
			Scalable and durable storage solutions like Amazon S3 and EBS.
        Databases: 
			Managed database services, including Amazon RDS and DynamoDB.
        Networking: 
			Virtual private cloud (VPC), content delivery (Amazon CloudFront), and load balancing.
        Analytics: 	
			Big data processing and analytics tools such as Amazon Redshift and EMR.
        Machine Learning: 
			Services like Amazon SageMaker for building and deploying machine learning models.
        Security: 
			Identity and access management (IAM) and various security and compliance tools.
        IoT: 
			Services to build, deploy, and manage IoT applications.
        DevOps: 
			Tools for automating application deployment and management.

    Global Reach: AWS has a vast global network of data centers and regions, making it accessible to customers all over the world. Customers can choose from multiple availability zones within each region for redundancy and fault tolerance.

    Pricing: AWS employs a pay-as-you- pricing model, where users are billed for the resources they consume, making it cost-effective and scalable for a wide range of users.

    Market Dominance: AWS is known for its extensive market share and customer base, ranging from startups to large enterprises. It has a rich ecosystem of third-party tools and partners.

Microsoft Azure:

    Overview: Microsoft Azure is a cloud computing platform and service offered by Microsoft. It was initially launched in 2010 and has since grown into one of the largest cloud providers in the world.

    Services: Azure provides a comprehensive set of cloud services, including:
        Compute: 
			Virtual machines (Azure VMs), containers (Azure Kubernetes Service), and serverless computing (Azure Functions).
        Storage: 
			Scalable object storage (Azure Blob Storage) and file storage (Azure Files).
        Databases: 
			Managed database services like Azure SQL Database and Cosmos DB.
        Networking: 
			Virtual networks (Azure VNet), content delivery (Azure Content Delivery Network), and load balancing.
        Analytics: 
			Data analytics and big data solutions, including Azure Data Lake and HDInsight.
        AI and Machine Learning: 
			Azure Machine Learning and Cognitive Services.
        Security and Identity: 
			Azure Active Directory (AD) and security tools.
        IoT: 
			Azure IoT Hub for managing IoT devices and data.
        DevOps: 
			Azure DevOps for application lifecycle management.

    Global Presence: Azure has a vast network of data centers and regions globally, similar to AWS. Customers can choose from multiple availability zones for high availability.

    Hybrid Cloud Capabilities: Azure offers hybrid cloud solutions, allowing organizations to integrate on-premises infrastructure with cloud services seamlessly.

    Integration with Microsoft Products: Azure integrates well with Microsoft's extensive software ecosystem, making it a natural choice for organizations already using Microsoft technologies.

    Pricing: Azure follows a pay-as-you- pricing model, similar to AWS, allowing customers to pay for resources on-demand.

Both AWS and Azure are highly respected and offer robust, reliable, and secure cloud services. The choice between them often depends on an organization's specific needs, existing technology stack, and strategic als. Many businesses use a combination of both AWS and Azure, known as a multi-cloud strategy, to leverage the strengths of each platform.


Get started from here

		Key services
-------------------------------------------------------------------------------------------------------------------------
Amazon Web Services (AWS) offers a vast array of cloud services designed to meet various computing, storage, database, networking, analytics, machine learning, security, and management needs. Here are some of the key AWS services across different cateries:

1. Compute Services:

    Amazon Elastic Compute Cloud (EC2): 
		Provides resizable virtual servers (instances) for running applications.
    AWS Lambda: 
		Allows you to run code in response to events without provisioning or managing servers (serverless computing).

2. Storage Services:

    Amazon S3 (Simple Storage Service): 
		Scalable object storage for data storage, backup, and retrieval.
    Amazon EBS (Elastic Block Store): 
		Block-level storage volumes for use with EC2 instances.
    Amazon Glacier: 
		Low-cost storage for archiving and long-term backup.

3. Database Services:

    Amazon RDS (Relational Database Service): 
		Managed relational database service supporting multiple database engines like MySQL, PostgreSQL, and SQL Server.
    Amazon DynamoDB: 
		Fully managed NoSQL database service.
    Amazon Redshift: 
		Data warehousing service for analytics and reporting.

4. Networking Services:

    Amazon VPC (Virtual Private Cloud): 
		Provides isolated network resources for creating private networks within AWS.
    Amazon Route 53: 
		Scalable domain name system (DNS) web service.
    AWS Direct Connect: 
		Dedicated network connection between on-premises data centers and AWS.

5. Analytics Services:

    Amazon EMR (Elastic MapReduce): Managed big data platform for processing and analyzing vast datasets.
    Amazon Athena: Query service for analyzing data in Amazon S3 using SQL.
    Amazon Kinesis: Real-time data streaming and processing service.

6. Machine Learning and AI Services:

    Amazon SageMaker: Fully managed service for building, training, and deploying machine learning models.
    AWS Rekognition: Image and video analysis service for object and facial recognition.
    Amazon Comprehend: Natural language processing (NLP) service for sentiment analysis and text extraction.

7. Security and Identity Services:

    AWS IAM (Identity and Access Management): 
		Service for managing user access and permissions.
    Amazon Cognito: 
		Identity and user management for mobile and web applications.
    Amazon Inspector: 
		Automated security assessment service for resources.
	Amazon organization: 

8. Developer and DevOps Services:

    AWS CodeDeploy: 
		Automated deployment service for applications to EC2 instances.
    AWS CodePipeline: 
		Continuous integration and continuous delivery (CI/CD) service.
    AWS CloudFormation: 
		Infrastructure as code (IAC) service for defining and deploying AWS resources.

9. Internet of Things (IoT) Services:

    AWS IoT Core: 
		Managed cloud platform for connecting and managing IoT devices.
    Amazon FreeRTOS: 
		Real-time operating system for IoT devices.

10. Management and Monitoring Services:
- Amazon CloudWatch: 
	Monitoring and observability service for AWS resources.
- AWS CloudTrail: 
	Audit trail service for tracking API calls made on your account.

11. AWS ECS and EKS:
	Containerization and orchestration

These are just a few of the many AWS services available. AWS continues to expand and innovate its offerings, providing solutions for a wide range of use cases across industries, from startups to enterprises. Users can choose and combine these services to build scalable, reliable, and cost-effective cloud-based applications and solutions.


Microsoft Azure is a comprehensive cloud computing platform offering a wide range of services to meet various IT infrastructure, development, and data management needs. Here are some key Azure services caterized by their functionalities:

1. Compute Services:

    Azure Virtual Machines (VMs): 
		Provides scalable virtual machines running Windows or Linux.
    Azure Functions: 
		Serverless compute service for event-driven applications.
    Azure Kubernetes Service (AKS): 
		Managed Kubernetes container orchestration.

2. Storage Services:

    Azure Blob Storage: 
		Scalable object storage for unstructured data.
    Azure Table Storage: 
		NoSQL data storage for semi-structured data.
    Azure Files: 
		Managed file shares for cloud and on-premises deployments.

3. Database Services:

    Azure SQL Database: 
		Managed relational database service.
    Azure Cosmos DB: 
		Globally distributed NoSQL database.
    Azure Database for MySQL/PostgreSQL: Managed database services for these engines.

4. Networking Services:

    Azure Virtual Network (VNet): 
		Private network for Azure resources.
    Azure Load Balancer: 
		Load balancing service for high availability.
    Azure Application Gateway: 
		Web traffic load balancer with web application firewall (WAF).

5. Analytics Services:

    Azure Synapse Analytics (formerly SQL Data Warehouse): Data warehousing and analytics service.
    Azure Databricks: Apache Spark-based analytics platform.
    Azure Data Lake Storage: Scalable data lake for big data analytics.

6. Machine Learning and AI Services:

    Azure Machine Learning: Machine learning service for building, training, and deploying models.
    Azure Cognitive Services: Suite of AI services for computer vision, speech, language, and more.
    Azure Bot Service: Service for building intelligent, conversational bots.

7. Security and Identity Services:

    Azure Active Directory (AD)/ Azure Entra ID: 
		Identity and access management service.
    Azure Key Vault: 
		Securely manage keys, secrets, and certificates.
    Azure Security Center: 
		Threat protection and security management service.

8. Developer and DevOps Services:

    Azure DevOps Services: Tools for planning, developing, testing, and delivering software.
    Azure Functions: Serverless compute for event-driven applications.
    Azure Logic Apps: Workflow automation service.

9. Internet of Things (IoT) Services:

    Azure IoT Hub: Managed service for IoT device messaging and management.
    Azure IoT Central: SaaS solution for IoT application development.

10. Management and Monitoring Services:
- Azure Monitor: Comprehensive monitoring and analytics service.
- Azure Policy: Policy enforcement service to enforce organizational standards.
- Azure Resource Manager: Management layer for organizing and deploying Azure resources.

These services represent only a subset of Azure's extensive offerings. Azure continues to expand and innovate, offering solutions for various industries and use cases. Organizations can leverage these services to build, deploy, and manage applications and infrastructure in a flexible and scalable manner.




---------------------------------


Service #1. Amazon EC2 [Elastic Compute Cloud]
	AWS EC2 AWS Service Amazon EC2 
		virtual servers 
		computing infrastructure 
			with the best suitable 
				processors, 
				networking facilities, and 
				storage systems. 
		highly secure, 
		reliable, 
		performing computing infrastructure 
			meeting business demands. 
		access resources quickly and 
		dynamically scale capacities as per demands.

Service #2. Amazon S3
	Amazon S3 
		object storage AWS service
		highly scalable. 
		access any quantity of data from anywhere. 
		data is stored in ‘storage classes’ 
			to reduce costs 
			no extra investment and 
			manage it comfortably. 
		data is highly secured and 
		supports meeting audit and compliance requirements. 
		handle any volume of data 
			with Amazon S3’s 
			robust access controls, 
			replication tools, and 
			higher visibility. 
		maintain data version controls and 
		preventing accidental deletion.
	
	
Service #3. AWS Aurora
	AWS RDS AWS Service  
	MySQL and PostgreSQL compatible relational database 
		high performance. 
	five times faster than standard MySQL databases. 
	can automate crucial tasks 
		e.g. 
			hardware provisioning, 
			database setup and 
			backups, and 
			patching. 
	Amazon Aurora 
		distributed, 
		fault-tolerant, 
		self-healing 
			storage system 
	can scale automatically as per needs. 
	reduce costs significantly 
	enhance databases' 
		security, 
		availability, and 
		reliability.	
	
Service #4. Amazon DynamoDB
	DynamoDB AWS Service DynamoDB 
		fully managed and 
		serverless NoSQL database AWS service. 
		fast and flexible database system 
		low cost. 
		single-digit millisecond performance 
		with unlimited throughput and storage. 
		in-built tools to generate 
			actionable insights, 
			useful analytics, and 
			monitor traffic trends in applications.	
	
	
Service #5. Amazon RDS
	AWS RDS 
	managed Relational Database 
		for 
			MySQL, 
			PostgreSQL, 
			Oracle, 
			SQL Server, and 
			MariaDB. 
		allows the 
			setup, 
			operation, and 
			scale of a 
				relational database in the cloud quickly. 
		high performance 
			by automating the tasks such as 
				hardware provisioning, 
				database setup, 
				patching, and 
				backups. 
	don’t need to 
		install and 
		maintain 
			the database software. 
	can optimize costs 
	high availability, 
	security, and 
	compatibility for your resources.	
	
	
Service #6. Amazon Lambda
	AWS Lambda 
	serverless and event-driven computing AWS service. 
	run codes automatically 
	without worrying about servers and clusters. 
	codes can be uploaded directly 
		don't need to provision or manage infrastructure. 
	automatically accepts 'code execution requests' 
	pay the price only for the computed time
	effective cost-control.	
	

Service #7. Amazon VPC
	AWS VPC 
		Virtual Private Cloud
		
		controls the virtual networking environment
			resource placement, 
			connectivity, and 
			security. 
	build and manage compatible VPC networks 
		across cloud AWS resources and on-premise resources. 
	improves security 
		inbound and 
		outbound connections. 
	monitors VPC flow logs 
		delivered to Amazon S3 and Amazon Cloudwatch 
		gain visibility over network dependencies and traffic patterns. 
	Amazon VPC 
		detects anomalies in the 
			patterns, 
			prevents data leakage, and 
			troubleshoots network connectivity and 
			configuration issues.


Service #8. Amazon CloudFront
	Amazon CloudFront 
		delivers content globally, 
		high performance and security. 
		delivers data with high speed and low latency. 
		content is delivered to destinations successfully 
			with automated network mapping and 
			intelligent routing mechanisms. 
		Improved security of data 
			with traffic encryption 
			access controls. 
	data can be transferred within milliseconds 
		with its in-built 
			data compression, 
			edge computing capabilities, and 
			field-level encryption. 
	stream high-quality video using AWS media services 
		to any device 
			quickly and consistently using Amazon CloudFront.

Service #9. AWS Elastic Beanstalk
	Supports running and managing web applications. 
		on an aws service
	we deploy code
		web app infrastructure managed by aws
	allows easy deployment of applications from 
		capacity provisioning, 
		load balancing
		auto-scaling 
		application health monitoring. 
		
	auto-scaling properties
	manage peaks in workloads and traffic 
		with minimum costs. 
	developer-friendly tool 
		manages 
			servers, 
			load balancers, 
			firewalls, and 
			networks simply. 
			

Service #10. Amazon EC2 Auto-scaling
	scales computing capacity 
		add or remove EC2 instances automatically. 
	two types of scaling 
		dynamic scaling and 
			responds to the presently changing demands
		predictive scaling. 
			responds based on predictions. 
			e.g. increase based on time etc.
	Through Amazon EC2 Auto-scaling
		identify the unhealthy EC2 instances, 
		terminate them
		replace them with new instances.


Service #11. Amazon ElastiCache
	 fully-managed, flexible, in-memory caching AWS service. 
	 supports increasing the performance of your applications and database. 
	 reduce the load in a database 
		by caching data in memory. 
	in-memory with 
	high speed, 
	microsecond latency
	high throughput. 
	reduce costs and 
	eliminate the operational overhead of your business.

Service #12. Amazon S3 Glacier
	archive storage in the cloud 
		at a low cost. 
	built with three storage classes 
		S3 Glacier instant retrieval, 
			supports immediate access 
		flexible retrieval, and 
		deep archive. 
	to data, and the flexible class allows flexible access within minutes to hours with no cost. The third one, deep archive, helps archive compliance data and digital media. Overall, they support you to access data from archives faster.

Service #13. Amazon Lightsail

	website and applications building AWS service. 
	offers 
		Virtual Private Server instances, 
		containers, 
		databases, and 
		storage. 
	serverless computing service with AWS Lambda. 
	Can create websites 
		using pre-configured applications such as 
			WordPress, 
			Magento, 
			Prestashop, 
			Joomla in a few clicks and at a low cost. 
	In addition to this, it is the best tool for testing, so you can create, test, and delete sandboxes with your new ideas.

Service #14. Amazon Sagemaker
	allows building, training, and deploying Machine Learning (ML) models at a large capacity. 
	It is an analytical tool that functions based on Machine Learning power to analyze data more efficiently. With its single tool-set, you can build high-quality ML models quickly. Amazon Sagemaker not only generates reports but provides the purpose for generating predictions too. In addition, Amazon Ground Truth Plus creates datasets without labeling applications.

Service #15. Amazon SNS
	Amazon Simple Notification Service (SNS). 
	messaging service 
		between Application to Application (A2P) and 
		Application to Person (A2Person). 
	A2P 
		many-to-many messaging 
			between 
				distributed systems, 
				microservices, and 
				event-driven serverless applications. 
		supports applications to send messages to many users via 
			mail, 
			SMS, etc. 
	For instance
		can send up to ten messages in a single API request. 
	effective filtering systems
		subscribers will receive messages that they are interested in. 
	Besides, Amazon SNS works alongside Amazon SQS to deliver messages accurately and consistently.

Service #16. Amazon EBS
	AWS EBS AWS Service Amazon Elastic Block Store (EBS) 
		block storage service. 
	It supports scaling high-performance workloads such as SAP, Oracle, and Microsoft products. And it provides better protection against failures up to 99.999%. It helps to resize clusters for big data analytics engines such as Hadoop and Spark. Also, you can build storage volumes, optimize storage performance, and reduce costs. Amazon EBS’s lifecycle management creates policies that help create and manage backups effectively.

Service #17. Amazon Kinesis
	AWS Kinesis AWS Service  It is the AWS service that analyses the video as well as data streams. Amazon Kinesis collects, processes, and analyzes all types of streaming data. Here, the data may be audio, video, application logs, website clickstreams, and IoT telemetry. Then, it generates real-time insights within seconds once the data has arrived. With the help of Amazon Kinesis, you could stream and process a large quantity of real-time data with low latencies, very simply.

Service #18. Amazon Elastic File System (EFS) 
	Amazon Elastic File System AWS Service  Amazon EFS is the fully managed file system for Amazon EC2. And it is a simple and serverless elastic file system. You can create and configure file systems without provisioning, deploying, patching, and maintenance using Amazon EFS. Here, files can be added and deleted as per the scaling needs. Especially, you can pay only for the used space, hence this service helps to reduce costs.

Service #19. AWS IAM
	AWS IAM AWS Service  It is the Identity and Access Management (IAM) service offered by AWS to securely access the applications and resources. It regulates access to various resources based on roles and access policies; as a result, you can achieve a fine-grained access control on your resources. The AWS IAM access analyzer helps streamline permission management through setting, verifying, and refining. In addition, AWS IAM attribute-based access control helps create fine-grained permissions based on user attributes such as department, job role, team name, etc.

Service #20. Amazon SQS
	AWS SQS AWS Service  Amazon SQS is a fully-managed message queuing service. There are two types of message queuing services: SQS Standard and SQS FIFO. Here, the SQS standard offers the features such as maximum throughput, best-effort ordering, and quick delivery. And SQS FIFO processes messages only once in the same order by which they have been sent. Also, Amazon SQS allows decoupling or scaling microservices, distributed systems, and serverless applications. It helps you send, receive and manage messages in a large volume. Moreover, there is no need to install and maintain other messaging software, reducing costs significantly. Besides, scaling is carried out quickly and automatically in this service.

Service #21. Amazon RedShift
	AWS Redshift AWS Service Amazon Redshift is a quick, simple, and cost-effective data warehousing service. You can gain insights about cloud data warehousing in an easy, faster, and more secure way. It allows analysis of all the data in operational databases, data lakes, data warehouses, and third-party data. And Amazon Redshift helps analyze a large volume of data and run complex analytical queries. With its automation capabilities, this service increases query speed and provides the best price performance.

Service #22. Amazon Cloudwatch
	Amazon Cloudwatch AWS Service  This AWS service monitors the cloud resources and applications keenly. It is a single platform that helps to monitor all AWS resources and applications; it increases visibility to respond to issues quickly. Mainly, Amazon Cloudwatch provides actionable insights to optimize monitoring applications, systemwide performance changes, and resource utilization. And you can get a complete view of the health of AWS resources, applications, and services running on AWS and on-premises. In addition, Amazon Cloudwatch helps to detect anomalies in the behavior of the cloud environment, set alarms, visualize logs and metrics, make automated actions, troubleshoot issues, and discover insights.

Service #23. Amazon Chime
	Amazon Chime AWS Service Amazon Chime is a communication service. It is a single solution that offers audio calling, video calling, and screen sharing capabilities. With the help of this service, you can make quality meetings, chat, and video calls both inside and outside of your organization. And more features can be added to this service as per your business needs. Mainly, you can set calls for a pre-defined time to automatically make calls on time. Amazon Chime helps you not to miss a meeting amidst your hectic schedule at work. Besides, you can pay as per the usage of resources by which you can reduce the costs significantly.

Service #24. Amazon Cognito
	amazon cognito aws service It is the identity management AWS service. Amazon Cognito manages identities for accessing your applications and resources. Mainly, this service helps add sign-in, sign-up, and access control the web and mobile apps quickly. It can support millions of users to sign in with familiar applications such as Apple, Facebook, ogle, and Amazon. In Amazon Cognito, the feature ‘Cognito user pools’ can be set up quickly without any infrastructure, and the pool members will have a directory profile. It supports multi-factor authentication and encryption of data-at-rest and data-in-transit.

Service #25. Amazon Inspector 
	Amazon Inspector AWS Service Amazon Inspector is an automated vulnerability management service. This service offers continuous and automated vulnerability management for Amazon EC2 and Amazon ECR. It allows scanning AWS workloads for software vulnerabilities and unwanted network exposure. Amazon Inspector quickly identifies vulnerabilities, which helps to take immediate actions to resolve them before it worsens the applications. Moreover, it supports meeting compliance requirements and reduces meantime-to-remediate vulnerabilities. And it provides you with accurate risk scores and streamlined workflow.

Service #26. AWS Firewall Manager
	AWS Firewall Manager AWS Service It is the central management service of firewall rules. The firewall manager supports managing firewall rules across all the applications and accounts. The common security rules help to manage new applications included over time. It is the one-time solution for consistently creating firewall rules and security policies and implementing them across the infrastructure. AWS firewall manager helps you audit VPC security groups for compliance requirements and control network traffic effectively.

Service #27. Amazon Appflow
	Amazon Appflow AWS Service Amazon Appflow is a no-code service that allows the integration of SaaS applications and AWS services effortlessly. To be more precise, it securely automates dataflows integrating third-party applications and AWS services without using codes. You can transfer data between SaaS applications such as Salesforce, SAP, Zendesk, etc. since Amazon Appflow can be integrated with other applications in a few clicks. Especially, a large volume of data can be moved without breaking it up into batches using this service.

Service #28. Amazon Route 53
	Amazon Route53 AWS Service It is a scalable cloud Domain Name System (DNS) service. It allows end-users to connect with Amazon EC2, Elastic load balancers, Amazon S3 buckets, and even outside AWS. In this service, the feature ‘Route 53 application recovery controllers’ configure DNS health checks and helps to monitor the ability of systems to recover from failures. And ‘Route 53 traffic flow’ helps manage traffic across the globe using routing methods such as latency-based routing, Geo DNS, Geoproximity, and weighted round-robin.

Service #29. AWS Cloud Formation
	Amazon Inspector AWS ServiceThis AWS service creates and manages resources with templates. It is a single platform that can handle all AWS accounts across the globe. It automates resource management with AWS service integration and offers turnkey application distribution and vernance controls. Also, AWS Cloud Formation can automate, test, and deploy infrastructure with continuous integration and delivery. And you can run applications right from AWS EC2 to complex multi-region applications using this service.

Service #30. AWS Key Management Service (KMS)
	AWS KMS AWS Service AWS KMS manages the creation and control of encryption keys. It means that AWS KMS creates cryptographic keys and controls their uses across various applications. You can achieve a secure and resilient service using hardware resilient modules to protect keys. This service can be integrated with AWS Cloudtrail to provide logs of all key usage to precisely fulfil compliance and regulatory requirements.


		differences
-------------------------------------------------------------------------------------------------------------------------
https://www.geeksforgeeks.org/comparisons-between-azure-vs-aws/

Compare azure vm vs aws ec2
https://www.linkedin.com/pulse/one-comparison-aws-ec2-vs-azure-vm-continuuminnovations/

Azure Virtual Machines (VMs) and Amazon Elastic Compute Cloud (EC2) are both cloud infrastructure services that offer virtualized computing resources, but they differ in their technical details and internal implementations. Here are some technical differentiators:

1. Hypervisor Technology:

    Azure Virtual Machines: Azure primarily uses Microsoft's Hyper-V as its hypervisor technology. Hyper-V is a type-1 hypervisor, which means it runs directly on the hardware without requiring an underlying operating system. It provides efficient resource isolation and management for VMs.

    Amazon EC2: EC2 instances run on a hypervisor based on the Xen virtualization platform. Xen is also a type-1 hypervisor known for its performance and security. AWS has customized Xen to optimize it for its cloud environment.

2. Virtual Machine Types and Sizes:

    Azure Virtual Machines: Azure offers a variety of VM types optimized for different workloads, including General Purpose, Compute Optimized, Memory Optimized, and more. Each type has multiple sizes with varying CPU, memory, and storage configurations.

    Amazon EC2: EC2 provides a wide range of instance types optimized for specific use cases, such as Compute Optimized, Memory Optimized, Storage Optimized, and GPU instances. Each instance type is designed with specific hardware configurations to meet different workload requirements.

3. Availability Zones:

    Azure Virtual Machines: Azure Availability Zones provide high availability by distributing VM instances across physically separate data centers within a region. This architecture enhances fault tolerance and redundancy.

    Amazon EC2: AWS offers a similar concept called Availability Zones, where instances can be placed in different data centers within a region. This provides resiliency and availability for EC2 instances.

4. Operating Systems and Marketplace:

    Azure Virtual Machines: Azure provides a marketplace with a wide range of pre-configured virtual machine images for various operating systems, including Windows Server, Linux distributions, and third-party software. You can also bring your own custom images.

    Amazon EC2: EC2 offers a vast marketplace with a plethora of Amazon Machine Images (AMIs) for various operating systems and applications. This includes Amazon Linux, Windows, and a wide selection of third-party AMIs.

5. Integration with Ecosystem:

    Azure Virtual Machines: Azure VMs integrate seamlessly with other Azure services, such as Azure Blob Storage, Azure SQL Database, and Azure Active Directory. Azure Resource Manager (ARM) templates are used for deployment and management.

    Amazon EC2: EC2 instances are part of the broader AWS ecosystem, which includes services like Amazon S3, Amazon RDS, and AWS IAM. AWS CloudFormation is used for infrastructure as code (IAC) management.

6. Management Tools:

    Azure Virtual Machines: Managed using the Azure Portal, Azure Power, Azure CLI, and ARM templates. Azure Automation can also be used for scripting and automation.

    Amazon EC2: Managed through the AWS Management Console, AWS CLI, and AWS CloudFormation templates. AWS also provides third-party integrations and development kits.

While both Azure Virtual Machines and Amazon EC2 offer similar core capabilities, these technical differences and considerations may influence your choice depending on your specific use case, existing infrastructure, and familiarity with the respective platforms.


aws s3 vs azure block storage 
	https://learn.microsoft.com/en-us/azure/architecture/aws-professional/storage
	https://stonefly.com/blog/comparison-aws-s3-infrequent-access-azure-cool-blob-storage/
	
	
Amazon Simple Storage Service (S3) and Amazon Elastic Block Store (EBS) are both storage services provided by Amazon Web Services (AWS), but they serve different purposes and have distinct technical differences:

Amazon Simple Storage Service (S3):

    Object Storage: Amazon S3 is an object storage service designed for storing and retrieving large amounts of unstructured data, such as images, videos, documents, and backups.

    Scalability: S3 is highly scalable and can store an unlimited number of objects in various "buckets." Objects can range from a few bytes to several terabytes in size.

    Durability: S3 offers high durability, with data automatically replicated across multiple data centers within an AWS region. It provides 11 nines (99.999999999%) of data durability.

    Access Control: S3 allows fine-grained access control through access policies and bucket policies. You can define permissions at the bucket and object levels.

    Data Access: S3 data is accessed over HTTP/HTTPS using RESTful API calls, and it's suitable for use cases like web hosting, data lakes, and content distribution.

    Storage Classes: S3 offers different storage classes, such as Standard, Intelligent-Tiering, Glacier, and more, to optimize costs based on data access patterns.

    Lifecycle Policies: You can configure S3 to automatically transition objects between storage classes or delete them based on defined policies.

Amazon Elastic Block Store (EBS):

    Block Storage: 
		EBS provides block-level storage volumes that are attached to Amazon EC2 instances. It's designed for applications that require low-latency, high-performance storage.

    Volume Types: EBS offers several volume types optimized for different use cases, including 
		General Purpose (SSD), 
		Provisioned IOPS (SSD), 
		Throughput Optimized HDD, and 
		Cold HDD.

    Scalability: EBS volumes can be attached to EC2 instances and can range in size from a few gigabytes to multiple terabytes. You can resize volumes on the fly.

    Persistence: EBS volumes are persistent; data remains even if the associated EC2 instance is stopped or terminated.

    Snapshots: EBS allows you to create point-in-time snapshots of volumes, which can be used for 
		data backup, 
		disaster recovery, and 
		creating new volumes.

    Block-Level Access: 
		EBS provides block-level access,
		making it suitable for 
			traditional filesystems, 
			databases, and 
			applications that require direct control over the block storage.

    Latency: 
		EBS volumes have low-latency characteristics, making them well-suited for applications that require fast and consistent I/O performance.

    Encryption: EBS volumes can be encrypted at rest using AWS Key Management Service (KMS) for security.


	network drive you 
		can attach instances while they run
		like a “network USB stick”
		not a physical drive
	uses the network to communicate the instance
		bit of latency
	
	• They can only be mounted to one instance at a time 
	• They are bound to a specific availability zone
		Can be attached to an instance in the same az 
	• Free tier: 
		30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month

	
	• It can be detached from an EC2 instance and attached to another one quickly
	• It’s locked to an Availability Zone (AZ)
	• An EBS Volume in us-east-1a cannot be attached to us-east-1b
	• To move a volume across
		snapshot it
	• Have a provisioned capacity (size in GBs, and IOPS)
	• You get billed for all the provisioned capacity
	• You can increase the capacity of the drive over time


Primary technical difference between Amazon S3 and Amazon EBS 
	use case and access patterns. 
	S3 
		Object storage
		designed for scalable object storage 
			accessible over the internet
	EBS 
		block-level storage volumes 
		optimized for EC2 instances 
			direct control over individual blocks of data. 
	
	Delete on termination
		Delete when vm is deleted.

EBS Snapshots
	• Make a backup (snapshot) of your EBS volume at a point in time
	• Not necessary to detach volume to do snapshot
		but recommended
	• Can copy snapshots across AZ or Region

	EBS Snapshot Archive
		• Move a Snapshot to an ”archive tier” that is 75% cheaper
		• Takes within 24 to 72 hours for restoring the archive
	Recycle Bin for EBS Snapshots
		• Setup rules to retain deleted snapshots so you can recover them after an accidental deletion
		• Specify retention (from 1 day to 1 year)
	Fast Snapshot Restore (FSR)
		• Force full initialization of snapshot to have no latency on the first use ($$$)




VNET to VPC
https://devblogs.microsoft.com/premier-developer/differentiating-between-azure-virtual-network-vnet-and-aws-virtual-private-cloud-vpc/



EC2 Instance Store
	• EBS volumes are network drives with good but “limited” performance
	• If you need a high-performance hardware disk, use EC2 Instance Store
	• Better I/O performance
	• EC2 Instance Store lose their storage if they’re stopped (ephemeral)
	• Good for buffer / cache / scratch data / temporary content
	• Risk of data loss if hardware fails
	• Backups and Replication are your responsibility



EBS Volume Types
	• EBS Volumes come in 6 types
		• gp2 / gp3 (SSD): 
			General purpose SSD volume that balances price and performance for a wide variety of workloads
		• io1 / io2 (SSD): 
			Highest-performance SSD volume for 
				mission-critical low-latency or
				high-throughput workloads
		• st1 (HDD): 
			Low cost HDD volume 
			designed for frequently accessed, throughput intensive workloads
		• sc1 (HDD): 
			Lowest cost HDD volume designed for less frequently accessed workloads
		• EBS Volumes 	
			characterized in Size | Throughput | IOPS (I/O Ops Per Sec)
		• When in doubt always consult the AWS documentation – it’s good!
		• Only gp2/gp3 and io1/io2 can be used as boot volumes
			
			
	https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#solid-state-drives

	

Some EBS can be attached to multiple instances.
	io1/io2 may support this in certain regions.
	
	
	
	

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up AWS and Azure accounts, navigating dashboards.
-------------------------------------------------------------------------------------------------------------------------
Setting Up an AWS Account:

    Create an AWS Account:
        Visit the AWS website (https://aws.amazon.com/) and click on "Create an AWS Account."
        Follow the on-screen instructions to create your AWS account, providing necessary information and payment details.

    Access the AWS Management Console:
        Once your AWS account is created, you can access the AWS Management Console by signing in with your account credentials.

    Dashboard Navigation:
        The AWS Management Console is the central hub for managing AWS services. Here's how to navigate it:
            Services Menu: Click on "Services" in the top-left corner to access a dropdown menu of AWS services caterized by type.
            Search Bar: Use the search bar at the top to quickly find and access specific AWS services.
            Regions: In the top-right corner, select the AWS region you want to work in. Different regions offer different availability zones and services.
            Billing and Cost Management: You can access billing and cost management information from the "Billing & Cost Management" section.

Setting Up an Azure Account:

    Create an Azure Account:
         to the Azure website (https://azure.com/) and click on "Start free."
        Follow the instructions to create your Azure account. You'll need to provide payment information, but Azure often offers a free trial with credits.

    Access the Azure Portal:
        After creating your Azure account, you can access the Azure Portal by signing in with your credentials.

    Dashboard Navigation:
        The Azure Portal is the interface for managing Azure services. Here's how to navigate it:
            Azure Services: Click on "All services" on the left sidebar to access a list of all Azure services, organized by catery.
            Search Bar: Use the search bar at the top to quickly find and access specific Azure services.
            Resource Groups: Azure organizes resources into resource groups. You can manage resources within these groups.
            Cost Management + Billing: Access billing and cost management information from the "Cost Management + Billing" section.

Navigating the Dashboards:

Both AWS and Azure offer a wide range of services, and their dashboards are designed to help you manage and interact with these services efficiently. Here are some common tasks you can perform:

    Creating Resources: You can create virtual machines, databases, storage accounts, and more by navigating to the respective service in the dashboard and following the setup wizard.

    Monitoring and Management: The dashboards provide tools for monitoring the performance and health of your resources. You can set up alerts, view logs, and access performance metrics.

    Security and Identity: Configure access control, user permissions, and security settings to protect your resources.

    Billing and Cost Management: Monitor your usage and costs. Both AWS and Azure provide cost estimation tools and billing dashboards.

    Support and Documentation: Access support resources, documentation, and communities for assistance with using the services.

It's important to explore and familiarize yourself with the AWS and Azure dashboards as you start working with cloud services. Each cloud provider may have specific terminology and features, so taking time to learn the navigation is beneficial. Additionally, be mindful of any free tier options or trial credits to manage costs while learning and experimenting.


-------------------------------------------------------------------------------------------------------------------------
				
	2	Computing, Networking & Storage
-------------------------------------------------------------------------------------------------------------------------

Cloud computing 
	on-demand delivery of 
		compute power, 
		database, 
		storage, 
		applications, and 
		other IT resources 
	through a 
		cloud services platform 
	via 
		the internet 
	with 
		pay-as-you- pricing. 
	

	cloud services platform 
		provides rapid access 
			to flexible and low-cost IT resources. 
	don’t make large upfront investments 
		in hardware and 
		spend a lot of time on the heavy lifting of managing that hardware. 
	provision exactly the right type and size of computing resources 
		you need to power your newest bright idea 
		or operate your IT department. 
	access as many resources as you need, 
		almost instantly
		pay for what you use.


"Computing, Networking & Storage" are fundamental components of cloud computing and data center infrastructure. They encompass various services and technologies that enable organizations to build, manage, and scale their IT resources in a flexible and cost-effective manner. Here's an overview of these components:
Computing:

    Virtual Machines (VMs):
        Virtual machines are virtualized instances of physical servers. They allow you to run operating systems and applications in isolated environments, providing flexibility and scalability.
        Cloud providers like AWS, Azure, and ogle Cloud offer VM services where you can launch and manage virtual machines as needed.

    Containers:
        Containers are lightweight, portable, and consistent runtime environments for applications and their dependencies. Docker is a popular containerization technology.
        Container orchestration platforms like Kubernetes help manage and scale containerized applications.

    Serverless Computing:
        Serverless computing abstracts server management away from developers. It allows you to run code in response to events without provisioning or managing servers.
        AWS Lambda, Azure Functions, and ogle Cloud Functions are examples of serverless platforms.

Networking:

    Virtual Private Cloud (VPC) / Virtual Network (VNet):
        VPCs and VNets are private, isolated network environments within a cloud provider's infrastructure. They allow you to define your own network topology, IP addressing, and security rules.
        VPCs and VNets provide the foundation for building secure and isolated cloud-based networks.

    Load Balancers:
        Load balancers distribute incoming network traffic across multiple servers or instances to ensure high availability, scalability, and reliability.
        Cloud providers offer load balancer services like AWS Elastic Load Balancing and Azure Load Balancer.

    Content Delivery Network (CDN):
        CDNs cache and deliver content (e.g., web pages, images, videos) from edge locations closer to end-users. This reduces latency and improves performance.
        Services like AWS CloudFront and Azure Content Delivery Network offer CDN capabilities.

    Firewalls and Security Groups:
        Firewalls and security groups are used to control inbound and outbound traffic to resources within a VPC or VNet. They help enforce network security policies.
        AWS Security Groups and Azure Network Security Groups are examples of such services.

Storage:

    Object Storage:
        Object storage is used to store and manage unstructured data, such as files, images, and videos. Data is stored in containers called "buckets" (AWS S3, Azure Blob Storage).

    Block Storage:
        Block storage provides raw storage volumes that can be attached to virtual machines. It's used for file systems, databases, and applications (AWS EBS, Azure Disk Storage).

    File Storage:
        File storage systems allow multiple virtual machines to access shared file systems simultaneously. It's useful for data sharing among instances (AWS EFS, Azure File Storage).

    Database Storage:
        Cloud providers offer managed database services with scalable and highly available storage (AWS RDS, Azure SQL Database).

    Backup and Data Recovery:
        Services for data backup, snapshotting, and disaster recovery are essential for data protection and business continuity.

    Storage Tiers and Lifecycle Policies:
        Cloud providers offer storage tiers with different performance and cost characteristics. Lifecycle policies help automate data movement between tiers.

These components form the backbone of modern cloud computing and data center infrastructure. Organizations leverage these services and technologies to build scalable, reliable, and secure IT solutions that meet their specific needs. The choice of computing, networking, and storage components depends on factors like workload requirements, performance, scalability, and cost considerations.



-------------------------------------------------------------------------------------------------------------------------
				
		AWS: EC2 (VMs), VPC, EBS.
-------------------------------------------------------------------------------------------------------------------------

https://www.youtube.com/watch?v=MUMyZ3FFfdg

https://aws.amazon.com/about-aws/global-infrastructure/

https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html

Refer image in above link
Amazon cloud computing resources 
	hosted in multiple locations world-wide. 
	locations 
		AWS Regions,
			separate geographic area
		Availability Zones, 
			Each AWS Region has 
				multiple, isolated locations 
					known as Availability Zones
		Local Zones (like edge locations). 
			can place resources 
				e.g.
					compute and 
					storage, 
						in multiple locations 
							closer to your users. 
		Amazon RDS 
				enables you to place resources like 
					DB instances, and 
					data 
						in multiple locations. 
		N.B: Resources aren't replicated across AWS Regions unless you do so specifically.
			
			aws ec2 describe-regions	# Regions that are enabled for your account.
			aws ec2 describe-regions --all-regions

--------------------------
aws ec2 run-instances  --image-id ami-0cc87e5027adcdca8 --count 1  --instance-type t2.micro  --key-name verticurl  --security-group-ids sg-01df926c57ac13510 --subnet-id subnet-0a35682045e96d83d --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"VolumeSize\":30,\"DeleteOnTermination\":false}}]"  	
--------------------------
aws ec2 terminate-instances --instance-ids i-1234567890abcdef0
--------------------------

--tag-specifications= 'ResourceType=instance,Tags=[{Key=Name,Value=demo-server}]' 'ResourceType=volume,Tags=[{Key=Name,Value=demo-server-disk}]'

		Find name of the us-east-2 Region.
			aws lightsail get-regions --query "regions[?name=='us-east-2'].displayName" --output text
				ans: ohio
				
				

		(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)
		AWS Outposts 
			brings native AWS 
				services, 
				infrastructure, and 
				operating models 
					to virtually any data center, 
						co-location space, 
					or 
						on-premises facility.

		Wavelength Zones 
			allow developers to build applications that 
				deliver ultra-low latencies to 5G devices and end users. 
			Wavelength deploys 
				standard AWS compute and storage services 
					to the edge of telecommunication carriers' 5G networks.



	Amazon operates 
		state-of-the-art, highly-available data centers. 
	Rare but failures can occur 
		affect availability of [DB] instances 
			that are in the same location. 
	If you host all your DB instances in one location 
		none of your DB instances will be available.



	AWS Region 
		completely independent. 

		For e.g. any Amazon RDS activity you initiate 
			(for example, creating database instances or listing available database instances) 
			runs only in your current default AWS Region. 
			default AWS Region 
				can be changed in the console
			or 
				set the AWS_DEFAULT_REGION environment variable. 
			Or 
				override by using the 
					--region parameter 
					with the AWS Command Line Interface (AWS CLI). 
			
			More info: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html
		
	
	Availability Zones
		aws ec2 describe-availability-zones --region region-name
		aws ec2 describe-availability-zones --region us-east-1
			6 zones
		aws ec2 describe-availability-zones --region us-east-2
			3 zones
			
	https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html
		Availability Zones are multiple, isolated locations within each Region.




Amazon Elastic Block Store (Amazon EBS) 
---------------------------------------
block storage service provided by AWS that is designed for use with EC2 (Elastic Compute Cloud) instances. EBS volumes are highly available and durable block-level storage devices that you can attach to your EC2 instances. Here are some detailed notes about Amazon EBS:


Lab: 
	


	1. Create an ebs and (amazon) instance in the same subnet (availability zone)
		diff in one availability zone (same region) cannot connect 
		
		Note down the device name - say "/dev/xvdf1" - /dev/xvdf 
		
	2. From the ebs dashboard attach the ebs to an instance 
	3.  to instance dashboard
	4. Login to instance 
	5. lsblk 
		will display 
	6. lsblk -fs
		will disply - but not mounted or attached.
		
	To use this we need to create a partition and mount it.
	7. mkdir /tmp/images
	
	See all disk 
	8. fdisk -l 
		will list device 
	9. fdisk /dev/xvdf (Find this from the above command or noted earlier)
	10. fdisk 
		m - help 
		n - add new parition
		p - print partition table 
			enter default 
		w - write table to disk and exit 
	11. partprobe 
		inform kernel about partition changes 
	12. mkfs.xfs /dev/xvdf1
		format partition with xfs file system
		if you get an error 
			clear the directory using 
			sudo mkfs -t ext3 /dev/xvdf
			and retry mks.xfs command above.
		
	13. lsblk -fs 
	14. mount /dev/xvdf1 /tmp/images 
	15. lsblk -fs 
	To make it persistent 
	16. vi /etc/fstab
	i 
	/dev/xvdf1	/tmp/images 	xfs 	defaults 0 0 
	(device name-	mount point dir name- 	filesystem name 	defaults 0 0) 
	
	 to the directory and save files 
	cd /tmp/images
		mkdir test 
	df -h 
	
	unmount 
	17. df -h # disk is still there 
	17. umount /tmp/images  
		(not unmount - umount)
		this should be done from outside the disk 
	18. lsblk -fs 

	
	Now we can detach and delete the ebs.

	mount back 
	19. mount -a 



    Volume Types:
        Amazon EBS provides several volume types
			each optimized for different use cases:
				General Purpose SSD (gp2): 
					Suitable for most workloads, 
					balance of price and performance.
            Provisioned IOPS SSD (io1): 
				Designed for I/O-intensive applications
				can define desired number of IOPS (Input/Output Operations Per Second).
				e.g. databases
			Throughput Optimized HDD (st1): 
				Best for frequently accessed, throughput-intensive workloads like 
					big data and 
					data warehouses.
            Cold HDD (sc1): 
				Ideal for less frequently accessed workloads with large, sequential I/O.
            Magnetic (standard): 
				Older, 
				lower-cost option for workloads with low I/O requirements.

    Volume Size:
        EBS volumes can range in size from 
			1 GB to 16 TB (varies by volume type). 
		You can change the size of a volume on the fly, but you can't shrink it, only grow it.

    Snapshots:
        EBS snapshots are point-in-time backups of your EBS volumes. You can create snapshots manually or set up automated snapshot schedules.
        Snapshots are incremental, meaning only the changed data is saved with each new snapshot, which saves storage costs.
        You can use snapshots to create new EBS volumes or share them with other AWS accounts.

    Encryption:
        You can encrypt your EBS volumes using AWS Key Management Service (KMS) keys. Encrypted volumes provide additional data security.
        You can encrypt both boot volumes and data volumes.

    Performance:
        The performance of EBS volumes depends on the volume type selected:
            gp2 and io1 volumes are designed for low-latency, high-IOPS workloads.
            st1 and sc1 volumes are optimized for throughput and are best for streaming data.
            Magnetic volumes offer baseline performance with the ability to burst in performance when needed.

    Use Cases:
        EBS is commonly used for a wide range of applications, including running database systems (e.g., MySQL, PostgreSQL), application servers, and file storage.
        It is also used for creating custom RAID arrays, hosting application logs, and storing backup data.

    Attachment and Detachment:
        EBS volumes can be attached to and detached from EC2 instances as needed. You can also detach a volume from one instance and attach it to another.
        Attaching an EBS volume to an EC2 instance requires that the instance and volume are in the same Availability Zone.

    Multi-Attach:
        Some EBS volume types, like gp3, support multi-attach, which allows attaching a single EBS volume to multiple EC2 instances simultaneously. This is useful for shared storage scenarios.

    RAID with EBS:
        EBS volumes are often used in conjunction with RAID (Redundant Array of Independent Disks) configurations to improve performance, redundancy, and fault tolerance.

    Costs:
        You are charged for the provisioned capacity and IOPS, as well as any snapshots created from your EBS volumes.
        EBS pricing varies depending on the volume type, size, and region.

    Lifecycle Management:
        You can use AWS Data Lifecycle Manager to automate the creation and retention of snapshots and set policies for volume deletion.

    Volume Resizing:
        You can resize EBS volumes, but it involves stopping the associated EC2 instance in most cases.

    Elastic Volumes:
        With Elastic Volumes, you can adjust the size, performance, and type of EBS volumes on the fly without requiring instance downtime.

Amazon EBS is a fundamental component of AWS infrastructure, offering flexible, high-performance block storage for a variety of use cases, and it plays a crucial role in achieving data durability and availability within the AWS ecosystem.




Azure Disk
----------


	managed block storage service 
		by Microsoft Azure
	scalable and high-performance storage 
	can be attached to virtual machines (VMs) 
	used to 
		persist data, 
		operating system disks, 
		application data
			etc. 
	various types of disks available 
	different performance characteristics to meet the needs of different workloads. 
	
	Types of Disks:
		Azure Disk offers several types of disks, including:

		Standard HDD: 
			Standard Hard Disk Drives 
			cost-effective and 
			suitable for workloads with low I/O requirements.
		Standard SSD: 
			Standard Solid-State Drives 
			better performance than HDDs 
			slightly higher cost.
		Premium SSD: 
			Premium Solid-State Drives 
			high-performance disks 
			optimized for I/O-intensive workloads and mission-critical applications.
		Ultra Disk: 
			highest levels of performance 
			designed for the most demanding applications.
	Disk Sizes and Performance Tiers:
		Azure Disk supports 
			different sizes and 
			performance tiers 
				for each type of disk
		can select the right combination of capacity and performance for your workloads.

	Managed Disks:
		Azure Disk offers managed disks
		simplify disk management tasks like 
			provisioning, 
			scaling, and 
			data replication. 
		Managed disks 
			associated with an Azure resource group 
			automatically replicated 
				to provide high availability.

	Availability Zones and Availability Sets:
		can use with 
			Availability Zones or 
			Availability Sets 
				to ensure high availability of your applications. 
	Availability Zones 
		replicate your VMs and disks across datacenter availability zones 
			for increased fault tolerance.

	Snapshots:
		Azure Disk supports creating snapshots
			read-only copies of a disk at a point in time. 
		can be used for 
			backup, recovery, and 
			creating new disks.

	Disk Encryption:
		encryption at rest by default
			for security of your data. 
		can also enable Azure Disk Encryption for additional security.

	Scaling:
		You can dynamically scale the size and performance of Azure Disks without downtime. 
		This allows you to adapt to changing workload requirements.

	Use Cases:
		Azure Disk is suitable for a wide range of use cases, including hosting virtual machines, databases, application data, and more. It can be used with various Azure services and integrated into your Azure solutions.

	Cost Management:
		Azure Disk pricing is based on the type of disk, size, and performance tier you choose. Monitoring and cost management tools in Azure help you keep track of your disk usage and costs.

	Geographic Redundancy:
		Azure Disk provides options for geographic redundancy, allowing you to replicate your disks across different Azure regions for disaster recovery and high availability.

	In summary, Azure Disk is a versatile and scalable storage solution that can be tailored to meet the specific requirements of your Azure-based applications and workloads. It offers a range of disk types, sizes, and performance options to accommodate various use cases while providing robust features for data protection and high availability.


lab: 
	Create a windows server azure vm
	Attach a disk from vm - disk section.
	
	Login to azure vm using rdp
		Run 
			diskmgmt.msc 
			Temporary storage 
				volatile memory 
				can loose data 
				given by ms for free as od will 
	Back on browser 
		vm 
			disk 
				Attach new disk 
	back on rdp
		wait for some time 
		automatically disk appears
		right click (on left side)
			initialize disk 
			OK 
		right click again (on right side this time)
			new simple volume
			Next 
			give volume name in label 
			
		right click (check side)
			open 
			work with it.
			
	Without loosing data increase disk size 
		back in browser 
			detach disk from vm from vm details 
		to to disk 
			increase size and resize 
		to to vm
			attach the disk back 
			
		back in rdp 
			diskmgmt.msc
			right click 
				extend disk 
					Next 
					
-------------------------------------------------------------------------------------------------------------------------
				
		Azure: Virtual Machines, Virtual Network, Disk Storage.
-------------------------------------------------------------------------------------------------------------------------

Virtual Machines (VMs), Virtual Networks, and Disk Storage are fundamental components of cloud computing and data center infrastructure. Let's explore each of these components in more detail:
Virtual Machines (VMs):

    Definition: Virtual Machines are virtualized instances of physical servers. They allow you to run operating systems, applications, and workloads in isolated environments within a shared physical host.

    Key Characteristics:
        Isolation: 
			VMs are isolated from each other and share the underlying physical hardware resources, making them suitable for running different workloads on a single server.
        Scalability: 
			VMs can be easily scaled up or down by provisioning or deprovisioning instances as needed.
        Flexibility: 
			VMs support various operating systems and can run a wide range of applications.
        Resource Allocation: 
			You can allocate specific CPU, memory, and storage resources to each VM.

    Use Cases:
        Hosting web applications and websites.
        Running multiple operating systems on a single physical server (virtualization).
        DevOps and test environments.
        Legacy application support.
        High-performance computing (HPC).

Virtual Network:

    Definition: A Virtual Network (VNet) is a logically isolated network environment within a cloud provider's infrastructure. It allows you to define your own network topology, IP addressing, and routing rules.

    Key Characteristics:
        Isolation: VNets provide network isolation, allowing you to create private networks for your resources.
        Customization: You can define subnets, IP address ranges, and security groups to control network traffic.
        Connectivity: VNets can be connected to on-premises networks or other VNets through virtual private network (VPN) or peering connections.
        Security: Network security groups (NSGs) and firewalls help control traffic in and out of VNets.

    Use Cases:
        Creating isolated environments for applications and services.
        Segmentation of resources for security and compliance.
        Connecting on-premises data centers to the cloud.
        Hosting multi-tier applications with public and private subnets.

Disk Storage:

    Definition: Disk Storage provides persistent storage for virtual machines and applications. Cloud providers offer various types of storage options to cater to different needs.

    Key Characteristics:
        Block Storage: Block storage provides raw storage volumes that can be attached to VMs. It's used for file systems, databases, and applications. You can choose from different disk types, such as standard HDD, standard SSD, premium SSD, etc.
        Object Storage: Object storage is used for storing unstructured data, and it's accessed via APIs. It's suitable for storing files, images, videos, and backups.
        File Storage: File storage systems provide shared file systems that can be accessed by multiple VMs simultaneously. They are used for data sharing among instances.
        Snapshot and Backup: Most cloud providers offer snapshot and backup services to create point-in-time copies of your data for disaster recovery and data protection.

    Use Cases:
        Storing and managing data for applications.
        Providing scalable and durable storage for cloud-native applications.
        Backing up and archiving data.
        Supporting databases and file sharing across VMs.

These components are fundamental building blocks for creating and managing cloud infrastructure. The choice of VMs, virtual networks, and storage solutions depends on your specific use cases, performance requirements, scalability needs, and cost considerations. Cloud providers offer a variety of services and options to meet these diverse requirements.



-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Launching VMs, setting up VPCs/Virtual Networks, and attaching storage
-------------------------------------------------------------------------------------------------------------------------
Try this: 

Launching virtual machines (VMs), setting up Virtual Private Clouds (VPCs) or Virtual Networks, and attaching storage in Microsoft Azure involves several steps. Below is a high-level guide to performing these tasks:
Launching VMs in Azure:

    Sign in to Azure Portal:
         to the Azure Portal and sign in with your Azure account.

    Create a Resource Group:
        Resource groups help organize and manage related Azure resources. Create a new resource group or use an existing one.

    Create a Virtual Machine:
        Click on "Create a resource" or the "+" button on the left-hand menu.
        In the search box, type "Windows Virtual Machine" or "Linux Virtual Machine," and select the desired option.
        Configure the VM settings, including:
            Virtual machine name and region.
            Operating system image (Windows or Linux).
            Size of the VM (compute resources).
            Administrative username and password (for Windows) or SSH key (for Linux).
            Networking settings, such as virtual network and subnet.
            Optional settings like extensions, tags, and monitoring.

    Review and Create:
        Review your VM configuration and click "Review + create."
        Azure will validate the configuration for any errors or issues.

    Create the VM:
        After validation passes, click "Create" to provision the virtual machine.
        Azure will deploy the VM based on your configuration.

    Access the VM:
        Once the VM is provisioned, you can access it using remote desktop (RDP) for Windows or SSH for Linux.
        Connect to the VM using the credentials you specified during configuration.

Setting Up Virtual Networks (VNets) in Azure:
---------------------------------------------
    Sign in to Azure Portal:
        If not already signed in,  to the Azure Portal and log in.

    Create a Virtual Network:
        Click on "Create a resource" or the "+" button on the left-hand menu.
        In the search box, type "Virtual Network" and select "Virtual Network."

    Configure VNet Settings:
        Provide a name for the VNet and select the subscription and resource group.
        Choose the region for the VNet.
        Specify the IP address space for the VNet and configure subnets within it.

    Security and Routing:
        Configure Network Security Groups (NSGs) to control inbound and outbound traffic.
        Set up route tables if necessary to define custom routing.

    Review and Create:
        Review your VNet configuration and click "Review + create."
        Azure will validate the configuration.

    Create the VNet:
        After validation, click "Create" to create the Virtual Network.



-------------------------------------------------------------------------------------------------------------------------
				
	3	Identity & Access Management (IAM)
-------------------------------------------------------------------------------------------------------------------------
AWS Identity and Access Management (IAM) 
	aws service 
	securely control access to AWS resources. 
	centrally manage permissions 
	control which AWS resources 
	IAM control 
		who is authenticated (signed in) 
	and 
		authorized (has permissions) to use resources.

Root account
create an AWS account
	get one sign-in identity 
	has complete access to all AWS services and resources 
		called AWS account root user 
	accessed by signing in with the 
		email address and password 
	Safeguard your root user credentials 
	
	

IAM features
------------
	1. Shared access to your AWS account
		can grant 
			other people permission to 
				administer and 
				use resources in your AWS account 
					without sharing 
						password or 
						access key.

	2. Granular permissions
	
		grant different permissions 
			to different people 
				for different resources. 
		For e.g.
			full access to 
				EC2
				S3
				DynamoDB
				Amazon Redshift
			allow 
				read-only access to 
					S3 buckets
			administer just some EC2 instances
			access your billing information but nothing else.

	3. Secure access to AWS resources 
		for applications that run on Amazon EC2
		
		securely provide credentials for 
			applications that run on EC2 instances. 
		provide permissions 
			for your application 
			to access other AWS resources. 
		Examples 
			access 
				S3 buckets and DynamoDB tables
				from ec2 applications.

	4. Multi-factor authentication (MFA)
		two-factor authentication to 
			your account and 
			to individual users 
				for extra security. 
		provide 
			password or	access key 
		and 
			code from a specially configured device. 
			
	5. Identity federation
		allow users 
			who already have accounts elsewhere—
			for example
				in your corporate network or with an internet identity provider—
				to get temporary access to your AWS account.

	6. Identity information for assurance
		use AWS CloudTrail, 
			you receive log records 
				include information like 
					who made requests for resources 
						based on IAM identities.

	7. PCI DSS Compliance
		IAM supports 
			processing, 
			storage, and 
			transmission of credit card data 
				by a merchant or service provider
		validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). 

	8. Integrated with many AWS services
		https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html
		

	9. Eventually Consistent
		IAM
			is eventually consistent. 
		achieves high availability 
			replicate data across multiple servers 
			in Amazon's data centers around the world. 
		change some data 
			change is committed and safely stored. 
		change must be replicated 
			across IAM
			can take some time. 
		Such changes include 
			creating or updating 
				users, 
				groups, 
				roles, or 
				policies. 
		do not include such IAM changes in 
			critical, 
			high-availability 
				code paths of your application. 
		make IAM changes 
			in a separate initialization or 
			setup routine that you run less frequently. 
		verify 
			changes have propagated 
				before production workflows depend on them. 
		
	10. Free to use
		IAM and AWS Security Token Service (AWS STS) are 
			features of your AWS account 
			offered at no additional charge. 
		charged only when you access other AWS services using 
			your IAM users or AWS STS temporary security credentials. 

	11. Accessing IAM
		can work with IAM in following ways.

		a. AWS Management Console
			browser-based interface 
				manage IAM and AWS resources. 
				
		b. AWS Command Line Tools
			issue commands at your system's command line 
			command line 
				can be faster and 
				more convenient 
					than the console. 
			can automate

		c. AWS provides two sets of command line tools: 
			AWS Command Line Interface (AWS CLI) 
		and 
			AWS Tools for Windows Power. 
			
		d. AWS SDKs
			AWS provides SDKs (software development kits) 
			consist of 
				libraries and 
				sample code 
					for various programming languages and 
				platforms 
					(Java, Python, Ruby, .NET, iOS, Android, etc.). 
				convenient way to create programmatic access to 
					IAM and AWS. 
				
		16. IAM HTTPS API
			You can access IAM and AWS programmatically 
				by using the IAM HTTPS API, 
				issue HTTPS requests 
					directly to the service. 
			While using the HTTPS API
				include code to digitally sign requests 
					using your credentials. 
					
	https://catalog.workshops.aws/general-immersionday/en-US/basic-modules/30-iam


AWS (flow chart)
	Refer diagram in https://shunliz.gitbooks.io/aws-certificate-notes/content/iam.html

	deny all by default.
	if a policy/role is not present 
		user can't do anything.
	get all policies and roles
		identify policies and roles associated with the current resource
		ignore all others
		consider all applicable
	
		if a deny policy exist
			yes
				ignore allow policies and deny
			no
				if there is a allow policy
					allow
		no policy applicable
			deny

-------------------------------------------------------------------------------------------------------------------------
				
		
		AWS IAM: Users, roles, policies.
-------------------------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html

An AWS Identity and Access Management (IAM) user 
	entity that you create in AWS. 
	represents 
		human user 
	or 
		workload who uses the IAM user 
			to interact with AWS. 
	consists of a 
		name and 
		credentials.
Two types
	Root user 
	IAM user


How AWS identifies an IAM user
	A "friendly name"
	An Amazon Resource Name (ARN)
		arn:aws:iam::account-ID-without-hyphens:user/Richard	
			(this is standard - can't be seen on screen when you create)
			after creating - go to the user details 
	unique identifier for the IAM user	

You can access AWS in different ways depending on the IAM user credentials:
	Console password: 
	Access keys
	SSH keys for use with CodeCommit
	Server certificates
	
Following options to administer passwords
	Manage passwords for your IAM users. 
	Manage access keys for your IAM users.
	Enable multi-factor authentication (MFA) for the IAM user. 
		Find unused passwords and access keys. 
		security best practice 
			remove passwords and access keys 
				when users no longer need them.

--------------------------------------------------------------------------------------------------------

IAM Users
	represents the 
		person or 
		service 
			who uses the access to interact with AWS.
	IAM Best Practice 
		Create Individual Users
	User credentials can consist of the following
		Password
			access through console
		Access Key/Secret Access Key 
			access AWS services through 
				API, 
				CLI or 
				SDK
	IAM user 
		starts with no permissions 
		should be granted permissions 
			as per the job function requirement
	IAM Best Practice – 
		Grant least Privilege
	Each IAM user 
		associated with one and only one AWS account.
	IAM User 
		cannot 	be renamed from console
		be done from CLI or SDK tools.
	IAM handles the renaming of user w.r.t 
		unique id, 
		groups, 
		policies 
			where the user was mentioned as a principal. 
		However, you need to handle the renaming in the policies where the user was mentioned as a resource	

IAM users and permissions
	By default, 
		new IAM user 
			has no permissions. 
				except to login
				
	can assign 
		permissions through 
			roles
			policies
				
	can also add a permissions boundary to your IAM users. 
		advanced feature 
		allows you to use 
			AWS managed policies 
			limit the maximum permissions 
				policy can grant to an IAM user or role. 


IAM users and accounts
	Each IAM user 
		is associated with one AWS account only. 
		defined within your AWS account
		don't need to have a payment method 
		AWS activity performed by IAM users 
			billed to root account.

	The number and size of 
		IAM resources in an AWS account 
			are limited. 
	
IAM users as service accounts
	An IAM user 
		resource in IAM 
		has associated 
			credentials and 
			permissions. 
	An IAM user 
		can represent 
			a person or 
			an application 
				referred as a service account. 
	do not embed access keys 
		directly into your application code. 
	put access keys in 
		known locations 
	
	Most common features
		Users
		Groups
		Policies
		Roles
		https://www.youtube.com/watch?v=jP-1qPe6P4s&list=PLv2a_5pNAko0Mijc6mnv04xeOut443Wnk&index=9
	https://267092042432.signin.aws.amazon.com/console


--------------------------------------------------------------------------------------------------------
	Groups 

IAM Groups
	IAM group 
		collection of IAM users
	IAM groups 
		specify permissions 
			for a collection of users 
				
	IAM Best Practice – 
		Use groups to assign permissions to IAM Users
	A group 
		not truly an identity 
			because it cannot be identified as a Principal 
			only a way to attach policies to multiple users at one time
	A group to users
		many-to-many
		user can belong to multiple groups (10 max)
	Groups cannot be nested 
		can only have users within it
	no default group 
		if one is required 
			it should be created with all users assigned to it.
	Deletion of the groups requires you to detach users and managed policies and delete any inline policies before deleting the group. With AWS management console, the deletion and detachment is taken care of.

--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html


IAM user group 
	collection of IAM users. 
	specify permissions 
		for multiple users
	easier to manage the permissions for those users. 
	For e.g.
		give admin permission to a group.
		add new user 
		move a user out
		
	attach an identity-based policy 
		to a user group 
			all of the users in the user group 
				receive the policy's permissions. 
	cannot identify a user group as a Principal 
		in a policy 
			(such as a resource-based policy) 
			because groups relate to permissions
				not authentication
				and principals are authenticated IAM entities. 
	
important characteristics of user groups:
	can contain many users
	user can belong to multiple user groups.
	User groups can't be nested; 
		can contain only users
		not other user groups.
	No default user group 
		automatically includes all users in the AWS account. 
	number and size of IAM resources 
		like 
			number of groups
			number of groups that a user can be a member of, are limited. 
	refer below		
		https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html


lab: 
Use cases with users 

		
	Use Case 1: Creating an IAM User and Assigning Permissions

		In this use case, you will create an IAM user and assign permissions to access specific AWS services.

		Steps:

			Sign in to the AWS Management Console:
				Navigate to the AWS Management Console (https://aws.amazon.com/).
				Sign in to your AWS account using your credentials.

			Access IAM Dashboard:
				In the AWS Management Console, go to the IAM dashboard by searching for "IAM" in the AWS services search bar.

			Create a New IAM User:
				In the IAM dashboard, click on "Users" in the left navigation pane.
				Click on the "Add user" button.
				Enter a username for the new IAM user and select the access type (Programmatic access, AWS Management Console access, or both).
				Set permissions for the user by adding them to one or more IAM groups or attaching policies directly.

			Assign Permissions (Example):
				To assign permissions, you can attach existing policies to the user. For example, you can attach the "Amazon S3 Full Access" policy to grant full access to Amazon S3.

			Review and Create User:
				Review the user's configuration and permissions.
				Click the "Create user" button.

			Download User Credentials:
				After creating the user, you will receive security credentials (Access Key ID and Secret Access Key) for programmatic access. Download these credentials and securely store them.


	Use Case 2: Developer User

		Create a Developer User:
			Create an IAM user for a developer.
			Attach policies that provide access to specific development-related services
				Amazon S3 for storage and 
				AWS Lambda for serverless computing.
			This user can work on development tasks without full administrative access.

	Use Case 3: Read-Only User

		Create a Read-Only User:
			Create an IAM user with read-only permissions.
			Attach policies like 
				"AmazonEC2ReadOnlyAccess" and 
				"AmazonRDSReadOnlyAccess" 
					to allow read-only access to EC2 instances and RDS databases.
			This user can view resources but cannot make changes.

	Use Case 4: Billing User

		Create a Billing User:
			Create an IAM user specifically for viewing billing information and cost management.
			Attach the "Billing" managed policy to grant access to billing and cost explorer services.
			This user can monitor AWS spending and cost details.

	Use Case 5: Application User

		Create an Application User:
			Create an IAM user to be used by an application running on an EC2 instance.
			Attach policies that grant access to specific AWS services required by the application, such as Amazon DynamoDB for database operations.
			Configure EC2 instance profiles to automatically grant the necessary permissions to the application without storing access keys.





--------------------------------------------------------------------------------------------------------
	Policies
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

Helps to define	
	group of permissions
	boundaries for them
	
	create policies 
		attach them to IAM identities 
			users
			groups 
			roles
			or AWS resources. 
	object in AWS 
		associated with 
			identity or 
			resource, 
				defines their permissions. 
	AWS evaluates these policies 
		when an IAM principal (user or role) 
			makes a request. 
	
	Most policies 
		stored in AWS as JSON documents. 
	AWS supports six types of policies: 
		identity-based policies, 
		resource-based policies, 
		permissions boundaries, 
		Organizations SCPs, 
		ACLs, and 
		session policies.

Policy definition 
-----------------
e.g. Policy definition 


	Consists of
	• Version: 
			policy language version, always include “2012-10-17”
	• Id: 
		an identifier for the policy (optional)
	• Statement: 
		one or more individual statements (required)
	• Statements consists of
	• Sid: an identifier for the statement (optional)
	• Effect: whether the statement allows or denies access
		(Allow, Deny)
	• Principal: account/user/role to which this policy applied to
	• Action: 
		list of actions this policy allows or denies
	• Resource: 
		list of resources to which the actions applied to
	• Condition: conditions for when this policy is in effect(optional)

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "FullAccess",
            "Effect": "Allow",
            "Action": ["s3:*"],
            "Resource": ["*"]
        },
        {
            "Sid": "DenyCustomerBucket",
            "Action": ["s3:*"],
            "Effect": "Deny",
            "Resource": ["arn:aws:s3:::customer", "arn:aws:s3:::customer/*" ]
        }
    ]
}


For example
	if a policy allows 
		GetUser 
			user 
				can get user information from 
					AWS Management Console, 
					AWS CLI
					AWS API. 
	
	
	
	Policy types
	------------
The following 
	policy types
		listed in 
			most frequently used to 
			less frequently used, 
			are available for use in AWS. 
			
(1) 
Identity-based policies – 
	Attach managed and inline policies to IAM identities 
	
	grant permissions to an identity.
		AWS users, 
		groups 
	
	Give access to 
		EC2 instances
		S3 buckets
		RDS databases
			etc.

		two types of identity-based policies 
			AWS-managed policies and 
			customer-managed policies.

			AWS-managed policies 
				predefined policies 
					created by AWS 
						we use to manage access to AWS resources. 
						
			Customer-managed policies
				create and manage us. 
				
				can be attached to 
					IAM users, 
					groups, or 
					roles
					can be modified as needed to 
						grant or 
						restrict access to AWS resources.
(2)
Resource-based policies – 
	Attach inline policies to resources. 
	e.g. 
		Amazon S3 bucket policies and 
		IAM role trust policies. 
		
		grant permissions to  
			principal specified in the policy. 
			Principals can be 
				same account 
			or 
				other accounts.
				
				


Use case with Policies 
-----------------------
Use Case 1: Enforcing Least Privilege Access

    Least Privilege Policy:
        Create an IAM policy that grants only the minimum permissions required for a specific task.
        Attach this policy to IAM users or roles to ensure they have access to only the resources and actions necessary for their job.
        This policy adheres to the principle of least privilege, reducing the risk of accidental or intentional misuse of permissions.

Use Case 2: Securing S3 Bucket Access

    S3 Bucket Policy:
        Create an S3 bucket policy that restricts access to a specific Amazon S3 bucket.
        Define conditions in the policy, such as allowing access only from specific IP addresses or requiring multi-factor authentication (MFA) for deletion.
        Apply the bucket policy to grant or deny access to users, roles, or even external accounts.



		
		
		
		

IAM Guidelines & Best Practices
---------------------------------
• Don’t use the root account except for AWS account setup
• One physical user = One AWS user
• Assign users to groups and assign permissions to groups
• Create a strong password policy
• Use and enforce the use of Multi Factor Authentication (MFA)
• Create and use Roles for giving permissions to AWS services
• Use Access Keys for Programmatic Access (CLI / SDK)
• Never share IAM users & Access Keys
		
---------------------------------------------------------------------
Hands on: 
a. Create a new user vilas1
b. login as vilas1
c. vilas1 cannot create a vpc
d. As admin create the following policy 
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AcceptVpcPeeringConnection",
                "ec2:AcceptVpcEndpointConnections",
                "ec2:AllocateAddress",
                "ec2:AssignIpv6Addresses",
                "ec2:AssignPrivateIpAddresses",
                "ec2:AssociateAddress",
                "ec2:AssociateDhcpOptions",
                "ec2:AssociateRouteTable",
                "ec2:AssociateSubnetCidrBlock",
                "ec2:AssociateVpcCidrBlock",
                "ec2:AttachClassicLinkVpc",
                "ec2:AttachInternetGateway",
                "ec2:AttachNetworkInterface",
                "ec2:AttachVpnGateway",
                "ec2:AuthorizeSecurityGroupEgress",
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:CreateCarrierGateway",
                "ec2:CreateCustomerGateway",
                "ec2:CreateDefaultSubnet",
                "ec2:CreateDefaultVpc",
                "ec2:CreateDhcpOptions",
                "ec2:CreateEgressOnlyInternetGateway",
                "ec2:CreateFlowLogs",
                "ec2:CreateInternetGateway",
                "ec2:CreateLocalGatewayRouteTableVpcAssociation",
                "ec2:CreateNatGateway",
                "ec2:CreateNetworkAcl",
                "ec2:CreateNetworkAclEntry",
                "ec2:CreateNetworkInterface",
                "ec2:CreateNetworkInterfacePermission",
                "ec2:CreateRoute",
                "ec2:CreateRouteTable",
                "ec2:CreateSecurityGroup",
                "ec2:CreateSubnet",
                "ec2:CreateTags",
                "ec2:CreateVpc",
                "ec2:CreateVpcEndpoint",
                "ec2:CreateVpcEndpointConnectionNotification",
                "ec2:CreateVpcEndpointServiceConfiguration",
                "ec2:CreateVpcPeeringConnection",
                "ec2:CreateVpnConnection",
                "ec2:CreateVpnConnectionRoute",
                "ec2:CreateVpnGateway",
                "ec2:DeleteCarrierGateway",
                "ec2:DeleteCustomerGateway",
                "ec2:DeleteDhcpOptions",
                "ec2:DeleteEgressOnlyInternetGateway",
                "ec2:DeleteFlowLogs",
                "ec2:DeleteInternetGateway",
                "ec2:DeleteLocalGatewayRouteTableVpcAssociation",
                "ec2:DeleteNatGateway",
                "ec2:DeleteNetworkAcl",
                "ec2:DeleteNetworkAclEntry",
                "ec2:DeleteNetworkInterface",
                "ec2:DeleteNetworkInterfacePermission",
                "ec2:DeleteRoute",
                "ec2:DeleteRouteTable",
                "ec2:DeleteSecurityGroup",
                "ec2:DeleteSubnet",
                "ec2:DeleteTags",
                "ec2:DeleteVpc",
                "ec2:DeleteVpcEndpoints",
                "ec2:DeleteVpcEndpointConnectionNotifications",
                "ec2:DeleteVpcEndpointServiceConfigurations",
                "ec2:DeleteVpcPeeringConnection",
                "ec2:DeleteVpnConnection",
                "ec2:DeleteVpnConnectionRoute",
                "ec2:DeleteVpnGateway",
                "ec2:DescribeAccountAttributes",
                "ec2:DescribeAddresses",
                "ec2:DescribeAvailabilityZones",
                "ec2:DescribeCarrierGateways",
                "ec2:DescribeClassicLinkInstances",
                "ec2:DescribeCustomerGateways",
                "ec2:DescribeDhcpOptions",
                "ec2:DescribeEgressOnlyInternetGateways",
                "ec2:DescribeFlowLogs",
                "ec2:DescribeInstances",
                "ec2:DescribeInternetGateways",
                "ec2:DescribeIpv6Pools",
                "ec2:DescribeLocalGatewayRouteTables",
                "ec2:DescribeLocalGatewayRouteTableVpcAssociations",
                "ec2:DescribeKeyPairs",
                "ec2:DescribeMovingAddresses",
                "ec2:DescribeNatGateways",
                "ec2:DescribeNetworkAcls",
                "ec2:DescribeNetworkInterfaceAttribute",
                "ec2:DescribeNetworkInterfacePermissions",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribePrefixLists",
                "ec2:DescribeRouteTables",
                "ec2:DescribeSecurityGroupReferences",
                "ec2:DescribeSecurityGroupRules",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeStaleSecurityGroups",
                "ec2:DescribeSubnets",
                "ec2:DescribeTags",
                "ec2:DescribeVpcAttribute",
                "ec2:DescribeVpcClassicLink",
                "ec2:DescribeVpcClassicLinkDnsSupport",
                "ec2:DescribeVpcEndpointConnectionNotifications",
                "ec2:DescribeVpcEndpointConnections",
                "ec2:DescribeVpcEndpoints",
                "ec2:DescribeVpcEndpointServiceConfigurations",
                "ec2:DescribeVpcEndpointServicePermissions",
                "ec2:DescribeVpcEndpointServices",
                "ec2:DescribeVpcPeeringConnections",
                "ec2:DescribeVpcs",
                "ec2:DescribeVpnConnections",
                "ec2:DescribeVpnGateways",
                "ec2:DetachClassicLinkVpc",
                "ec2:DetachInternetGateway",
                "ec2:DetachNetworkInterface",
                "ec2:DetachVpnGateway",
                "ec2:DisableVgwRoutePropagation",
                "ec2:DisableVpcClassicLink",
                "ec2:DisableVpcClassicLinkDnsSupport",
                "ec2:DisassociateAddress",
                "ec2:DisassociateRouteTable",
                "ec2:DisassociateSubnetCidrBlock",
                "ec2:DisassociateVpcCidrBlock",
                "ec2:EnableVgwRoutePropagation",
                "ec2:EnableVpcClassicLink",
                "ec2:EnableVpcClassicLinkDnsSupport",
                "ec2:ModifyNetworkInterfaceAttribute",
                "ec2:ModifySecurityGroupRules",
                "ec2:ModifySubnetAttribute",
                "ec2:ModifyVpcAttribute",
                "ec2:ModifyVpcEndpoint",
                "ec2:ModifyVpcEndpointConnectionNotification",
                "ec2:ModifyVpcEndpointServiceConfiguration",
                "ec2:ModifyVpcEndpointServicePermissions",
                "ec2:ModifyVpcPeeringConnectionOptions",
                "ec2:ModifyVpcTenancy",
                "ec2:MoveAddressToVpc",
                "ec2:RejectVpcEndpointConnections",
                "ec2:RejectVpcPeeringConnection",
                "ec2:ReleaseAddress",
                "ec2:ReplaceNetworkAclAssociation",
                "ec2:ReplaceNetworkAclEntry",
                "ec2:ReplaceRoute",
                "ec2:ReplaceRouteTableAssociation",
                "ec2:ResetNetworkInterfaceAttribute",
                "ec2:RestoreAddressToClassic",
                "ec2:RevokeSecurityGroupEgress",
                "ec2:RevokeSecurityGroupIngress",
                "ec2:UnassignIpv6Addresses",
                "ec2:UnassignPrivateIpAddresses",
                "ec2:UpdateSecurityGroupRuleDescriptionsEgress",
                "ec2:UpdateSecurityGroupRuleDescriptionsIngress"
            ],
            "Resource": "*"
        }
    ]
}

e. 	attach it to vilas1 (from Policy usage)
---------------------------------------------------------------------
(c)
Permissions boundaries – 
	Use a managed policy 
		as the permissions boundary 
			for an IAM entity 
				(user or role). 
		defines maximum permissions 
			that the identity-based policies 
				can grant to an entity
				but does not grant permissions. 
		Permissions boundaries 
			do not define 
				maximum permissions that a resource-based policy 
					can grant to an entity.

(d)
Organizations SCPs (service control policy) – 
	
	maximum permissions 
		for account members 
			of an organization or organizational unit (OU). 
			
(e)
Access control lists (ACLs) – 
	control 
		which principals 
			in other accounts 
				can access the resource 
					to which the ACL is attached. 
		similar to resource-based policies
		only policy type that does not use the JSON policy document structure. 
		cross-account permissions policies 
		cannot grant permissions to entities within the same account.
(f)
Session policies – 
	Pass advanced session policies 
		when you use 
			AWS CLI or 
			AWS API 
				to assume a role or a federated user. 
	Session policies limit the permissions that the role or user's identity-based policies grant to the session. 
	Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies.



--------------------------------------------------------------------------------------------------------
	Roles
--------------------------------------------------------------------------------------------------------
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html



https://www.knowledgehut.com/tutorials/aws/iam-roles
	How to create an IAM role using the AWS Management Console?


Use cases for Roles 

Use Case 3: EC2 Instance Role 

    EC2 Instance Role:
        Create an IAM role policy for EC2 instances.
        Attach this policy to an EC2 instance profile so that applications running on the instance can securely access AWS services without hardcoding access keys.
        Grant permissions to access services like S3, DynamoDB, or SSM based on the application's requirements.

Use Case 4: Cross-Account Access / users authenticated using other identity providers 

    Cross-Account Access:
        Establish trust between AWS accounts by creating an IAM role in one account and allowing another AWS account to assume that role.
        Define a cross-account access policy for the IAM role that specifies which actions and resources are accessible from the trusted account.
        This use case is useful for sharing resources securely between different AWS accounts, such as granting a third-party access to specific resources.

Use Case 5: Temporary Permissions with IAM Roles

    Temporary Permissions with IAM:
        Create IAM roles with time-limited permissions.
        Attach policies that grant permissions for tasks like launching EC2 instances or accessing AWS services.
        Applications or users can assume these roles temporarily to perform specific actions and then automatically relinquish those permissions when the role session ends.				

	
	
Role for an user from different account	
	An IAM role 
		can be created from	
			AWS Management Console, 
			AWS CLI, 
			Tools for Windows Power or 
			IAM API.

	Sign in to the AWS Management Console.
	Open the IAM console.
	Creating IAM roles
	In the navigation pane of the console
		click on the ‘Roles’ 
		choose ‘Create role’ option.
		Creating IAM roles
	
	Click on ‘Another AWS account’ role type.
		For the ‘Account ID’
			type the AWS Account ID 
				to which permissions need to be granted.
	The administrator of the account 
		has the ability to grant permission 
			to give this role to any IAM user of that account.

AWS IAM Role
	similar to user
	an identity with permission policies 
		determine what the identity 
			can and 
			cannot do 
		identiy 
			external account
			resources
				
	not used to 
		associate 
			with a particular 
				user, 
				group.
	does not have any static credentials 
		password or access keys
	whoever assumes the role 
		provided with dynamic temporary credentials.
		can access delegation 
			to grant permissions to someone 
			that allows access to resources that you control.
	help to prevent accidental access 
		to or modification of sensitive resources.
	Modification of a Role 
		can be done anytime 
		changes propogate immediately.
	Advantages of roles
		Services like EC2 instances 
			needs to access other 
				AWS services
				e.g. s3.
		Cross-Account access – 
			users from different AWS accounts 
				have access to AWS resources 
					in a different account
			no need to create a user.
		Identity Providers & Federation
			Company uses a Corporate Authentication mechanism 
				User doesn't need to authenticate twice 
				or create duplicate users in AWS
			(Federation)Applications allowing login 
				through external authentication mechanisms 
					e.g. Amazon, Facebook, Google, etc
		Role can be assumed by
			IAM user within the same AWS account
			IAM user from a different AWS account
			AWS services such as 
				EC2, 
				EMR 
					to interact with other services
		An external user authenticated 
			by an external identity provider (IdP) service 
				that is compatible with 
					SAML 2.0 or 
					OpenID Connect (OIDC), or 
					a custom-built identity broker.

Role involves defining two policies
	Trust policy
		defines –
			who can assume the role
		involves 
			setting up a trust 
				between the account 
					that owns the resource 
						(trusting account) and 
						account that owns the user 
							that needs access to the resources (trusted account).
	Permissions policy
		defines – 
			what they can access
		determines 
			authorization
				grants the user of the role 
					with the needed permissions 
						to carry out the desired tasks on the resource
		Federation is creating a trust relationship 
			between an external Identity Provider (IdP) and AWS.
		Users can 
			sign in to an enterprise identity system 
				that is compatible with SAML


	Users can sign in to a 
		web identity provider, 
		such as Login with 
			Amazon, 
			Facebook, 
			ogle, or any 
			IdP 
				that is compatible with 
					OpenID connect (OIDC).
	When using 
		OIDC and SAML 2.0 
			to configure a trust relationship between these 
				external identity providers and AWS
					user is assigned an IAM role 
					receives temporary credentials 
					enable the user to access AWS resources.



	
	IAM 
		consists of a list of 
			AWS managed policies and 
			customer managed policies 
		A policy can be chosen from this or click on ‘Create Policy’ to open a new browser tab and create a new policy.
	
	Once the policy has been created
		user needs to return to the original tab.
	
	Click on the check box which is present next to the permission policies
		thereby indicating that the specific user has the permission to take up the role.
	
	policies can be attached to a role 
		
lab
https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/

https://www.youtube.com/watch?v=NHAuCWIHevk
	Create role with s3 full access
	click	
		Create role
		"Services" - ec2
		add permission  
			Search for S3FullAccess
			Give a name
			Create a role
		Verify the permission
		
	Launch instance with role
		While creating a new instance
		select "<role you created>" in IAM Role
	Access s3 bucket with cli commands
		mb s3://<bucketname>
		aws s3 ls
		aws s3 rb s3://<bucketname>



Lab: Refer file AWSTOCNotes.txt - Creating and Managing (aws cli) line 1859 ownwards
------------------------------------------------------------------------------------
	

Rohindh 
	create tenant 
	


-------------------------------------------------------------------------------------------------------------------------
				
		Azure Active Directory & RBAC.
-------------------------------------------------------------------------------------------------------------------------
Azure Active Directory (Azure AD) and 
Role-Based Access Control (RBAC) 
	core identity and access management components in Microsoft Azure 
		enable you to manage user identities 
		control access to Azure resources. 
		
Role Scope (Roles can be assigned at any level below)




					Management Group (created while creating account)
					----------------------------
					|							|
			Subscription 1				Subscription 2 (add subscription)	- tracking payment/cost 
			---------------------
			|					|
		Resource group 1 	Resource group 2
	-----------------
	|				|
Resource 1			Resource 2	
		


RBAC
----
Authorization system build using Azure Entra ID
Fine grained access management of Azure Resources
	at various scopes 
	
Role assignment is a combination of 
	Role definition
		list of permissions like 
			VM 
			SQL 
				delete SQL
			IAM 
				assign permissions 
	Security principal
		user 
		group 
		service principal 
			associated to an application 
			https://learn.microsoft.com/en-us/purview/create-service-principal-azure?wt.mc_id=searchAPI_azureportal_inproduct_rmskilling&sessionId=9a029fbd59244770b6f6af80346bcb3f
		managed identity 
			a service in azure 
				see below for the diff. between service principal and managed identity
		
	Scope 
		resource 
		resource groups
		subscriptions 
		managemet group (entire account)




Service Principal Vs Managed Identity
---------------------------------------------------------------------------------------------------------------------
Functionality 		|		Service Principal 							|	Managed Identity
---------------------------------------------------------------------------------------------------------------------
Definition: 		|Security identity used by                          |Azure resource 
					|	applications,                                   |	automatically created and managed by Azure
					|	services,                                       |created automatically when specific Azure resources
					|	automation tools                                |	e.g. Azure Web App, VM, Azure Functions etc.
					|		to access Azure resources.                  |
					|													|service principal can act as a wrapper for a managed identity
					|	non-human identity                              |type of identity used to  
					|	represent                                       |	authenticate and access Azure resources securely
					|		application/service.                        |
                    |                                                   |
                    |                                                   |
---------------------------------------------------------------------------------------------------------------------

Creation            |   explicitly by 									|can be  
					|		Azure AD administrator 						|	provisioned (system-assigned)
					|		or 											|	or 
					|		an application owner.                       |	standalone resources (user-assigned)
                    |       for specific applications	                |
					|													|
---------------------------------------------------------------------------------------------------------------------

Lifecycle Management| require manual management							|automatically managed by Azure
					|		credential rotation 						|	credential rotation and renewal
					|		renewal                                     |Administrators don't bother to manage
                    |                                                   |
---------------------------------------------------------------------------------------------------------------------

Scope				|different scopes									|Managed identities 
					|	application-level 								|		associated with specific Azure resources
					|	tenant-level 									|
                    |                                                   |
---------------------------------------------------------------------------------------------------------------------

Authentication      |Supports different methods for authentication		|Managed identities use Azure AD tokens for authentication
					|	client secrets									|	
					|	certificates, or 								|secure by design 
					|	managed identities								|	access Azure resources 
					|													|	without managing secrets
					|	                                                |   
---------------------------------------------------------------------------------------------------------------------







Managed Identity:

    Lifecycle Management: Managed identities are . Administrators do not need to handle credentials; Azure takes care of this.

    Scope: 

    Authentication: , eliminating the need for credentials. They are .

Key Differences:

    Creation: Service principals are created explicitly by administrators, while managed identities are automatically created by Azure for specific resources or can be explicitly created by users.

    Lifecycle Management: Service principals require manual management of credentials, while managed identities are fully managed by Azure, including credential rotation and renewal.

    Scope: Service principals can be scoped at various levels, while managed identities are scoped to the Azure resource where they are enabled.

    Authentication: Managed identities use Azure AD tokens for authentication, providing a secure and token-based authentication mechanism, whereas service principals may rely on credentials (client secrets or certificates).

		
Azure Active Directory (Azure AD):

    Definition: 
		Azure AD (Azure Entra ID) 
			Microsoft's cloud-based identity and access management service. 
			Identity provider for 
				Azure and 
				other Microsoft services. 

    Key Features:
        User Identity Management: 
			password reset.
			can authenticate
			single sign-on (SSO)
				identity management.
			Azure AD 
				create and manage user accounts
				assign roles and permissions
					Roles in azure are similar to policies in aws
					
				enable self-service 
        Single Sign-On (SSO): 
			Users can access multiple applications 
				with a single set of credentials
				
        Multi-Factor Authentication (MFA): 
			provide additional authentication factors beyond passwords.
        Application Integration: 
			integrate with thousands of 
				SaaS applications, 
				custom applications, and 
				on-premises resources.
        Azure AD B2B and B2C: 
			Azure AD B2B 
				allows collaboration with 
					external partners
			Azure AD B2C 
				enables customer identity and 
				access management for applications.
		
		
		Identity
			can be 
				person 
					vilasvarghese@ms.com
				application 
					payroll application 
						rights to db 
				device 
					has access
			
			usually has 
				password or
				key or
				certificate 
		
		
		Identity management 
			based on AAD now renamed to Azure Electra ID
			
		Traditional AD 
			does not work on IP
		Azure AD 
			identity as a service 
			application's can use Azure AD or 
		
	Adv of AAD
		Security 
			100*1000 companies 
		Reduce dev. time 
		Feature rich 
			MFA
			Conditional access
				done through Policies in azure 
				https://learn.microsoft.com/en-us/azure/active-directory/conditional-access/overview
			SSO
			
	how it works 
		clients 
			authenticate with AAD 
				get a token 
			provide token to applications
			applications validate the token with AAD 
			authenticate users 
			
			
	

    Use Cases:
        User Authentication: 
			Authenticate users to access Azure resources, applications, and services.
        Access Control: 
			Assign users to roles and grant permissions to access Azure resources.
        Single Sign-On: 
			Provide seamless access to various applications and services.
        Identity Protection: 
			Implement security measures like MFA to protect user accounts.
        Hybrid Identity: 
			Integrate with on-premises Active Directory for a hybrid identity solution.

Role-Based Access Control (RBAC):

    Definition: RBAC is an authorization system used in Azure to manage permissions for Azure resources. It allows you to control who can access and manage resources in Azure by defining roles and assigning them to users or groups.

    Key Features:
        Roles: 
			Azure provides built-in roles (e.g., Owner, Contributor, Reader) that define different levels of access to resources.
        Custom Roles: 
			You can create custom roles with specific permissions tailored to your organization's needs.
        Scope: 
			Roles can be assigned at different scopes, including subscription, resource group, and resource levels.
        Role Assignments: 
			Role assignments associate users, groups, or service principals with roles at a specific scope.

    Use Cases:
        Access Control: 
			RBAC enables fine-grained access control to Azure resources, ensuring that users have the appropriate permissions.
        Delegation: 
			Administrators can delegate specific responsibilities to users or groups without granting excessive permissions.
        Security: 
			RBAC helps enforce the principle of least privilege, minimizing the risk of unauthorized access and resource mismanagement.
        Compliance: 
			RBAC aids in achieving compliance with security and regulatory requirements.

In summary, Azure AD is responsible for user identity management and authentication, while RBAC focuses on controlling access to Azure resources. These two services work in conjunction to provide secure and efficient identity and access management within the Azure cloud environment. Properly configuring Azure AD and RBAC is crucial for ensuring the security and compliance of your Azure resources and applications.


-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up users, assigning roles, and configuring policies.
-------------------------------------------------------------------------------------------------------------------------
Try this: 
Setting up users, assigning roles, and configuring policies in Azure Active Directory (Azure AD) and Role-Based Access Control (RBAC) involves several steps. Here's a step-by-step guide on how to perform these tasks:
Setting Up Users in Azure AD:

    Log in to Azure Portal:
         to the Azure Portal (https://portal.azure.com/) and log in with your Azure AD administrator account.

    Navigate to Azure Active Directory:
        In the left-hand navigation pane, click on "Azure Entra ID." (AAD earlier)

    Create New Users:
		Search for users 
        To create a new user, click on "Users" under the "Manage" section.
        Click the "+ New user" button to add a new user.
        Fill in the user's details, including their username and password.
        Optionally, you can assign licenses and configure group membership.
        Click "Create" to add the user.

Assigning Roles in Azure RBAC:

    Navigate to Azure RBAC:
        In the Azure Portal,  to "Azure Entra ID" >>> "Role and administrators ," in left menu and select it.
		We can create "Custom Roles"

    Assign a Role to a User:
		Go back to "Users" 
			Select the User 
		Click on "Assigned roles" 
        Click on "+ Add assignments" and search for role assignment.
        Select the scope (e.g., subscription, resource group, or specific resource) where you want to assign the role.
        Choose the role you want to assign (e.g., Owner, Contributor, Reader).
        Search for and select the user(s) or group(s) to assign the role.
        Click "Save" to assign the role.

Configuring Policies in Azure AD:

Azure AD allows you to configure Conditional Access policies 
	control access to your applications and resources 
		based on certain conditions. 

Here's how to set up a basic policy:
    Navigate to Conditional Access:
        In the Azure Portal,  to "Azure Active Directory," then select "Security" and finally "Conditional Access."

    Create a New Policy:
        Click on "+ New policy" to create a new Conditional Access policy.
        Give the policy a name and configure the assignment. You can specify users, groups, or all users.

    Assign Access Controls:
        Under the "Access controls" section, specify the conditions for access. For example, you can require multi-factor authentication (MFA) for specific applications or locations.

    Session Policies (Optional):
        You can set session policies to control user sessions, such as session lifetime or persistent browser sessions.

    Enable the Policy:
        Once configured, click "On" to enable the policy.

These are the basic steps to set up users, assign roles, and configure Conditional Access policies in Azure AD and RBAC. Depending on your organization's specific requirements, you can create more complex policies and configure additional settings to enhance security and access control. Always follow security best practices and regularly review and update your access and security policies as needed.


















Azure Policies (Define policy at resource group level to allow/deny traffic)
---------------
	Built-In Policies: Azure provides a set of built-in policies that cover common compliance and security scenarios. You can assign these policies directly to your resources.

	Custom Policies: You can create custom policies to define specific rules and requirements for your organization's resources. Custom policies are written in JSON using Azure Policy definition language.

	Initiative Definitions: An Azure Policy initiative is a set of policies that are grouped together as a single unit to address specific compliance or governance goals. You can create custom initiative definitions.

	Assignments: Policies or initiatives can be assigned at various scopes within your Azure environment:

		Management Group: Policies can be assigned at the management group level to enforce compliance across multiple subscriptions.
		Subscription: You can assign policies to individual Azure subscriptions.
		Resource Group: Policies can be assigned to specific resource groups.
		Resource: Policies can also be assigned directly to specific Azure resources.

	Exclusions: Azure Policies allows you to exclude specific resources from policy enforcement when needed.


Lab:

	Search for Policy (Not Azure policies)
		Dashboard with compliance 
	Getting started (left menu)
		view definitions
			Search for location -> Allowed locations 
				Assign 
			Skip Advanced.
		Parameters: Select the region 
		Remediation: 
			default: azure policies applied only on new resources 
		
	Try to create a vm in a different region 
		attemp will fail.
		
				
			

N.B: Initiative: Group of policies 



Azure Policies encompass various aspects of security and governance within Microsoft Azure. The terms you mentioned refer to different aspects and use cases related to Azure Policies:

    Secure Foundations:
        Definition: Secure foundations in Azure refer to establishing a strong and secure base for your cloud infrastructure. It involves implementing best practices, security controls, and policies to ensure that your Azure environment is securely configured from the ground up.
        Azure Policy: Azure Policies can be used to enforce secure foundations by defining and enforcing rules that align with security best practices. For example, you can create policies that enforce the use of managed disks, enable auditing, or require encryption for storage accounts.

    Zero Trust:
        Definition: Zero Trust is a security framework that assumes that threats exist both outside and inside a network. It requires strict identity verification for anyone trying to access resources in the network, regardless of their location. Zero Trust aims to minimize the risk of data breaches by not trusting any entity by default.
        Azure Policy: Azure Policies can be used to enforce aspects of a Zero Trust security model by defining policies that require strong authentication, role-based access control, and data encryption. Policies can help ensure that only authorized users and applications have access to Azure resources.

    Project Administrator:
        Definition: A project administrator is a role or user responsible for managing specific projects or workloads within an Azure subscription. They have permissions to create, modify, and manage resources related to their projects.
        Azure Policy: Azure Policies can be used to restrict or grant permissions to project administrators by defining policies that control what actions they can perform on Azure resources. For example, you can create policies that restrict the ability to delete specific resource types or enforce naming conventions for resources.

    Emerging Threats:
        Definition: Emerging threats refer to new and evolving cybersecurity risks and vulnerabilities that may not be covered by traditional security measures. These threats often require proactive monitoring and protection.
        Azure Policy: While Azure Policies are primarily used for defining and enforcing configurations and access control, they may indirectly help address emerging threats by ensuring that resources are configured securely. For example, policies that enforce regular software updates, strong authentication, or network security can mitigate the risk of falling victim to emerging threats.

In summary, Azure Policies are a versatile tool for implementing security and governance in Azure. They can be used to enforce secure foundations, align with Zero Trust principles, control permissions for project administrators, and indirectly address emerging threats by ensuring proper configurations and access controls. The specific policies and configurations you implement will depend on your organization's security and compliance requirements.





-------------------------------------------------------------------------------------------------------------------------
				
	4	Storage & Caching
-------------------------------------------------------------------------------------------------------------------------

Storage and caching 
	purposes 
		data persistence
		data retrieval speed optimization
		cost-effective resource management. 
	
Storage in the Cloud:

    Object Storage:
        Definition: 
			designed for storing and managing unstructured data
				files, 
				images, 
				videos, and 
				backups. 
			Data is stored as objects in containers or buckets.
        Examples: 
			AWS S3, 
			Azure Blob Storage, 
			Gogle Cloud Storage.
        Use Cases: Storing and serving media files, backups, static website content, and data archiving.

    Block Storage:
        Definition: 
			Block storage provides raw storage volumes 
			Data stored in blocks 
			Can be attached to virtual machines. 
			It's suitable for 
				file systems, 
				databases, and 
				applications.
        Examples: 
			AWS EBS (Elastic Block Store), 
			Azure Disk Storage, 
			ogle Persistent Disks.
        Use Cases: 
			Hosting databases, 
			application storage, and 
			high-performance I/O workloads.

    File Storage:
        Definition: 
			File storage systems (like NFS) 
				shared file systems that 
				multiple virtual machines can access simultaneously. 
				They are ideal for data sharing among instances.
        Examples: 
			AWS EFS (Elastic File System), 
			Azure File Storage, 
			Gogle Cloud Filestore.
			NFS
        Use Cases: 
			Home directories, 
			application file sharing, and 
			content collaboration.

    Database Storage:
        Definition: 
			Cloud providers offer managed database services 
				scalable and highly available storage for 
					relational and NoSQL databases.
        Examples: 
			AWS RDS (Relational Database Service), 
			Azure SQL Database, 
			ogle Cloud SQL.
        Use Cases: 
			Running and scaling databases in the cloud.

Caching in the Cloud:

    Caching Overview:
        Definition: 
			Caching involves storing frequently accessed data 
				in a high-speed memory location (cache) 
				to improve data retrieval performance and 
				reduce the load on primary data storage.
        Use Cases: 
			Accelerating application performance, 
			reducing database load, and 
			enhancing user experience.

    In-Memory Caching:
        Definition: In-memory caching stores data in RAM for ultra-fast access. It's suitable for caching frequently used database queries or web page components.
        Examples: Redis, Memcached.
        Use Cases: Speeding up read-heavy workloads, session management, and API response caching.

    Content Delivery Network (CDN):
        Definition: CDNs are distributed networks of servers (edge locations) that cache and deliver content 
			(e.g., 
				web pages, 
				images, 
				videos) from locations closer to end-users.
        Examples: AWS CloudFront, Azure CDN, ogle Cloud CDN.
        Use Cases: Reducing latency, improving content delivery speed, and handling traffic spikes.

    Database Query Caching:
        Definition: Database query caching caches the results of database queries, reducing the need to re-run expensive queries.
        Use Cases: Improving database performance and reducing query load.

    API Response Caching:
        Definition: Caching API responses reduces the load on backend servers and improves API response times.
        Use Cases: Enhancing API performance, especially for read-heavy APIs.

Combining cloud storage with caching strategies can significantly enhance the performance, scalability, and cost-effectiveness of applications in the cloud. It's essential to choose the right storage and caching solutions based on your specific use case and performance requirements.

-------------------------------------------------------------------------------------------------------------------------
				
		AWS: S3 (object storage), Elasticache.
-------------------------------------------------------------------------------------------------------------------------

Amazon S3 (Simple Storage Service):

    Definition: 
		Amazon S3 
			scalable and highly durable object storage service 
				advertised as ”infinitely scaling” storage
			allows you to store and retrieve data 
				in the form of objects like 
					files, 
					images, 
					videos, and 
					backups. 
		It provides a simple and cost-effective solution for storing and managing vast amounts of data.


    Key Features:
        Object-Based Storage: 
			S3 stores data as objects in buckets, each with a unique name within a region.
        Durability: 
			Data stored in S3 is redundantly stored across multiple data centers, ensuring high durability.
        Scalability: 
			S3 can scale to handle virtually unlimited amounts of data, making it suitable for applications of all sizes.
        Security: 
			S3 offers access controls, encryption options, and audit logs to secure data.
        Data Lifecycle Management: 
			You can set policies to automate data archiving and deletion.
        Static Website Hosting: 
			S3 can host static websites with ease.
        Integration: 
			S3 integrates with other AWS services and external applications through APIs.

    Use Cases:
        Data Storage: 
			Storing and serving data, including media files, backups, and logs.
        Content Distribution: 
			Hosting content for websites and applications using S3's Content Delivery Network (CDN) capabilities.
        Data Archiving: 
			Archiving data for long-term retention.
        Data Lakes: 
			Building data lakes for data analytics and big data processing.
        Backup and Restore: 
			Storing backups and enabling disaster recovery.
		Disaster Recovery
		Hybrid Cloud storage
		Application hosting
		Media hosting
		Data lakes & big data analytics
		Software delivery
		accessed using http endpoint 


	Amazon S3 - Buckets
		• Amazon S3 
			store objects (files) in “buckets” 
			(like directories)
		• Buckets must have a globally unique name 
			(across 
				all regions 
				all accounts)
		• Buckets are defined at the region level
			
		• S3 looks like a global service 
			but buckets are created in a region
		• Naming convention
			• No uppercase, No underscore
			• 3-63 characters long
			• Not an IP
			• Must start with lowercase letter or number
			• Must NOT start with the prefix xn--
			• Must NOT end with the suffix -s3alias

	Amazon S3 - Objects
		• Objects (files) have a Key
		• The key is the FULL path:
			• s3://my-bucket/my_file.txt
			• s3://my-bucket/my_folder1/another_folder/my_file.txt
		• The key is composed of 
			prefix + object name
			• s3://my-bucket/my_folder1/another_folder/my_file.txt
		• There’s no concept of “directories” within buckets
			(UI will trick you to think otherwise)
		• Just keys with very long names that contain slashes (“/”)
		• Object values are the content of the body:
			• Max. Object Size is 5TB (5000GB)
			• If uploading more than 5GB, must use “multi-part upload”
		• Metadata (list of text key / value pairs – system or user metadata)
		• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
		• Version ID (if versioning is enabled)

	Amazon S3 – Security
		• User-Based
		• IAM Policies 
			which permission/API calls should be allowed for a specific user from IAM
		• Resource-Based
			• Bucket Policies – 
				bucket wide rules from the S3 console - allows cross account
			• Object Access Control List (ACL) – 
				finer grain (can be disabled)
			• Bucket Access Control List (ACL) – 
				less common (can be disabled)

		• Note: an IAM principal can access an S3 object if
			• The user IAM permissions ALLOW it 
				OR 
				resource policy ALLOWS it
			• AND there’s no explicit DENY
		• Encryption: encrypt objects in Amazon S3 using encryption keys



	S3 Bucket Policies
	• JSON based policies
		• Resources: buckets and objects
		• Effect: Allow / Deny
		• Actions: Set of API to Allow or Deny
		• Principal: The account or user to apply the policy to
	• Use S3 bucket for policy to:
		• Grant public access to the bucket
		• Force objects to be encrypted at upload
		• Grant access to another account 



{
    "Version": "2012-10-17",
    "Id": "ExamplePolicy01",
    "Statement": [
        {
            "Sid": "AllPublicRead",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject",
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::awsexamplebucket1/*",
                "arn:aws:s3:::awsexamplebucket1"
            ]
        }
    ]
}


Allowing another account permissions 
{
    "Version": "2012-10-17",
    "Id": "ExamplePolicy01",
    "Statement": [
        {
            "Sid": "AllAnotherUserRead",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": [
                "s3:GetObject",
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::awsexamplebucket1/*",
                "arn:aws:s3:::awsexamplebucket1"
            ]
        }
    ]
}



	Amazon S3 - Versioning
		• You can version your files in Amazon S3
		• It is enabled at the bucket level
		• Same key overwrite will change the “version”: 1, 2, 3….
		• It is best practice to version your buckets
			• Protect against unintended deletes (ability to restore a version)
			• Easy roll back to previous version
		• Notes:
			• Any file that is not versioned prior to enabling versioning will have version “null”
			• Suspending versioning does not delete the previous versions


	Amazon S3 – Replication (CRR & SRR)
		• Must enable Versioning in source and destination buckets
		• Cross-Region Replication (CRR)
		• Same-Region Replication (SRR)
		• Buckets can be in different AWS accounts
		• Copying is asynchronous
		• Must give proper IAM permissions to S3
	• Use cases:
		• CRR – compliance, lower latency access, replication across accounts
		• SRR – log aggregation, live replication between production and test accounts
		• After you enable Replication
			only new objects are replicated
		• Optionally
			replicate existing objects using S3 Batch Replication
			• Replicates existing objects and objects that failed replication
		• For DELETE operations
			• Can replicate delete markers from source to target (optional setting)
			• Deletions with a version ID are not replicated (to avoid malicious deletes)
		• There is no “chaining” of replication
			• If bucket 1 has replication into bucket 2, which has replication into bucket 3
			• Then objects created in bucket 1 are not replicated to bucket 3


	S3 Storage Classes
		Amazon S3 Standard - General Purpose
		Amazon S3 Standard-Infrequent Access (IA)
		Amazon S3 One Zone-Infrequent Access
		Amazon S3 Glacier Instant Retrieval
		Amazon S3 Glacier Flexible Retrieval
		Amazon S3 Glacier Deep Archive
		Amazon S3 Intelligent Tiering
		Can move between classes manually or using S3 Lifecycle configurations

	S3 Durability and Availability
	• Durability:
		• High durability 
			(99.999999999%, 11 9’s) of objects across multiple AZ
			• If you store 1,00,00,000 objects with Amazon S3
				on average expect loose a single object once every 10,000 years
		• Same for all storage classes
	• Availability:
		Measures how readily available a service is
		Varies depending on storage class
		Example: S3 standard has 99.99% availability = not available 53 minutes a year


	S3 Storage Classes – Infrequent Access
		For data that is less frequently accessed, but requires rapid access when needed
		Lower cost than S3 Standard

		Amazon S3 Standard-Infrequent Access (S3 Standard-IA)
			 99.9% Availability
			 Use cases: Disaster Recovery, backups
		Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)
			 High durability (99.999999999%) in a single AZ; data lost when AZ is destroyed
			 99.5% Availability
			 Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate



	Amazon S3 Glacier Storage Classes
		 Low-cost object storage meant for archiving / backup
		 Pricing: price for storage + object retrieval cost
		 Amazon S3 Glacier Instant Retrieval
			• Millisecond retrieval, great for data accessed once a quarter
			• Minimum storage duration of 90 days
		 Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):
			• Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
			• Minimum storage duration of 90 days
		 Amazon S3 Glacier Deep Archive – for long term storage:
			• Standard (12 hours), Bulk (48 hours)
			• Minimum storage duration of 180 days

	S3 Intelligent-Tiering
		 Small monthly monitoring and auto-tiering fee
		 Moves objects automatically between Access Tiers based on usage
		 There are no retrieval charges in S3 Intelligent-Tiering
		 Frequent Access tier (automatic): 
			default tier
		 Infrequent Access tier (automatic): 
			objects not accessed for 30 days
		 Archive Instant Access tier (automatic): 
			objects not accessed for 90 days
		 Archive Access tier (optional): 
			configurable from 90 days to 700+ days
		 Deep Archive Access tier (optional): 
			config. from 180 days to 700+ days







Amazon ElastiCache:
-----------------

    Definition: Amazon ElastiCache 
		managed in-memory caching 
			improve the 
				performance and 
				scalability of applications 
				
				store frequently accessed data in-memory.

    Key Features:
        In-Memory Caching: 
			ElastiCache supports popular open-source in-memory data stores like Redis and Memcached.
        High Availability: 
			It provides automatic failover and backup capabilities for cache clusters.
        Scaling: 
			You can easily scale cache nodes vertically or horizontally to meet your application's requirements.
        Security: 
			ElastiCache offers encryption at rest and in transit, access control, and integration with AWS Identity and Access Management (IAM).
        Monitoring and Metrics: 
			You can monitor cache performance and health using Amazon CloudWatch.

    Use Cases:
        Caching: 
			Caching frequently accessed data to reduce database load and improve application responsiveness.
        Session Management: 
			Storing session data in-memory for stateful applications.
        Real-Time Analytics: 
			Accelerating queries for real-time analytics.
        Content Acceleration: 
			Caching frequently requested content, such as images and API responses.
        Gaming: 
			Storing frequently used game data, such as leaderboards and user profiles.



In summary, Amazon S3 is a versatile and highly durable object storage service designed for data storage and retrieval, while Amazon ElastiCache is a managed caching service that enhances application performance by caching frequently accessed data in-memory. These services can be used together in an architecture to optimize the performance and cost-efficiency of applications hosted in AWS.



Amazon supports 2 types of Elastic Cache 
	Redis 
	Memcached
	
-------------------------------------------------------------------------------------------------------------------------
Feature 											|	Redis						|	Memcached
-------------------------------------------------------------------------------------------------------------------------f
1. (R)Distribute data among many nodes 				|	Yes						|	Yes

2. Keep data on disk with a point in time snapshot	|	Yes						|	Yes

3. Transaction support 								|	Yes						| 	No
	Execute a group of command as 
		isolated and atomic operation 

4. Support for native clustering (refer below)		|	No						| 	Yes

5. Max value/key size 								|	512MB/key				|	1MB/key
6. Value add 										|Advanced data types 		|	Faster than redis. 
													|	(Hashes, Sorted set, 	|	Good for multi-threaded env.
													|		List etc). 			|
7. Multip AZ										|Multy AZ with auto failover| 	Multi- node for partitioning data 
8. Read replicas 									| 	Yes						| 	No
9. HA												| 	Yes 					| 	No 
10. Backup and restore 								| 	Yes						| 	No 

-------------------------------------------------------------------------------------------------------------------------
	
	
Native clustering in Memcached refers to the built-in support for clustering or sharding data across multiple Memcached servers in a distributed system without the need for external libraries or tools. This feature allows Memcached to horizontally scale and distribute the caching workload across a cluster of servers to handle larger datasets and higher request volumes.


Application can be written to 
	Lazy Loading / Cache-Aside / Lazy Population
	Add or Update cache when database is updated
	
N.B: This should be managed in our code.	

	




Create a redis cluser 


https://stackoverflow.com/questions/10558465/memcached-vs-redis
	Why Redis wins over Memcached 
	
Create an ec2 instance
		in same subnet 
		ensure all traffic open with in subnet 
			alternativey open port 6379 
	ubuntu 
	Step 1: Install redis cli 
		sudo apt-get update -y 
		sudo apt-get install redis -y 
		redis-cli -h <server hostname> -p 6379 
	

	#azure wait for the creation to complete. After that console will directly connect to redis.
	
	
	
	
	alternatively in amazon linux follow 
		https://gist.github.com/todgru/14768fb2d8a82ab3f436
	
		
	Step 2: To set a key-value pair, use the SET command:

		SET mykey "Hello, Redis!"
			OK

	Step 3: To retrieve the value associated with a key, use the GET command:


		GET mykey
			"Hello, Redis!"

		You have successfully set and retrieved a value from Redis.
		
		redis> EXISTS mykey
		(integer) 1 (0 if it doesn't exit)
		redis> APPEND mykey "Hello"
		(integer) 5 / 18 (length of the value)
		redis> APPEND mykey " World"
		(integer) 11
		redis> GET mykey
		"Hello World"
		
		
		(b)	incrment a value
			SET mykey "10" (overwrite the previous value)
			INCR mykey
			GET mykey

			SET mykey "Hello"


	Step 4: Working with Other Data Types

	Redis supports various data types beyond simple key-value pairs:

		Lists: 
			You can use the LPUSH and RPUSH commands to add items to lists and LPOP and RPOP to remove items.
				-------------------------------------------------------
				LPUSH (Left Push): Adds one or more elements to the left end of a list.

					LPUSH mylist "item 1"
					LPUSH mylist "item 2"

					This will create a list named mylist 
						add item1 and item2 to its left end. 
						The list will look like this:
							mylist: [item2, item1]

						See it using 
							LRANGE mylist 0 -1

				RPUSH (Right Push): Adds one or more elements to the right end of a list.

					RPUSH mylist "item 3"
					RPUSH mylist "item 4"

					This will add item3 and item4 to the right end of the existing mylist. 
						The list will look like this:

						mylist: [item2, item1, item3, item4]
						See it using 
							LRANGE mylist 0 -1
					
					Push multiple values
						RPUSH mylist "item 5" "item 6"


				LPOP (Left Pop): Removes and returns the leftmost element from the list.

					LPOP mylist

						remove item2 from the left end of the list and return it.
					See it using 
						LRANGE mylist 0 -1
						

				RPOP (Right Pop): Removes and returns the rightmost element from the list.
						RPOP mylist
							remove item4 from the right end of the list and return it.
					See it using 
						LRANGE mylist 0 -1


					After executing these commands, the list mylist will look like this:

					mylist: [item1, item3]

			-------------------------------------------------------

		Sets: 
			Redis sets 
				unordered collections of unique elements. 
				Use commands like SADD to add members and SMEMBERS to retrieve all members of a set.
			-------------------------------------------------------
				Working with Sets:

					SADD: Add one or more members to a set.

				SADD myset "member1"
				SADD myset "member2" "member3"

				SMEMBERS: Retrieve all members of a set.

					SMEMBERS myset

				SISMEMBER: Check if a member exists in a set.

					SISMEMBER myset "member1"
						1 if present 

			-------------------------------------------------------
		Hashes: 
			Redis hashes are maps between string fields and string values. You can use HSET to set a field in a hash and HGET to retrieve its value.
			-------------------------------------------------------
				HSET: Set the field in a hash to a specific value.

				HSET myhash field1 "value1"
				HSET myhash field2 "value2"

				HGET: Retrieve the value associated with a field in a hash.

				HGET myhash field1

				HGETALL: Retrieve all field-value pairs in a hash.

					HGETALL myhash


			-------------------------------------------------------
		Sorted Sets: 
			Sorted sets are similar to sets but each member has a score. You can use commands like ZADD to add members with scores and ZRANGE to retrieve members by their rank.
			-------------------------------------------------------
			ZADD: Add one or more members to a sorted set, each with a score.


				ZADD myzset 1 "member1"
				ZADD myzset 2 "member2" 3 "member3"

				ZRANGE: Retrieve a range of members from a sorted set by rank.

					ZRANGE myzset 0 -1


			-------------------------------------------------------
	Step 5: Working with Expiration
			-------------------------------------------------------
			-------------------------------------------------------
			Redis allows you to set an expiration time for keys:

				
				To set a key with an expiration time (in seconds), use the SETEX command:

				SET myexpkey "My name is Vilas"
				SETEX myexpkey 10 "This will expire in 10 seconds"
					OK

			Wait for 10 seconds or less, and then try to retrieve the value:

				 GET myexpkey
				(nil)

				The key has expired and no longer exists.


		Step 6: Working with Pub/Sub (Publish/Subscribe):

			SUBSCRIBE: Subscribe to one or more channels.

				SUBSCRIBE mychannel

			PUBLISH: Publish a message to a channel.
				From a new terminal 
					PUBLISH mychannel "Hello, subscribers!"

			Key Management:

		Step 7: DEL: Delete one or more keys.
				set key1 value1 
				DEL key1 
				
				key2

		Step 8: EXISTS: Check if a key exists.

				set key1 value1 
				EXISTS key1

			
			KEYS: Get all keys matching a pattern (Caution: This command can be slow with a large dataset and is not recommended for production).

			
				KEYS my*

	
	
		Step 9: Start a Transaction (MULTI): 
		To begin a transaction, use the MULTI command. 
						
					

				MULTI

				Queue Commands: Between the MULTI and EXEC commands, you can queue up multiple Redis commands that you want to execute as part of the transaction. These commands are not executed immediately but are held in a queue until the EXEC command is issued.

				For example, let's say you want to set two keys in a transaction:



				SET key1 "value1"
				SET key2 "value2"

				Execute the Transaction (EXEC): To execute all the queued commands as a single atomic operation, use the EXEC command. This command will return the results of the individual commands within the transaction.



					EXEC

					If all the commands within the transaction are executed successfully, EXEC returns an array of the results of each command. If there is a problem with any of the commands (e.g., a key has been modified by another client during the transaction), EXEC will return a nil value, indicating a failed transaction.

				Here's a step-by-step example of how you can perform a transaction in a Redis CLI:



				# Start the transaction
				MULTI

				# Queue commands within the transaction
				SET key1 "value1"
				SET key2 "value2"

				# ROLLBAK 
					DISCARD
				# Execute the transaction
					EXEC



			Redis transactions and relational database transactions are two different approaches to managing data consistency and concurrency in the context of a database system. While both serve similar purposes, they have significant differences in how they operate:

				Data Model:

					Redis Transaction: Redis is a NoSQL, in-memory data store. Redis transactions operate on data structures like strings, lists, sets, and hashes. Transactions in Redis group multiple commands together and execute them atomically.

					Relational Database Transaction: Relational databases, such as MySQL, PostgreSQL, or Oracle, store data in structured tables with relationships. Transactions in relational databases typically involve SQL statements for data manipulation and querying.

				Atomicity:

					Redis Transaction: Redis transactions follow the "all-or-nothing" principle, meaning that all commands in a Redis transaction are executed atomically. If one command fails, the entire transaction is rolled back.

					Relational Database Transaction: Relational databases also follow the "all-or-nothing" principle. A relational database transaction ensures that a series of SQL statements either all succeed and are committed to the database or all fail and are rolled back.

				Isolation:

					Redis Transaction: Redis transactions do not provide the same level of isolation as traditional relational databases. Redis transactions are typically used for simple operations on individual keys, and there are no isolation levels like "read committed" or "serializable."

					Relational Database Transaction: Relational databases offer various isolation levels (e.g., "read uncommitted," "read committed," "repeatable read," "serializable") to control the visibility of data changes between concurrent transactions. This allows for fine-grained control over isolation and consistency.

				Durability:

					Redis Transaction: Redis offers different levels of durability, but it is primarily an in-memory database. Data may not be persisted immediately, and durability can be traded off for performance.

					Relational Database Transaction: Relational databases are known for their durability. They ensure that committed transactions are safely written to disk, providing strong durability guarantees.

				Use Cases:

					Redis Transaction: Redis transactions are well-suited for use cases where you need fast, atomic operations on individual keys, such as caching, counters, and leaderboard updates. They are less suitable for complex relational data models and multi-table joins.

					Relational Database Transaction: Relational database transactions are suitable for applications with complex data relationships, ACID compliance requirements, and more extensive data manipulation needs. They are commonly used in applications like e-commerce, banking, and enterprise systems.
				

			Step 6: Quit Redis

			To quit the Redis CLI, simply type:

			 QUIT

			This will exit the CLI and stop the Redis client.

			This hands-on session provides a basic introduction to using Redis. Redis offers a wide range of commands and features, making it a versatile tool for caching, real-time analytics, and more in your applications. Explore the Redis documentation for more advanced use cases and features.	

	These commands represent just a fraction of Redis's capabilities. 
	Redis offers a rich set of commands for various data structures and operations


-------------------------------------------------------------------------------------------------------------------------
				
		Azure: Blob Storage, Azure Cache.
-------------------------------------------------------------------------------------------------------------------------

Azure storage types 
	Blob Storage 
		Multiple storage tiers
	File Storage 
	Table Storage
	Queue storage
	Disk storage 


In Microsoft Azure, Blob Storage and Azure Cache are two distinct services that serve different purposes but are essential components for building scalable and performant cloud-based applications. Here's an overview of each service:
Blob Storage (Azure Blob Storage):

    Definition: 
		Azure Blob Storage 
			scalable object storage service 
			designed for storing and managing large volumes of unstructured data like 
				documents
				images
				videos
				backups, and 
				logs.
		Three storage tier  
			HOT
				frequent accessed data
			Cool 
				infrequent accessed data 
					lower availability
					higher durability
			Archive
				rarely access (preferably never)
		Azure Queue 
			for asynchronous processing 
		Azure Table storage 
			Unstructured and structured data 
				no schema
				no fk constraints 
		Azure File storage
			Binary large objects 
			Very similar to Blob storage 
				But Accessed using SMB protocol 
			Map network drive in windows 

    Key Features of Blob:
        Object Storage: Data is stored as blobs in containers, and each blob can be accessed via a unique URL.
        Durability: Blob Storage offers high durability with redundant storage across multiple data centers.
        Scalability: It can handle massive amounts of data and is suitable for applications of all sizes.
        Data Lifecycle Management: Policies can be defined to automate data retention, archiving, and deletion.
        Security: Blob Storage provides access controls, encryption, and integration with Azure Active Directory for identity and access management.
        Integration: It seamlessly integrates with other Azure services and external applications through REST APIs.

    Use Cases:
        Data Storage: Storing and serving large files, media, and backups.
        Content Distribution: Hosting static assets for websites and applications with Azure Content Delivery Network (CDN) integration.
        Data Lakes: Creating data lakes for analytics and big data processing.
        Archiving: Archiving data for long-term retention.
        IoT Data: Storing data generated by IoT devices.

Azure Cache (Azure Cache for Redis):

    Definition: Azure Cache for Redis is a fully managed, highly available, and high-performance in-memory data store service based on the popular open-source Redis cache.

    Key Features:
        In-Memory Caching: Azure Cache for Redis stores data in-memory, allowing for extremely fast data retrieval.
        High Availability: It provides built-in high availability with automatic failover and replication.
        Scaling: You can vertically or horizontally scale your cache to meet your application's requirements.
        Security: It supports encryption at rest and in transit, access control, and virtual network integration.
        Monitoring and Metrics: You can monitor cache performance and health using Azure Monitor and other tools.

    Use Cases:
        Caching: Caching frequently accessed data to reduce database load and improve application responsiveness.
        Session Management: Storing session data in-memory for stateful applications.
        Real-Time Analytics: Accelerating queries for real-time analytics.
        Content Acceleration: Caching frequently requested content, such as API responses and HTML fragments.
        Message Broker: Implementing a message broker for distributed systems using Redis Pub/Sub.

In summary, Azure Blob Storage is a versatile object storage service designed for data storage and retrieval, while Azure Cache for Redis is a managed caching service optimized for in-memory data storage and retrieval. These services can be used together to enhance the performance, scalability, and responsiveness of cloud-based applications hosted in Azure.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Storing and retrieving data from S3/Blob, setting up caching.
-------------------------------------------------------------------------------------------------------------------------
Try this: 
Storing and retrieving data from Amazon S3 (in AWS) or Azure Blob Storage (in Azure) and setting up caching with Amazon ElastiCache (in AWS) or Azure Cache (in Azure) involves several steps. Below, I'll provide a high-level overview of how to perform these tasks:
Storing and Retrieving Data from Amazon S3 (AWS):

Storing Data in Amazon S3:

    Sign in to AWS Console:
         to the AWS Management Console (https://aws.amazon.com/) and sign in with your AWS account.

    Create an S3 Bucket:
        In the AWS Console, navigate to Amazon S3.
        Click "Create bucket" to create a new bucket to store your data.
        Follow the prompts to configure the bucket settings, such as its name, region, and access permissions.

    Upload Data to the S3 Bucket:
        Select your bucket and click "Upload" to upload files or objects to the bucket.
        Follow the prompts to select files from your local machine and upload them to S3.

Retrieving Data from Amazon S3:

    Accessing Data in S3:
        To access data stored in S3, navigate to your bucket in the AWS S3 Console.
        Click on the object (file) you want to retrieve.
        You can use the provided URL to access the object programmatically or share it with others.




Static website hosting in s3 
https://docs.aws.amazon.com/AmazonS3/latest/userguide/HostingWebsiteOnS3Setup.html




Setting Up Caching with Amazon ElastiCache (AWS):

Creating an ElastiCache Cluster:

    Navigate to Amazon ElastiCache:
        In the AWS Console, navigate to Amazon ElastiCache.

    Create an ElastiCache Cluster:
        Click "Create" to create a new ElastiCache cluster.
        Select the Redis or Memcached engine depending on your caching requirements.

    Configure the Cluster:
        Set the cache node type, number of nodes, and other configuration settings.
        Choose the appropriate VPC and security group settings.

    Security and Access Control:
        Configure security group rules to allow access to the cache cluster.
        Optionally, enable encryption at rest and in transit.

    Create the Cluster:
        Review your configuration settings and create the ElastiCache cluster.

Using ElastiCache for Caching:

    Integration with Applications:
        Update your application code to use the ElastiCache endpoint for caching data.

    Cache Data:
        Store frequently accessed data in the cache to improve application performance.

Storing and Retrieving Data from Azure Blob Storage (Azure):

Storing Data in Azure Blob Storage:

    Sign in to Azure Portal:
         to the Azure Portal (https://portal.azure.com/) and sign in with your Azure account.

    Create an Azure Storage Account:
        In the Azure Portal, create a new Azure Storage account to store your data.
        Configure the storage account settings, including its name, region, and access permissions.

    Upload Data to Blob Storage:
        Select your storage account and navigate to "Containers" to create a new container.
        Upload files or objects to the container.

Retrieving Data from Azure Blob Storage:

    Accessing Data in Blob Storage:
        To access data stored in Azure Blob Storage, navigate to your storage account in the Azure Portal.
        Click on the container and object you want to retrieve.
        You can use the provided URL to access the object programmatically or share it with others.

Setting Up Caching with Azure Cache (Azure):

Creating an Azure Cache Instance:

    Navigate to Azure Cache:
        In the Azure Portal, navigate to Azure Cache.

    Create a Cache Instance:
        Click "Create" to create a new cache instance.
        Choose the Redis or Memcached cache type based on your caching requirements.

    Configure the Cache:
        Set the cache size, pricing tier, and other configuration settings.
        Choose the appropriate networking settings and access control options.

    Security and Access Control:
        Configure network security groups and access control to allow access to the cache.

    Create the Cache Instance:
        Review your configuration settings and create the Azure Cache instance.

Using Azure Cache for Caching:

    Integration with Applications:
        Update your application code to use the Azure Cache endpoint for caching data.

    Cache Data:
        Store frequently accessed data in the cache to improve application performance.

Please note that the specific steps and details may vary based on your AWS or Azure environment, the programming language and framework you are using, and your application's architecture. Be sure to refer to the official AWS and Azure documentation for more detailed and up-to-date instructions on using these services.

-------------------------------------------------------------------------------------------------------------------------
				
	5	Databases
-------------------------------------------------------------------------------------------------------------------------



Databases in the cloud, often referred to as cloud databases, are database management systems (DBMS) hosted and operated by cloud service providers. They offer scalable, flexible, and cost-effective solutions for storing, managing, and processing data. Here are key aspects of databases in the cloud:
Types of Cloud Databases:

    Relational Database Management Systems (RDBMS):
        Cloud providers offer managed RDBMS services that support traditional relational databases like MySQL, PostgreSQL, SQL Server, and Oracle.
        Examples: AWS RDS, Azure SQL Database, Google Cloud SQL.

    NoSQL Databases:
        NoSQL databases provide flexible and scalable data models for various use cases, such as document-oriented (MonDB), key-value (Redis), column-family (Cassandra), and graph databases (Neo4j).
        Examples: AWS DynamoDB, Azure Cosmos DB, Google Cloud Firestore.

    In-Memory Databases:
        In-memory databases store data in RAM, providing extremely fast read and write operations.
        Examples: Amazon ElastiCache (Redis), Azure Cache for Redis, ogle Cloud Memorystore.

    Data Warehouses:
        Data warehouses are used for analytical and reporting purposes, often with columnar storage and advanced query optimization.
        Examples: Amazon Redshift, Azure Synapse Analytics, ogle BigQuery.

    Serverless Databases:
        Serverless databases automatically handle scaling and resource management, allowing developers to focus on application logic.
        Examples: AWS Aurora Serverless, Azure SQL Database Serverless, ogle Cloud Firestore in Datastore mode.

Advantages of Cloud Databases:

    Scalability: Cloud databases can easily scale up or down based on demand, ensuring optimal performance and cost efficiency.

    High Availability: Most cloud database services offer built-in high availability, automated backups, and failover mechanisms.

    Managed Services: Cloud providers handle database maintenance tasks like patching, backups, and security, reducing administrative overhead.

    Global Reach: Cloud databases are available in multiple regions, allowing data to be replicated for low-latency access worldwide.

    Security: Cloud providers invest heavily in security measures, including encryption, authentication, and compliance certifications.

    Cost Control: Pay-as-you- pricing models let you control costs by only paying for the resources you use.

    Integration: Cloud databases can be easily integrated with other cloud services, like serverless functions, storage, and analytics.

Use Cases for Cloud Databases:

    Web Applications: 
		Cloud databases are ideal for web apps that require scalable and reliable data storage.

    Mobile Apps: 
		They can serve as the backend for mobile apps, supporting user profiles, content delivery, and real-time data synchronization.

    E-commerce: 
		Cloud databases handle product catalogs, order processing, and customer data for e-commerce platforms.

    Analytics: 
		Data warehousing solutions enable businesses to perform advanced analytics on large datasets.

    IoT: 
		Cloud databases store and analyze data generated by IoT devices.

    Content Management: 
		They manage and serve multimedia content for websites and applications.

    Gaming: 
		Cloud databases support game state, leaderboards, and player profiles for online gaming.

When selecting a cloud database, consider factors like data model, scalability requirements, geographic distribution, and budget. Different cloud providers offer various database services, so evaluate which one best aligns with your specific needs and technical stack.


CREATE TABLE student (
studentid varchar(50) primary key,
studentname varchar(100)
);


adityatrivedi08082001@gmail.com
sandeepkumarv8686@gmail.com
suraj.karthic@verticurl.com


Types of NoSQL databases
	Document-Based Database:
	Key-Value Stores:
	Column Oriented Databases:
	Graph-Based databases:
	Time series database: 


-------------------------------------------------------------------------------------------------------------------------
				
		AWS: RDS, DynamoDB.
-------------------------------------------------------------------------------------------------------------------------

Amazon Web Services (AWS) offers two popular database services: Amazon RDS (Relational Database Service) and Amazon DynamoDB. These services cater to different database needs and use cases. Here's an overview of both:
Amazon RDS (Relational Database Service):

Amazon RDS is a managed relational database service that makes it easier to set up, operate, and scale a relational database in the cloud. It supports several relational database engines, including MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. Key features and use cases include:

    Managed Service: 
		AWS handles administrative tasks such as 
			patching, 
			backups, and 
			scaling, 
				allowing you to focus on your application.

    Multiple Database Engines: 
		Choose from various database engines based on your application's requirements.

    High Availability: 
		RDS offers multi-AZ deployments for automatic failover and data replication to ensure high availability.

    Scalability: 
		You can scale your RDS instances vertically by changing the instance type or horizontally by adding read replicas.

    Security: 
		RDS provides security features like encryption at rest and in transit, IAM authentication, and VPC isolation.

    Backup and Restore: 
		Automated daily backups and database snapshots make it easy to recover data.

    Migration Tools: 
		RDS supports database migration from on-premises to the cloud or between different RDS database engines.

    Use Cases: 
		RDS is suitable for applications requiring structured, relational data, such as content management systems, e-commerce platforms, and line-of-business applications.
		
	
	Automated provisioning, OS patching
	Continuous backups and restore to specific timestamp (Point in Time Restore)!
	Monitoring dashboards
	Read replicas for improved read performance
	Multi AZ setup for DR (Disaster Recovery)
	Maintenance windows for upgrades
	Scaling capability (vertical and horizontal)
	Storage backed by EBS (gp2 or io1)
	BUT you can’t SSH into your instances
	
	Helps you increase storage on your RDS DB instance dynamically
	When RDS detects you are running out of free database storage, it scales automatically
	Avoid manually scaling your database storage
	You have to set Maximum Storage Threshold (maximum limit for DB storage)
	Automatically modify storage if:
		Free storage is less than 10% of allocated storage
		Low-storage lasts at least 5 minutes
		6 hours have passed since last modification
		Useful for applications with unpredictable workloads
		Supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL Server, Oracle)


	RDS Read Replicas for read scalability
		Up to 5 Read Replicas
		Within AZ, Cross AZ or Cross Region
		Replication is ASYNC
			so reads are eventually consistent
		Replicas can be promoted to their own DB
		Applications must update the connection string to leverage read replicas
		
	RDS – From Single-AZ to Multi-AZ
		• Zero downtime operation 
			(no need to stop the DB)
		• Just click on “modify” for the database
		• The following happens internally:
			• A snapshot is taken
			• A new DB is restored from the snapshot in a new AZ
			• Synchronization is established between the two databases


    DynamoDB Streams:
        DynamoDB Streams 
			Amazon DynamoDB feature 
				captures a time-ordered sequence of item-level modifications 
				in a table 
				stores this information in a log-like structure. 
				It enables you to track changes to your data in real-time.
        Whenever an item is added, updated, or deleted in a DynamoDB table, a corresponding event is recorded in the stream. These events can be consumed by various AWS services or custom applications for a wide range of use cases, such as triggering AWS Lambda functions, replicating data across tables or regions, and auditing changes to your data.
        DynamoDB Streams provides an easy way to build real-time applications and workflows that react to changes in your database.

    Point-in-Time Recovery:
        Point-in-Time Recovery (PITR) is a data protection feature in DynamoDB that allows you to restore your table to a specific point in time. 
		It helps you recover data that might have been accidentally deleted or modified.
        With PITR enabled, DynamoDB automatically backs up your table's data to a separate and durable backup store (usually maintained for up to 35 days). You can specify a timestamp or a time range to restore your table to a previous state.
        PITR is crucial for ensuring data resilience and meeting compliance requirements. It gives you the ability to recover your data from various scenarios, including user errors, application bugs, or data corruption.

    Time to Live (TTL):
        Time to Live is a feature in DynamoDB that allows you to automatically delete items from a table when they reach a specified expiration time. TTL is commonly used for managing data that has a limited shelf life, such as session data, temporary caches, or logs.
        Each item in a DynamoDB table can have its TTL attribute, which is a timestamp indicating when the item should be deleted. DynamoDB periodically scans the table for items with expired TTLs and removes them.
        TTL simplifies data lifecycle management, reduces storage costs, and ensures that you don't retain obsolete data in your tables indefinitely.


Amazon DynamoDB:

Amazon DynamoDB is a fully managed NoSQL database service that offers seamless scalability, low-latency data access, and high availability. It is designed for fast and predictable performance with automatic scaling. Key features and use cases include:

    Managed NoSQL Database: DynamoDB abstracts the underlying infrastructure, making it easy to operate and scale NoSQL databases.

    Serverless Option: You can use DynamoDB on-demand to pay only for the read and write capacity you consume.

    Automatic Scaling: DynamoDB automatically scales to handle traffic spikes and adjusts capacity based on demand.

    Low Latency: Single-digit millisecond latency ensures quick data access.

    Global Tables: Replicate data across multiple AWS regions for low-latency access worldwide.

    Security: DynamoDB supports encryption at rest and in transit, fine-grained access control, and integration with AWS Identity and Access Management (IAM).

    Backup and Restore: On-demand and continuous backups with point-in-time recovery.

    Use Cases: DynamoDB is ideal for applications requiring fast, highly available, and scalable NoSQL data storage, such as gaming, mobile apps, real-time analytics, and IoT applications.

Choosing between Amazon RDS and Amazon DynamoDB depends on your specific application requirements. If you need a traditional relational database with structured data, RDS is a od choice. If you require a highly scalable, NoSQL database for unstructured or semi-structured data, DynamoDB is a strong option. Some applications may even use both services in combination to meet different data storage needs within the same architecture.

-------------------------------------------------------------------------------------------------------------------------
				
		Azure: Azure SQL Database, Cosmos DB.
-------------------------------------------------------------------------------------------------------------------------

Data 
	Structured 
	Semi-structured 
		Schema less 
	Unstructured 


Cosmos db
	Globally distributed 
	Regional presence 
		54+ regions 
	HA 
		99.999%
	Elastic scale 
		Can handle 100 million req/sec 
	Low latency 
	Consistency options 
		different options 
	No schema or index management 
	
	Multiple API support 
		NoSQL 
			default
		Cassandra
		MongoDB
		Gremlin 
		Azure Table Storage
	Pay per request units (RU's)
		insert 
		update 
		delete 
		select 
			going through more rows more payment.
			
			

In Microsoft Azure, Azure SQL Database and Azure Cosmos DB are two distinct database services, each designed to address specific data storage and management requirements. Here's an overview of both services:
Azure SQL Database:

Azure SQL Database is a managed relational database service that is part of Microsoft's Azure cloud platform. It's designed to provide a fully managed and scalable database solution for applications that require structured, transactional data storage. Key features and use cases include:

    Managed Service: Azure SQL Database handles many administrative tasks, such as patching, backups, and high availability, allowing developers to focus on application development.

    SQL Server Compatibility: It is based on the SQL Server database engine, providing compatibility with existing SQL Server applications and tools.

    Scalability: Azure SQL Database can scale up or down based on demand, allowing you to adjust resources and performance as needed.

    High Availability: It offers built-in high availability with automatic failover and geo-replication options for disaster recovery.

    Security: Azure SQL Database provides robust security features, including encryption at rest and in transit, threat detection, and Azure Active Directory integration.

    Elastic Pools: You can use elastic pools to manage and share resources among multiple databases, reducing costs and improving resource utilization.

    Use Cases: Azure SQL Database is well-suited for applications that require a traditional relational database model, such as business-critical applications, e-commerce platforms, and line-of-business applications.

Azure Cosmos DB:

Azure Cosmos DB is a globally distributed, multi-model database service that is designed for building highly responsive and scalable applications with low-latency data access. It supports multiple data models, including document, key-value, graph, and column-family. Key features and use cases include:

    Multi-Model Support: Azure Cosmos DB allows you to choose the data model that best fits your application's requirements, making it versatile for a wide range of use cases.

    Global Distribution: Data can be replicated and accessed from multiple Azure regions, ensuring low-latency access for users worldwide.

    Automatic Scaling: Cosmos DB can automatically scale throughput and storage based on workload demands, eliminating the need for manual capacity planning.

    Consistency Models: It offers various consistency models, allowing you to choose between strong, eventual, or session consistency depending on your application's needs.

    Multi-API Support: Azure Cosmos DB supports multiple APIs, including SQL, MonDB, Cassandra, Gremlin, and Table, making it compatible with various development platforms and tools.

    Security: It provides security features such as encryption, role-based access control, and integration with Azure AD for identity management.

    Use Cases: Azure Cosmos DB is suitable for globally distributed applications, real-time analytics, IoT solutions, gaming leaderboards, and any application that requires seamless and low-latency access to data.

Choosing between Azure SQL Database and Azure Cosmos DB depends on the specific requirements of your application. If you need a traditional relational database with structured data, Azure SQL Database is a solid choice. If you require a globally distributed, multi-model database with low-latency access for unstructured or semi-structured data, Azure Cosmos DB is a strong option. In some cases, applications may use both services in a complementary manner within a single architecture.
User


-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Launching a database instance, inserting and querying data.
-------------------------------------------------------------------------------------------------------------------------

Launching a database instance, inserting data, and querying data typically involve specific steps based on the database service you are using. Below, I'll provide a high-level overview of how to perform these tasks using Amazon RDS (AWS) and Azure SQL Database (Azure), which are managed relational database services.
Launching a Database Instance:

Amazon RDS (AWS):

    Sign in to AWS Console:
         to the AWS Management Console (https://aws.amazon.com/) and sign in with your AWS account.

    Navigate to Amazon RDS:
        In the AWS Console, navigate to Amazon RDS.

    Create a DB Instance:
        Click "Create database" to start the instance creation process.
        Choose the database engine (e.g., MySQL).
        Configure instance details such as DB instance size, storage, and security settings.
			attach it to an amazon ec2 instance while creating it.
		
		

    Create the Database:
        Review your configuration settings and click "Create database" to launch the instance.
	Go to the security group and open traffic between them.
		Standard create
		MySQL
		MySQL Community
		Free tier
		Password 
		Storage autoscaling
			Enable storage autoscaling
		Connect to an EC2 compute resource
			create amazon linux instance and attach 
		Choose existing
			subnet group 
				ensure security group and vpc are same
		Password authentication
		No Monitoring 
		
	connect to ec2 instance 
		sudo dnf update -y
		sudo dnf install mariadb105-server
		mysql
		
		mysql -u admin -p -h <db name from rds - e.g. database-1.ca3hd2rqifcx.us-east-2.rds.amazonaws.com>
			-p is for password and not port
			
			

SELECT d.department_name, e.employee_name, e.salary
FROM dept d
INNER JOIN emp e ON d.department_id = e.department_id
WHERE (e.department_id, e.salary) IN (
    SELECT department_id, MAX(salary)
    FROM emp
    GROUP BY department_id
);


    SELECT department_id, MAX(salary) AS max_salary
    FROM emp
    GROUP BY department_id
	
SELECT emp.department_id, MAX(emp.salary), dept.dept_name
FROM emp, dept
WHERE emp.department_id = dept.dept_id
GROUP BY emp.department_id, dept.dept_name;
	

Azure SQL Database (Azure):

    Sign in to Azure Portal:
         to the Azure Portal (https://portal.azure.com/) and sign in with your Azure account.

    Navigate to Azure SQL Database:
        In the Azure Portal, navigate to Azure SQL Database.

    Create a Database:
        Click "Create" to start the database creation process.
        Choose the database engine (Azure SQL Database).
        Configure settings such as server, resource group, and pricing tier.

    Configure Firewall Rules:
        Set up firewall rules to allow access to your database server from specific IP addresses.

    Create the Database:
        Review your configuration settings and click "Create" to create the database.

Inserting Data:

Amazon RDS (AWS):

    Connect to the Database:
        Use a database client or AWS provided tools to connect to your RDS instance.

    Execute SQL Queries:
        Insert data into tables using SQL INSERT statements. For example:

        sql

        INSERT INTO your_table (column1, column2, ...)
        VALUES (value1, value2, ...);

Azure SQL Database (Azure):

    Connect to the Database:
        Use a database client or Azure Data Studio to connect to your Azure SQL Database.

    Execute SQL Queries:
        Insert data into tables using SQL INSERT statements, just like with RDS.

Querying Data:

Amazon RDS (AWS):

    Connect to the Database:
        Use a database client or AWS provided tools to connect to your RDS instance.

    Execute SQL Queries:
        Retrieve data from tables using SQL SELECT statements. For example:

        sql

        SELECT * FROM your_table WHERE condition;

Azure SQL Database (Azure):

    Connect to the Database:
        Use a database client or Azure Data Studio to connect to your Azure SQL Database.

    Execute SQL Queries:
        Retrieve data from tables using SQL SELECT statements, similar to RDS.

These are high-level steps for launching a database instance, inserting data, and querying data. The exact steps and tools you use may vary based on your specific database engine and client preferences. Always refer to the official documentation for your chosen database service for detailed instructions and best practices.




dynamodb
---------
Create a dynamodb instance with fields being 
	id 
	name

Go to PartiQL editor 
INSERT INTO vilastable VALUE 
{
'id':  '1',
'name': 'Vilas' 
};

Run - this would insert a record

Using the ... run a select query like 
SELECT * FROM "vilastable"

https://aws.amazon.com/dynamodb/getting-started/?trk=81ad3c91-d1b1-4a75-90af-b3c48ab4fe43&sc_channel=ps&ef_id=Cj0KCQjwmvSoBhDOARIsAK6aV7i_F_uPTeritZh2K1M_rnHALqGv9f5g6RJyQ-TtVafoMDS0sb0YIsAaAgAtEALw_wcB:G:s&s_kwcid=AL!4422!3!472839475899!p!!g!!dynamodb%20training!11363780097!110903193546

https://aws.amazon.com/tutorials/build-an-application-using-a-no-sql-key-value-data-store/
-------------------------------------------------------------------------------------------------------------------------
				
Week 2		Advanced Cloud Concepts & Introduction to DevOps
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	1	Integration & Messaging
-------------------------------------------------------------------------------------------------------------------------


Cloud messaging 
	communication paradigm 
		leverages cloud computing resources 
			to facilitate the exchange of 
				data, 
				information, and 
				messages 
					between different 
						components, 
						services, and 
						applications. 
	
1. Asynchronous Communication:
	Cloud messaging 
		designed for asynchronous communication
			messages are sent and received independently 
				of each other
			sender and receiver 
				do not need to be actively engaged at the same time.
2. Key Components:

	Messages: 
		Messages 
			contain 
				data or information 
			needs to be 
				exchanged between different 
					components or 
					services 
						within a cloud-based system.

	Publishers: 
		Publishers are entities 
			responsible for sending messages 
				to a message broker 
				or a messaging service within the cloud.

	Message Brokers: 
		Message brokers 
			cloud services or components 
			act as intermediaries
				receive
				store, and 
				distribute messages 
					to the appropriate recipients or subscribers.

	Subscribers: 
		Subscribers 
			entities that 
				receive and 
				process messages 
					published to a message broker. 
		Subscribers can be 
			services, 
			applications, 
			devices, or 
			other cloud resources.

3. Pub-Sub Model:

	Cloud messaging 
		follows 
			Publish-Subscribe (Pub-Sub) model
			publishers send messages 
				to topics or channels
			subscribers 
				express interest in specific topics. 
		model allows 
			efficient broadcasting of messages to multiple subscribers.
4. Messaging Patterns:

	Point-to-Point: 
		In this pattern
			a message is sent 
				from one sender 
					to a specific receiver or queue. 
		Only one receiver processes the message.

	Publish-Subscribe (Pub-Sub): 
		In this pattern
			messages are sent to a topic
		multiple subscribers 
			receive and 
			process the messages independently.

	Request-Reply: 
		sender initiates a request
		receiver responds with a reply message. 
		synchronous communication scenarios.

5. Benefits of Cloud Messaging:

	Decoupling: 
		Cloud messaging decouples the components of a distributed system, allowing them to operate independently and asynchronously. This enhances system flexibility and scalability.

	Reliability: 
		Messages can be reliably delivered to subscribers even in the presence of network failures or system downtime.

	Scalability: 
		Cloud messaging systems are designed to scale horizontally to accommodate increasing message loads, making them suitable for high-throughput scenarios.

	Event-Driven Architecture: 
		Cloud messaging is a fundamental component of event-driven architectures, enabling systems to respond to events or triggers in real time.

	Load Balancing: 
		Messaging systems can distribute workloads across multiple consumers or subscribers, improving system performance and resource utilization.

6. Use Cases:

	Real-Time Notifications: 
		Cloud messaging 
			used to send real-time notifications to users or applications. 
			For example, push notifications to mobile devices or emails for new events.

	Microservices Communication: 
		In microservices architectures, 
			cloud messaging helps different microservices communicate and 
				collaborate asynchronously.

	IoT (Internet of Things): 
		IoT devices use cloud messaging to send and receive data from the cloud, enabling remote monitoring and control.

	Big Data Processing: 
		Messaging systems play a role in distributing data and processing tasks in big data analytics pipelines.

7. Cloud Messaging Services:

	Major cloud providers offer managed messaging services
		Amazon SNS (Simple Notification Service)
		AWS SQS (Simple Queue Service)
		Google Cloud Pub/Sub
		Azure Service Bus
		simplify the implementation of cloud messaging in applications and systems.
	Cloud messaging is a fundamental concept in modern distributed computing, enabling the creation of responsive, scalable, and loosely coupled systems. It plays a crucial role in various applications, from real-time communication to the coordination of distributed services and components in cloud environments.


-------------------------------------------------------------------------------------------------------------------------
				
		AWS: SQS (messaging), SNS (notifications).
-------------------------------------------------------------------------------------------------------------------------


• Synchronous communication between applications 
	can be problematic 
	if there are sudden spikes of traffic
• What if you need to suddenly encode 1000 videos 
	
• better to decouple your applications,
	• using SQS: queue model
	• using SNS: pub/sub model
	• using Kinesis: real-time streaming model
• These services can scale independently from our application!


SQS
---

	Amazon Simple Queue Service (SQS) is a managed messaging service provided by Amazon Web Services (AWS). It is designed to facilitate the decoupling of the components of a cloud application, making it easier to build scalable and fault-tolerant distributed systems. Here's a detailed overview of AWS SQS:

Producer1 														Consumer1
Producer2	--send message-->	SQS Queue  	<--- poll messages	Consumer2
Producer3 														Consumer3

	1. Messaging Model:

		Queue-Based: SQS follows a message queue model, where messages are placed in a queue and processed by consumers in a distributed manner. This decouples the sender and receiver, allowing asynchronous communication.
	2. Key Features:

		Managed Service: 
			AWS SQS 
				fully managed
					AWS handles 
						infrastructure provisioning, 
						scaling, and 
						maintenance
				developers 
					focus on building applications.

		Distributed and Highly Available: 
			SQS 
				distributed across multiple Availability Zones (AZs) 
				high availability and fault tolerance.

		At-Least-Once Delivery: 
			SQS 
				messages are delivered at least once. 
				consumers 
					must be designed to handle duplicate messages.

		Message Retention: 
			Messages are stored in the queue 
				for a configurable retention period (1 minute to 14 days).
					seconds options doesn't work if tried with a low value
			Default: 4 days.
				message can be lost after retention period
				
		Visibility Timeout: 
			When a message is pulled from the queue by a consumer, 
				it becomes invisible for a specified period (visibility timeout). 
				If the consumer doesn't delete or process the message within that time, it becomes visible again for another consumer to pick up.
			

		Low latency 
			<10 ms to publish and receive
		Message size: 
			256KB per message 
		Long Polling: 
			Consumers can use long polling to reduce the number of empty responses and decrease costs.

		Dead-Letter Queues: 
			Messages that cannot be processed successfully after a specified number of retries can be sent to a dead-letter queue (DLQ) for further analysis.

		Security: 
			SQS integrates with AWS Identity and Access Management (IAM) for access control, 
				and messages can be encrypted at rest and in transit.

	3. Types of Queues:

	Standard Queue: 
		Offers high throughput, 
			best-effort ordering (not guaranteed), 
			and at-least-once delivery.

	FIFO Queue (First-In-First-Out): 
		Guarantees that messages are delivered 
			exactly once and in the order they were sent. 
		Suitable for applications that require strict message sequencing.

• Oldest offering 
	(over 10 years old)
• Fully managed service
	decouple applications
• Unlimited 
	throughput
	number of messages in queue
• Default retention of messages: 
	4 days, maximum of 14 days min 1 min
• Low latency (<10 ms on publish and receive)
• Limitation of 256KB per message sent
• Can have duplicate messages 
	(at least once delivery, occasionally)
• Can have out of order messages 
	(best effort ordering)
	use FIFO to remove this

Producer 

	• Produced to SQS 
		using 
			SDK (SendMessage API)
			other aws services 
				also use sdk internally
	• The message is persisted in SQS 
		until 
			a consumer deletes it
			or 
			• Message retention: 
				default 4 days
				min: 1 min.
				max: 14 days
	• SQS standard: unlimited throughput

SQS – Consuming Messages
	• Consumers (running on EC2 instances, servers, or AWS Lambda)…
	• Poll SQS for messages (receive up to 10 messages at a time)
	• Process the messages (example: insert the message into an RDS database)
	• Delete the messages using the DeleteMessage API

SQS/SNS with Auto Scaling Group (ASG)
	SQS/SNS 
		Cloudwatch metrics 
			ApproximateNumberOfMessages event 
			Raises cloudwatch alarm 
				increases/reduces instances in ASG.
		Number of messages reduces/increases.
		
Amazon SQS - Security
• Encryption:
	• In-flight encryption using HTTPS API
	• At-rest encryption using KMS keys
	• Client-side encryption if the client wants to perform encryption/decryption itself
• Access Controls: IAM policies to regulate access to the SQS API
• SQS Access Policies (similar to S3 bucket policies)
	• Useful for 
		cross-account access to SQS queues
		allowing other services (SNS, S3…) to write to an SQS queue		
		automated access management 
		public access
		temporary access
		
SQS – Message Visibility Timeout
	• After a message is polled by a consumer, it becomes invisible to other consumers
	• By default, the “message visibility timeout” is 30 seconds
	• That means the message has 30 seconds to be processed
	• After the message visibility timeout is over, the message is “visible” in SQS
		

SQS – Message Visibility Timeout
	• If a message is not processed within the visibility timeout, it will be processed twice
	• A consumer could call the ChangeMessageVisibility API to get more time
	• If visibility timeout is high (hours), and consumer crashes, re-processing will take time
	• If visibility timeout is too low (seconds), we may get duplicates				


Amazon SQS – FIFO Queue
	• FIFO = First In First Out (ordering of messages in the queue)
	• Limited throughput: 300 msg/s without batching, 3000 msg/s with
	• Exactly-once send capability (by removing duplicates)
	• Messages are processed in order by the consumer

Amazon SQS – Dead Letter Queue (DLQ)
	• If a consumer fails to process a message within the
		Visibility Timeout…
		the message goes back to the queue!

	• We can set a threshold of how many times a message can go back to the queue
	• After the MaximumReceives threshold is exceeded, the message goes into a Dead Letter Queue (DLQ)
	• Useful for debugging!
	• DLQ of a FIFO queue must also be a FIFO queue
	• DLQ of a Standard queue must also be a Standard queue
	• Make sure to process the messages in the DLQ before they expire:
	• Good to set a retention of 14 days in the DLQ


SQS DLQ – Redrive to Source
	• Feature to help consume messages in the DLQ to understand what is wrong with them
	• When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom code

Amazon SQS – Delay Queue
	• Delay a message (consumers don’t see it immediately) up to 15 minutes
	• Default is 0 seconds (message is available right away)
	• Can set a default at queue level
	• Can override the default on send using the DelaySeconds parameter

Amazon SQS - Long Polling
	• When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue
	• This is called Long Polling
	• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application.
	• The wait time can be between 1 sec to 20 sec (20 sec preferable)
	• Long Polling is preferable to Short Polling
	• Long polling can be enabled at the queue level or at the API level using
		ReceiveMessageWaitTimeSeconds

SQS Extended Client
	• Message size limit is 256KB, how to send large messages, e.g. 1GB?
	• Using the SQS Extended Client (Java Library)

SQS – Must know API
	• CreateQueue (MessageRetentionPeriod), DeleteQueue
	• PurgeQueue: delete all the messages in queue
	• SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage
	• MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API)
	• ReceiveMessageWaitTimeSeconds: Long Polling
	• ChangeMessageVisibility: change the message timeout
	• Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your costs

SQS FIFO – Deduplication
	• De-duplication interval is 5 minutes
	• Two de-duplication methods:
	• Content-based deduplication: will do a SHA-256 hash of the message body
	• Explicitly provide a Message Deduplication ID

SQS FIFO – Message Grouping
	• If you specify the same value of MessageGroupID in an SQS FIFO queue,
	you can only have one consumer, and all the messages are in order
	• To get ordering at the level of a subset of messages, specify different values
	for MessageGroupID
	• Messages that share a common Message Group ID will be in order within the group
	• Each Group ID can have a different consumer (parallel processing!)
	• Ordering across groups is not guaranteed



	4. Use Cases:

	Decoupling Components: SQS is used to decouple components of a distributed system, allowing them to operate independently and asynchronously.

	Scalable Workload Processing: SQS is commonly used for processing workloads that can be parallelized, such as batch processing and data processing.

	Event-Driven Architecture: SQS plays a crucial role in event-driven architectures, where it helps manage events and messages between microservices.

	Asynchronous Communication: It is used for asynchronous communication between distributed components, improving system resilience and performance.

	5. Integration:

	SQS can be easily integrated with other AWS services, such as AWS Lambda, Amazon EC2, and Amazon S3, as well as with custom applications.
	6. Pricing:

	SQS charges based on the number of requests (sends, receives, deletes, etc.) and the amount of data transferred in and out of the service.
	Amazon SQS is a reliable and highly scalable messaging service that plays a crucial role in building distributed and decoupled cloud applications. It simplifies the development of scalable and fault-tolerant systems by handling the complexities of message management and delivery


	what is deduplication id?
	


SNS
---

	Amazon Simple Notification Service (SNS) is a fully managed messaging service provided by Amazon Web Services (AWS). SNS is designed to facilitate the publication and distribution of messages or notifications to a large number of recipients or subscribers. It is a key building block for creating event-driven and distributed systems. Here's a detailed overview of AWS SNS:

	1. Messaging Model:

	Publish-Subscribe (Pub-Sub): SNS follows a pub-sub messaging model, where publishers send messages to topics, and subscribers (including AWS services and custom applications) receive those messages.
	2. Key Features:

	Managed Service: AWS SNS is a fully managed service, which means AWS handles the infrastructure, scalability, and availability, allowing developers to focus on building applications.

	Distributed and Highly Available: SNS is distributed across multiple Availability Zones (AZs) within a region for high availability and fault tolerance.

	Multiple Protocols: SNS supports various protocols for message delivery, including HTTP/HTTPS, email/SMTP, SMS, application endpoints (for mobile apps), and more.

	Fan-Out Messaging: SNS allows one message to be sent to multiple subscribers simultaneously, enabling fan-out messaging for broadcasting events.

	Message Filtering: SNS offers message filtering capabilities based on attributes, allowing subscribers to receive only the messages that match their criteria.

	Dead-Letter Queue (DLQ): You can configure a DLQ for topics to capture and retain messages that cannot be successfully delivered to subscribers.

	Security: SNS integrates with AWS Identity and Access Management (IAM) for access control, and messages can be encrypted at rest and in transit.

	3. Components:

	Topic: A topic is a communication channel to which messages can be published. Subscribers subscribe to topics to receive messages. Topics can have one or more subscribers.

	Publisher: Publishers are entities that send messages to topics. Publishers can be AWS services, mobile apps, or custom applications.

	Subscriber: Subscribers are endpoints that receive messages published to topics. Subscribers can be AWS Lambda functions, SQS queues, HTTP/HTTPS endpoints, mobile devices (for push notifications), and more.



	4. Use Cases:

	Event-Driven Architectures: SNS is commonly used to implement event-driven architectures where events or messages trigger actions in various components of a system.

	Real-Time Notifications: SNS is used for sending real-time notifications to users or systems when certain events occur, such as new emails or system alerts.

	Decoupling Systems: SNS helps decouple the components of a distributed system, allowing them to operate independently and asynchronously.

	Scalable and Responsive Applications: It is used for building applications that need to scale horizontally and react to changes or events in real time.

	5. Integration:

	SNS can be easily integrated with other AWS services, including AWS Lambda, Amazon SQS, Amazon S3, and more.
	6. Pricing:

	SNS pricing is based on the number of messages published, delivered, and the use of other features like message filtering and SMS deliveries.
	Amazon SNS is a versatile and highly scalable messaging service that simplifies the process of sending messages and notifications to distributed components, applications, and users. It is a crucial building block for building event-driven and responsive cloud applications and systems.



• The “event producer” 
	only sends message to one SNS topic
• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications
• Each subscriber to the topic will get all the messages (note: new feature to filter messages)
• Up to 1,25,00,000 subscriptions per topic
• 100,000 topics limit

Many AWS services can send data directly to SNS for notifications
	CloudWatch Alarms
	Auto Scaling Group
	CloudFormation
	AWS Budgets Lambda
	AWS DMS
	DynamoDB
	RDS Events	


Amazon SNS – How to publish
	• Topic Publish (using the SDK)
		• Create a topic
		• Create a subscription (or many)
		• Publish to the topic
	• Direct Publish (for mobile apps SDK)
		• call api 
		
Amazon SNS – Security
• Encryption:
	• In-flight encryption using HTTPS API
	• At-rest encryption using KMS keys
	• Client-side encryption if the client wants to perform encryption/decryption itself
• Access Controls: IAM policies to regulate access to the SNS API
• SNS Access Policies (similar to S3 bucket policies)
	• Useful for cross-account access to SNS topics
	• Useful for allowing other services ( S3…) to write to an SNS topic		


SNS + SQS: Fan Out
	• Push once in SNS, receive in all SQS queues that are subscribers
	• Fully decoupled, no data loss
	• SQS allows for: data persistence, delayed processing and retries of work
	• Ability to add more SQS subscribers over time
	• Make sure your SQS queue access policy allows for SNS to write
	• Cross-Region Delivery: works with SQS Queues in other regions


Application: S3 Events to multiple queues
	• For the same combination of: event type (e.g. object create) and prefix
	(e.g. images/) you can only have one S3 Event rule
	• If you want to send the same S3 event to many SQS queues, use fan-out		


Amazon SNS – FIFO Topic
• FIFO = First In First Out (ordering of messages in the topic)

	• Similar features as SQS FIFO:
		• Ordering by Message Group ID (all messages in the same group are ordered)
		• Deduplication using a Deduplication ID or Content Based Deduplication
	• Can only have SQS FIFO queues as subscribers
	• Limited throughput (same throughput as SQS FIFO)


SNS – Message Filtering
	• JSON policy used to filter messages sent to SNS topic’s subscriptions
	• If a subscription doesn’t have a filter policy, it receives every message



    Number of retries: 
		This setting is configured to "3." 
		maximum number of times SNS 
			will attempt to deliver a message 
				to a subscribed endpoint 
					before giving up. 
		In this case, SNS will try up to three times to deliver the message.

    Retries without delay: 
		This setting is configured to "0." 
		SNS will not perform any immediate retries 
			without introducing any delay. 
		If a message delivery fails, 
			SNS will wait before attempting a retry, 
			as defined by the next settings.

    Minimum delay: 
		This setting is configured to "20 seconds." 
		if a message delivery fails
			SNS will wait a minimum of 20 
				seconds before attempting the first retry.

    Maximum delay: 
		configured to "20 seconds." 
		It sets the upper limit on the delay between retries. 
		Even if the initial delay is longer 
			(e.g., due to other factors), 
			SNS will not wait longer than 20 seconds between retries.

    Minimum delay retries: 
		This setting is configured to "0." 
		that there are no retries with the minimum delay of 20 seconds. 
		In other words, the first retry will be performed after 20 seconds.

    Maximum delay retries: 
		configured to "0" as well. 
		no retries with the maximum delay of 20 seconds. 
		Subsequent retries will also occur after 20 seconds.

    Maximum receive rate: 
		The value is set as a dash ("-"), 
		no specified maximum receive rate. 
		This setting is used when you want to limit 
			the rate at which messages are delivered 
			to a particular endpoint. 
			In this case, no rate limit is specified.

    Content-Type: This setting specifies the content type for the delivered messages. The content type is set to "text/plain; charset=UTF-8," indicating that the messages will be in plain text format with UTF-8 character encoding.

    Retry-backoff function: The setting is configured as "Linear." This means that the delay between retry attempts will be constant (in this case, 20 seconds), as opposed to an exponential backoff where the delay increases with each retry.

    Override subscription policy: This setting indicates whether the delivery policy overrides any subscription policies set on individual subscriptions. If this is set to "true," the delivery policy will take precedence over subscription policies.

Overall, this delivery policy ensures that SNS will attempt to deliver messages up to three times, with a 20-second delay between each retry. It uses a linear retry-backoff function, and there are no immediate retries without delay. The content type for the messages is specified, and the policy does not limit the receive rate. Subscription policies may be overridden by this delivery policy if the "Override subscription policy" setting is set to "true."

-------------------------------------------------------------------------------------------------------------------------
				
		Azure: Azure Queue Storage, Azure Service Bus.
-------------------------------------------------------------------------------------------------------------------------
Azure Queue Storage and Azure Service Bus are both messaging services offered by Microsoft Azure, but they serve slightly different purposes and are designed for different use cases. Here's an overview of each service:

    Azure Queue Storage:
        Type: Azure Queue Storage is a part of Azure Storage Services and is designed as a simple message queue service.
        Use Case: It is primarily used for asynchronous communication between different components of an application. Messages are sent to a queue and can be processed by one or more consumers.
        Features:
            FIFO (First-In-First-Out) delivery of messages.
            At-least-once message delivery guarantee (a message may be delivered more than once, but it won't be lost).
            Scalable and cost-effective for handling large volumes of messages.
            Limited support for publish-subscribe patterns (through shared queues).
        Scenarios: Commonly used for decoupling application components, background job processing, and handling event-driven scenarios.

    Azure Service Bus:
        Type: Azure Service Bus is a fully-featured messaging service that provides both message queuing and publish-subscribe (pub-sub) messaging patterns.
        Use Case: It is used for building more complex messaging solutions where you need features like pub-sub, advanced message routing, and transactions.
        Features:
            Supports queuing and publish-subscribe patterns.
            Advanced message filtering and routing capabilities.
            Supports transactions for atomic operations.
            Dead-letter queues for handling undeliverable or expired messages.
            Scheduled messages for future delivery.
            Sessions for handling related messages together.
            Express and Premium tiers with varying levels of performance and features.
        Scenarios: Used in scenarios where more advanced messaging patterns and features are required, such as order processing, IoT telemetry, and financial services.

In summary, if you need a simple, cost-effective message queue for basic communication between components of an application, Azure Queue Storage is a od choice. However, if you require more advanced messaging features like publish-subscribe, advanced routing, and transactions, then Azure Service Bus is the better option.

The choice between the two services depends on the specific requirements of your application and the messaging patterns you need to implement. It's also worth noting that Azure offers other messaging services like Azure Event Hubs and Azure IoT Hub, which are designed for specific use cases such as event streaming and IoT device communication.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up messaging queues and sending/receiving messages.
	-------------------------------------------------------------------------------------------------------------------------
	
	Setting up messaging queues and sending/receiving messages typically involves using a cloud service like Azure Queue Storage or Azure Service Bus. Below, I'll provide you with a step-by-step guide on how to set up a simple queue and send/receive messages using Azure Queue Storage as an example. You can adapt a similar process for Azure Service Bus or other messaging services.

Prerequisites:

    An Azure account (you can sign up for a free trial if you don't have one).
    Azure SDK or Azure Portal access.

Step 1: Create an Azure Queue Storage

    Log in to the Azure Portal (https://portal.azure.com/).

    Click on "+ Create a resource" on the left sidebar.

    Search for "Queue Storage" and select it.

    Click the "Create" button.

    Fill in the necessary information like the storage account name, resource group, location, etc.

    Click "Review + create," and then "Create" to create the storage account.

Step 2: Access Your Queue

    Once the storage account is created, navigate to it.

    In the storage account,  to "Queues" under the "Services" section.

    Click the "+ Queue" button to create a new queue. Give it a name, e.g., "myqueue."

Step 3: Sending Messages

You can send messages to the queue programmatically using an SDK or Azure CLI. Here's an example using Azure CLI:

    Install Azure CLI if you haven't already.

    Open a terminal and use the following command to send a message to the queue:

    

    az storage message put --queue-name myqueue --content "Hello, Queue!"

Step 4: Receiving Messages

You can receive messages from the queue programmatically as well. Here's an example using Azure CLI:

    Use the following command to receive a message from the queue:


    az storage message peek --queue-name myqueue

    This command will retrieve a message from the queue without removing it. To remove it from the queue, you would use the az storage message get command.

    To process messages in your application, you would typically use an SDK in your preferred programming language to connect to the queue and retrieve messages. The process would involve:
        Authenticating to your Azure account.
        Connecting to the queue.
        Receiving and processing messages.
        Deleting messages after processing them to ensure they are not processed again.

Remember to handle exceptions and implement proper error handling in your application.

This is a basic example to get you started with setting up messaging queues and sending/receiving messages using Azure Queue Storage. More complex scenarios, such as handling multiple consumers, implementing retry mechanisms, and managing message visibility, would require additional considerations and code in your application.
	
	
Azure queue tutorial 	
https://learn.microsoft.com/en-us/azure/storage/queues/storage-quickstart-queues-java?tabs=powershell%2Cpasswordless%2Croles-azure-portal%2Cenvironment-variable-windows%2Csign-in-azure-cli	

Azure service bus tutorial 
https://learn.microsoft.com/en-us/java/api/overview/azure/messaging-servicebus-readme?view=azure-java-stable
-------------------------------------------------------------------------------------------------------------------------
			
	2	DevOps and Agile
-------------------------------------------------------------------------------------------------------------------------

DevOps and Agile are two complementary approaches to software development and IT operations that aim to improve the efficiency, speed, and quality of software delivery. While they have distinct principles and practices, they share common als of collaboration, flexibility, and customer-centricity. Here's a detailed overview of each:

Agile:

    Philosophy and Principles:
        Agile is a software development methodology that emphasizes collaboration, customer feedback, and iterative development.
        It follows the Agile Manifesto, which includes principles such as valuing individuals and interactions, working software, customer collaboration, and responding to change.

    Key Practices:
        Scrum: A popular Agile framework that divides work into time-boxed iterations called "sprints." It includes roles (Scrum Master, Product Owner, Development Team), artifacts (Product Backlog, Sprint Backlog), and ceremonies (Daily Standup, Sprint Review).
        Kanban: A visual management method that focuses on visualizing work, limiting work in progress, and continuous flow. It's often used in support and maintenance teams.
        Extreme Programming (XP): An Agile methodology that emphasizes practices like test-driven development, continuous integration, pair programming, and frequent releases.

    Benefits:
        Rapid delivery of incremental value to customers.
        Continuous improvement through feedback loops.
        Adaptability to changing requirements.
        Increased collaboration and transparency.

    Challenges:
        Requires a cultural shift within organizations.
        May not be suitable for all types of projects.
        Can be challenging to scale in large organizations.

DevOps:

    Philosophy and Principles:
        DevOps is a set of practices and cultural philosophies that aim to automate and integrate the processes of software development and IT operations.
        It emphasizes breaking down silos between development and operations teams to enable faster and more reliable software delivery.

    Key Practices:
        Automation: The automation of repetitive tasks in areas such as code deployment, configuration management, and testing.
        Collaboration: Promotes collaboration between development, operations, and other relevant teams to ensure the entire software delivery process runs smoothly.
        Continuous Integration/Continuous Deployment (CI/CD): The practice of automatically building, testing, and deploying code changes in a streamlined and efficient manner.
        Monitoring and Feedback: Continuous monitoring of applications and infrastructure to identify issues early and receive feedback for improvement.

    Benefits:
        Faster time-to-market for software.
        Improved quality and reliability of software.
        Increased collaboration and communication between teams.
        Reduced manual errors and downtime.

    Challenges:
        Requires significant cultural and organizational change.
        Complexity in implementing automation and CI/CD pipelines.
        Security and compliance considerations need to be addressed.

Integration of Agile and DevOps:

Agile and DevOps often work hand in hand to create a seamless software development and delivery process. Agile focuses on the development phase, delivering customer value in iterations, while DevOps focuses on the end-to-end delivery pipeline, automating processes from code commit to production deployment. Together, they enable organizations to deliver high-quality software quickly and respond to customer feedback effectively.

In summary, Agile and DevOps are complementary approaches that help organizations become more agile, customer-centric, and efficient in delivering software solutions. While they have distinct principles and practices, their integration can lead to significant improvements in the software development and delivery lifecycle.

-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to DevOps: Principles, lifecycle, benefits.
-------------------------------------------------------------------------------------------------------------------------

Introduction to DevOps:

DevOps is a set of practices, principles, and cultural philosophies that aims to automate and integrate the processes of software development (Dev) and IT operations (Ops). It seeks to create a collaborative and efficient environment where development and operations teams work together seamlessly to deliver high-quality software more rapidly and reliably. DevOps addresses the traditional barriers and silos between these two functions, promoting a culture of shared responsibility and continuous improvement.

Principles of DevOps:

    Automation: DevOps promotes the automation of repetitive tasks, such as code builds, testing, deployment, and infrastructure provisioning. Automation reduces manual errors, accelerates processes, and ensures consistency.

    Collaboration: DevOps emphasizes breaking down silos between development, operations, and other relevant teams. Collaboration fosters shared als and responsibilities, leading to faster problem resolution and better outcomes.

    Continuous Integration (CI): CI involves automatically integrating code changes into a shared repository multiple times a day. It ensures that code changes are tested early and frequently, reducing integration issues.

    Continuous Deployment (CD): CD extends CI by automatically deploying code changes to production or staging environments after passing tests. It enables rapid and reliable releases.

    Monitoring and Feedback: DevOps promotes continuous monitoring of applications and infrastructure. Monitoring provides valuable feedback on system performance and issues, allowing teams to respond promptly and make data-driven improvements.

    Infrastructure as Code (IaC): IaC treats infrastructure provisioning and management as code. It enables automated, consistent, and version-controlled infrastructure setups, reducing configuration drift and promoting scalability.

DevOps Lifecycle:

The DevOps lifecycle consists of several key stages:

    Plan: In this phase, teams define als, requirements, and strategies for software development and delivery. They prioritize features and plan the work ahead.

    Code: Developers write, test, and integrate code changes into a shared repository. CI tools automatically build and test the code.

    Build: Automated build tools compile and package code into deployable artifacts. These artifacts are versioned and stored for later deployment.

    Test: Automated testing tools validate code changes, ensuring they meet quality and functionality requirements. Continuous testing identifies and reports issues early.

    Deploy: CD pipelines automate the deployment of code changes to various environments, including staging and production. Deployments are consistent and reliable.

    Operate: Operations teams monitor application and infrastructure performance in production environments. They respond to incidents, optimize resource usage, and ensure system availability.

    Monitor: Continuous monitoring tools track application and system metrics, providing real-time feedback. Teams use this data to identify issues, plan improvements, and maintain system health.

Benefits of DevOps:

    Faster Time-to-Market: DevOps accelerates the software development and delivery process, enabling organizations to release new features and updates more rapidly.

    Improved Quality: Automation, continuous testing, and monitoring result in higher-quality software with fewer defects and vulnerabilities.

    Enhanced Collaboration: DevOps fosters collaboration between development, operations, and other teams, reducing communication barriers and promoting shared responsibility.

    Increased Efficiency: Automation reduces manual and repetitive tasks, increasing efficiency and reducing the risk of human errors.

    Greater Reliability: Continuous monitoring and feedback help identify and address issues proactively, leading to more reliable and available systems.

    Cost Savings: By automating processes and optimizing resource usage, DevOps can lead to cost savings in the long run.

    Competitive Advantage: Organizations that adopt DevOps can respond to market changes and customer demands more swiftly, gaining a competitive edge.

In conclusion, DevOps is a cultural and technical approach that streamlines software development and operations, promoting automation, collaboration, and continuous improvement. Its principles and practices enable organizations to deliver software faster, with higher quality, and improved reliability.

-------------------------------------------------------------------------------------------------------------------------
				
		Overview of Agile methodology: Scrum, Kanban.
-------------------------------------------------------------------------------------------------------------------------
How Does Agile Work? 

Here is the working of Agile - 

    Define the project: 
		The team, along with the customer, defines the project's goals, objectives, and requirements.
		start defining backlog 
			stories (high level and low level)
    Create a backlog: 
		A backlog is a prioritized list of tasks that need to be completed. The customer, product owner, and the team work together to create the backlog.
    Plan the sprint: 
		The team plans the sprint by selecting the highest-priority tasks from the backlog and determining how much work can be completed in the upcoming sprint.
    Execute the sprint: 
		The team works on completing the tasks planned for the sprint, with daily meetings to check progress and address any issues.
    Review and demo: 
		At the end of the sprint, the team demonstrates the completed work to the customer and gets feedback.
    Retrospect: 
		The team retrospects on the sprint, discussing what went well, what didn't, and what can be improved for the next sprint.
    Repeat: 
		The process is repeated for each sprint until the project is completed. The product is incrementally developed and delivered to the customer in small chunks.
    Continuously improve: 
		Agile methodologies focus on continuous improvement. The team reflects on its progress and makes adjustments as necessary to improve processes, tools, and communication for the next sprint.




https://www.atlassian.com/agile/scrum/sprints


Agile is a software development approach that emphasizes iterative and incremental development, flexibility, collaboration, and customer feedback. Two popular Agile methodologies are Scrum and Kanban. Let's provide a detailed overview of both methodologies, highlighting their similarities and differences:


Sprint 
	iterative and 
	timeboxed 
		learning cycle composed of five

1. Refinement - 
	The routine of clearly expressing, prioritizing, and breaking down
	objectives into smaller achievable chunks. This can happen at any time within
	the Sprint to prepare for future sprints.
2. Planning - 
	The routine of determining what will be achieved by the end of the
	Sprint and how the work will be accomplished.
3. Check-In - 
	Short and frequent conversations around the learning to align,
	check progress, adapt, address impediments, and provide support.
4. Review - 
	The process of assessing, validating, and providing feedback
	on the learning in the Sprint.
5. Retrospective - 
	The process of reflecting on the Sprint and identifying
	actionable commitments to improve how we learn and collaborate.




Scrum:

1. Overview:

    Scrum 
		an Agile framework 
		provides a structured approach to software development.
    organizes work 
		into time-boxed iterations 
		called "sprints," 
		usually lasting 2-4 weeks.
    Scrum is characterized by 
		roles, 
		events (ceremonies), and 
		artifacts.

2. Roles:

    Scrum Master: 
		Responsible for facilitating 
			Scrum events
			removing impediments
			ensure the team follows Scrum principles.
    Product Owner: 
		Represents the customer's interests
		manages the product backlog
		sets priorities.
    Development Team: 
		Cross-functional team members responsible for delivering the work.

3. Ceremonies:

    Sprint Planning: 
		The team plans the work to be done during the sprint.
		define current sprint goal 
			
    Daily Scrum: 
		A daily stand-up meeting 
			team members discuss progress
			plan for the day
			identify blockers.
    Sprint Review: 
		A meeting at the end of each sprint 
			team demonstrates the completed work to stakeholders.
    Sprint Retrospective: 
		A reflective meeting 
			end of each sprint  
			identify areas for improvement.

4. Artifacts:

    Product Backlog: 
		A prioritized list of 
			features, 
			enhancements, and 
			bug fixes.
    Sprint Backlog: 
		A subset of the product backlog 
			team commits to completing during the sprint.
    Increment: 
		The product increment 
			potentially shippable product increment 
			created during a sprint.


Agile Principles

	https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/

To make a process Agile, the following principles need to be satisfied.

    Iterative Development: Agile projects are divided into short cycles called iterations, where a small portion of the project is developed and delivered. This approach allows teams to make regular improvements and gather feedback along the way.
    Customer Collaboration: Regular interaction with customers and stakeholders ensures that the project aligns with their needs and expectations. This dynamic collaboration helps in refining requirements and adjusting priorities.
    Adaptive Planning: Instead of rigid planning upfront, Agile embraces changing requirements and adapts its plans accordingly. This allows teams to respond effectively to evolving circumstances.
    Continuous Feedback: Agile thrives on frequent feedback loops. Regular reviews and retrospectives help teams identify areas for improvement, establishing a culture of continuous learning.
    Empowered Teams: Agile empowers cross-functional teams with the autonomy to make decisions. This autonomy enhances accountability, innovation, and ownership of the project's success.


Exercise
	1. Product Backlog 
		a. Login
		b. Customer can buy 
		c. Make payment 
		d. Admin can make offers
		e. Partners can make offers
		f. Track where my product is?
		g. Search for products.
		h. Partners can modify the specification 
		i. Different language 
		j. 

	2. Spring backlog
		a. Login view page 
			identify which framework 
			build wireframe
			study 
			
			
			out of scope: MFA
			
		b. Create account page / Sign up 
		c. integrating authentication with external products 
		d. Storing the data 
			i. authentication 
			ii.  authorization  
				change password 
			ii. which database?
			iv. database schema design 
			
		e. update profile 
			update password 
		f. front end design 
		g. forgot password 
		h. captcha validation 
		
		middle layer separate study 
		i. middle layer 
			detailed requirment study 
			design 
			
		
		
		

Kanban:

https://businessmap.io/kanban-resources/getting-started/what-is-kanban

1. Overview:

    Kanban 
		Agile methodology 
		focuses on 
			visualizing work, 
			limiting work in progress (WIP)
			achieving a continuous flow of work.
    No specific 
		roles or 
		ceremonies
		offer greater flexibility in implementation.

2. Principles:

    Visualize Workflow: 
		Teams use a Kanban board 
			to visualize the flow of work 
				items from the 
					"To Do" column 
				to the 
					"Done" column.
		Include informations like 
			Where in the process team agrees to do a particular work item.
				design
				development
				testing?
				
			where the team delivers work item to customer.
			Policies that determine what work should exist 
				in a particular stage
			WIP Limits 
			
			
Continous deployment 
Continous integration 
Continous build 
Continous testing
Continous static analysis
Continous delivery
Continous development 

	

	
					
    Limit Work in Progress: 
		Teams set WIP limits 
			for each column 
			to prevent overloading team members.
		Each member encouraged to complete wip 
			before picking anything new 
	Manage Flow: 
		Teams aim to optimize the flow of work items 
			through the system, 
			minimizing bottlenecks.
		Track flow of work in service 
		Maximize value delivery 
		Minimize lead time 
		Become as predictable as possible
    Make Process Policies Explicit: 
		Teams define and make explicit their 
			process policies, 
			ensuring everyone understands 
				how work is done.
		You can't improve something 
			you don't understand 
		People would not be involved 
			unless they understand the value in it.
	Following may help
				Distributed 
				Visible 
				Well-defined 
				Subject to change 
				Work policies 
    Implement Feedback Loops: 
		Kanban encourages teams to 
			continuously review and 
			adapt their processes based on feedback.
	Improve collaboratively, evolve experimentally 




3. Practices:

    Kanban Board: 
		A visual representation of the workflow
			typically consisting of columns 
				representing stages of work.
    WIP Limits: 
		Enforce limits on 
			how many items can be in 
				progress in each column.
    Pull System: 
		Work items are pulled from the 
			"To Do" column 
			into the 
			"In Progress" column 
				when capacity allows.
    Cycle Time: 
		Measure the time it takes for work items to move from start to finish.


Adv. of kanban 
	Identify issues 
	Flexibility 
		no 
			phase 
			duration 
			roles 
				etc.
	Save time 	
	Empowers teams
	Balance productivity 
	Increased customer satisfaction 
Similarities:

    Customer-Centric: 
		Both Scrum and Kanban 
			focus on 
				delivering value to the customer and 
				emphasize customer feedback.

    Iterative and Incremental: 
		Both methodologies promote 
			iterative and 
			incremental development
			allow for regular inspection and adaptation.

    Flexibility: 
		Both Scrum and Kanban allow 
			teams to adapt to 
				changing requirements and priorities.

Differences:

    Roles and Ceremonies: 
		Scrum defines specific 
			roles 
				(Scrum Master, Product Owner) and 
			ceremonies 
				(Sprint Planning, Daily Scrum) while 
		Kanban does not prescribe 
			specific roles or ceremonies.

    Time-Boxed vs. Continuous: 
		Scrum uses time-boxed iterations 
			(sprints) while 
		Kanban allows for 
			continuous flow of work without fixed timeframes.

    WIP Limits: 
		Kanban places a strong emphasis on 
			WIP limits 
				to manage the flow of work
		Scrum does not have explicit WIP limits.

    Predictability: 
		Scrum provides more predictability 
			due to fixed sprint lengths
		Kanban offers 
			more flexibility 
				in terms of when work items are completed.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up a basic Agile project board.
-------------------------------------------------------------------------------------------------------------------------

Game: 
	The common goal is for the team to move a ball or several balls around the table. Team members must all touch the ball or balls once. After one team member touches the ball, the next person must do the same. The Scrum team earns a point if they successfully manage to move the ball around the table.
	
	
	Each sprint lasts for three minutes, and the whole team must participate in five sprints to see who wins the Ball Point game. During the first sprint, the team discusses their strategy and takes notes to anticipate how many points they will score in the first minute.

	The second minute involves moving the ball around the table. The Scrum team records their points and new learning in the third minute.

	

Setting up a basic Agile project board using Jira is a straightforward process. Jira is a popular Agile project management tool that provides features for creating and managing Agile boards, such as Scrum boards and Kanban boards. Here are the steps to set up a basic Agile project board using Jira:

	Prerequisites:

    Access to a Jira account.

Steps:

    Log in to Jira:
        Open your web browser and navigate to your Jira instance.
        Log in to your Jira account with your credentials.

    Create a New Project:
        Click on "Projects" in the top menu bar.
        Click on the "+ Create Project" button.

    Select Project Type:
        Choose the project type that aligns with your Agile methodology. 
			For example, you can choose "Scrum Software Development" for Scrum or "Kanban Software Development" for Kanban.

    Configure Project Details:
        Fill in the necessary project details, such as project name, key, and project lead.
        Click the "Create" button to create the project.

    Create a Board:
        After creating the project, you'll be prompted to create a board. If not, you can create a board later.
        Choose the board type based on your Agile methodology (Scrum or Kanban) and click the "Create Board" button.

    Board Configuration:
        Customize your board's settings, including the board name, filter, and permissions. You can also choose to start with sample data.
        Click the "Create" button to create the board.

    Add Issues (User Stories or Tasks):
        In your project, you can start adding issues (user stories, tasks, or other work items) to the backlog.
        Click on "Backlog" in the left sidebar and then click the "+ Create issue" button to add new issues.

    Plan Your First Sprint (For Scrum):
        If you're using Scrum, you can plan your first sprint by dragging issues from the backlog to the sprint.
        Click on the "Active sprints" or "Backlog" tab on your board to manage sprint planning and execution.

    Work on Tasks:
        As your team works on tasks, they can move them across the board columns (e.g., To Do, In Progress, Done) by simply dragging and dropping.

    Customize Columns and Workflow:
        You can customize the columns and workflow to match your team's process.  to "Board settings" and then "Columns" or "Workflows" to make changes.

    Monitor Progress:
        Use Jira's built-in reports and charts to monitor progress, track velocity, and gather insights on your team's performance.

    Collaborate and Communicate:
        Encourage your team to use Jira's commenting and collaboration features to discuss tasks and updates directly on the issues.

    Review and Retrospective (For Scrum):
        At the end of a sprint, hold a sprint review and retrospective meeting to assess completed work and discuss improvements.

    Repeat:
        Continue working in iterations, adding new issues to the backlog, and planning sprints (for Scrum) or managing work in progress (for Kanban).

This is a basic guide to set up an Agile project board in Jira. Depending on your team's specific needs and processes, you may want to explore additional features and configurations within Jira, such as setting up custom workflows, automations, and integrating with other tools to streamline your Agile project management process.

-------------------------------------------------------------------------------------------------------------------------
				
	3	Version Control Systems
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Git and GitHub.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Branching, merging, pull requests.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Cloning a repo, making changes, and creating pull requests
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	4	Build Tools
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Overview of Maven and Gradle.
-------------------------------------------------------------------------------------------------------------------------

Maven and Gradle are two popular build automation tools used in the world of Java and other JVM-based languages. They both serve the purpose of managing dependencies, building projects, and automating various development tasks, but they have different approaches and features. Here's an overview of each:

Maven:

    Overview:
        Maven is an open-source build automation and project management tool.
        It uses XML-based configuration files (pom.xml) to define project structure, dependencies, and build tasks.
        Maven follows the convention-over-configuration approach, which means it enforces a standard project structure and build lifecycle by default.
        Maven has a large repository of dependencies known as the Central Repository, making it easy to manage and download libraries.

    Key Concepts:
        POM (Project Object Model): The central configuration file in Maven projects that defines the project's metadata, dependencies, plugins, and build lifecycle phases.
        Lifecycle: Maven provides a set of predefined build lifecycle phases, such as compile, test, package, and install, which you can execute sequentially.
        Plugins: Maven is highly extensible through plugins. Plugins are used to perform various tasks during the build process.

    Advantages:
        Strong convention over configuration reduces the need for extensive build configuration.
        Extensive plugin ecosystem for various tasks.
        Dependency management is straightforward and well-integrated.

    Disadvantages:
        XML-based configuration can be verbose and hard to read.
        Limited flexibility for complex build scenarios.
        Limited support for parallel builds.

Gradle:

    Overview:
        Gradle is an open-source build automation tool that uses Groovy or Kotlin DSL (domain-specific language) for build script configuration.
        It provides a flexible and highly customizable build system.
        Gradle is known for its performance and support for incremental builds.

    Key Concepts:
        Build Script: Gradle build scripts are written in Groovy or Kotlin and are highly expressive and readable.
        Dependencies: Similar to Maven, Gradle supports dependency management and can fetch dependencies from various repositories.
        Task-Based: Gradle builds are task-based, allowing you to define custom tasks and dependencies between them.

    Advantages:
        Highly flexible and customizable build scripts.
        Support for incremental builds, which can significantly improve build times.
        Rich plugin ecosystem and the ability to reuse existing Maven dependencies.
        Excellent support for multi-module projects.

    Disadvantages:
        Learning curve for newcomers, especially if not familiar with Groovy or Kotlin.
        Build.gradle files can become complex if not organized properly.

Comparison:

    Configuration: Maven uses XML for configuration, while Gradle uses Groovy or Kotlin DSL. Gradle's DSL is generally considered more concise and readable.

    Flexibility: Gradle is more flexible and customizable, making it suitable for complex and custom build scenarios. Maven, on the other hand, enforces conventions and may require workarounds for non-standard configurations.

    Performance: Gradle is known for its performance and is often preferred for large projects due to its support for incremental builds.

    Plugin Ecosystem: Both Maven and Gradle have extensive plugin ecosystems, but Gradle's build script flexibility allows for more advanced plugins.

    Learning Curve: Maven is easier to get started with due to its conventions, while Gradle has a steeper learning curve, especially for developers new to Groovy or Kotlin.

In summary, both Maven and Gradle are powerful build automation tools with their strengths and weaknesses. The choice between them often depends on project requirements, team familiarity, and the desired level of flexibility and customization.

-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Nexus and JFrog.
-------------------------------------------------------------------------------------------------------------------------

Why package management
	Package versioining
	easy deployment
	security 
	manage the package
	build package
	distribute package
	avoid complexity 
	package dependency management 
	store artifacts
	standard process and directory layout
	saving time
	integrity of package
	enforces standard process
	schedule maintanance
	backup 
	mirror 


Nexus and JFrog are two popular artifact repository managers used in the software development and DevOps processes. They play a crucial role in managing and storing binary artifacts, such as libraries, dependencies, Docker images, and more. Here's a detailed introduction to both:

Nexus:

1. Overview:

    Sonatype Nexus Repository Manager, commonly referred to as Nexus, is an open-source repository manager that allows you to store and manage binary artifacts.

2. Key Features:

    Artifact Repository: Nexus is primarily used to store and manage binary artifacts like JAR files, Docker images, npm packages, and more.
    Dependency Management: It supports dependency management for various technologies and languages, including Java, JavaScript, Ruby, and more.
    Security: Nexus offers robust security features, including role-based access control, LDAP integration, and vulnerability scanning to ensure artifact security.
    Proxying and Caching: Nexus can act as a proxy to external repositories like Maven Central, reducing the need to download artifacts repeatedly.
    Repository Formats: It supports multiple repository formats, such as Maven, npm, Docker, PyPI, and more, making it versatile for various development ecosystems.
    REST API: Nexus provides a RESTful API for automation and integration with other DevOps tools.

3. Benefits:

    Centralized Artifact Management: Nexus provides a centralized location for storing and managing artifacts, ensuring consistency across your organization.
    Improved Build Speed: Proxying and caching external repositories can significantly improve build times by reducing download times.
    Enhanced Security: Nexus helps you identify and mitigate security vulnerabilities in your artifacts.
    Customizability: You can customize Nexus to meet your specific requirements and workflows.

4. Editions:

    Nexus is available in two editions: Nexus Repository OSS (open-source) and Nexus Repository Pro (commercial). The Pro version offers additional features and support.

JFrog Artifactory:

1. Overview:

    JFrog Artifactory is a popular commercial artifact repository manager developed by JFrog, Inc. It is available in both open-source and commercial editions.

2. Key Features:

    Artifact Repository: Artifactory serves as a universal repository manager, supporting various binary artifact formats and technologies.
    Dependency Management: It provides dependency resolution and management for different ecosystems, including Maven, npm, Docker, and more.
    Security: Artifactory offers security features such as access control, LDAP integration, and comprehensive vulnerability scanning through JFrog Xray.
    High Availability: Artifactory can be configured for high availability to ensure uninterrupted access to artifacts.
    Metadata Management: It maintains rich metadata for artifacts, making it easy to search and retrieve them.
    REST API: Artifactory provides a RESTful API for automation and integration with CI/CD pipelines.

3. Benefits:

    Comprehensive Solution: Artifactory offers a robust, all-in-one solution for artifact management, security, and dependency resolution.
    Integration with JFrog Xray: Xray is a security and compliance tool that integrates seamlessly with Artifactory to scan for vulnerabilities and enforce security policies.
    High Availability: Artifactory can be configured for high availability, ensuring minimal downtime.
    Universal Repository Manager: It supports a wide range of technologies, making it versatile for heterogeneous development environments.

4. Editions:

    Artifactory is available in several editions, including Artifactory OSS (open-source), Artifactory Pro (commercial), and Artifactory Enterprise (commercial with advanced features and support).

Comparison:

    Both Nexus and Artifactory are powerful artifact repository managers, and the choice between them often depends on specific organizational needs, licensing preferences, and the existing technology stack.
    Artifactory has a more comprehensive offering, especially with the integration of JFrog Xray for security and compliance. Nexus, however, has strong capabilities and is open-source.
    Evaluating features, licensing costs, and support options is essential when choosing between the two.

In summary, Nexus and JFrog Artifactory are essential components of modern software development and DevOps pipelines, ensuring efficient artifact management, security, and compliance for organizations of all sizes. The choice between them should consider factors like the specific technology stack, budget, and required features.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Building a simple Java project using Maven, storing artifacts in Nexus.
-------------------------------------------------------------------------------------------------------------------------
docker volume create --name nexus-data
docker run -d -p 8081:8081 --name nexus -v nexus-data:/nexus-data sonatype/nexus3


References: 
https://ahgh.medium.com/how-to-setup-sonatype-nexus-3-repository-manager-using-docker-7ff89bc311ce
https://www.devopsschool.com/blog/how-to-upload-the-artifacts-in-sonatype-nexus/
https://mincong.io/2018/08/04/maven-deploy-artifacts-to-nexus/
https://rodolfombc.medium.com/deploying-java-artifacts-on-a-nexus-repository-with-maven-939a80406acf
https://www.baeldung.com/maven-deploy-nexus


docker run -d -p 8081:8081 --name nexus sonatype/nexus3

click sign in 

Click settings gear 
Roles 
	Create role 
		Type: Nexus role 
		Role ID: deploy
		Role Name: deploy 
		
	create a deploy role with nx-all (no sub role)
	
Users 
	Create a (local) user with that role 
	
	ID: deploy 
	First name: deploy 
	Last name: deploy 
	Email: vilas_varghese@yahoo.com
	Password: deploy@123
	Confirm password: deploy@123
	Status: Active 
	Roles 
		Move deploy role to right 
		
		Click - Create local user 
		
		
	
Repository
	Create hosted repo. 
	Name: vilashostedrepo
	Online: check 
	Version Policy: Release 
	Layout policy: Strict
	Add Content-Disposition header ... : inline 
	Storage - Blob store: default
	Strict Content Type Validation: check
	Deployment policy: Allow redeploy
	
	
	
  yum update -y
  yum install docker -y
  systemctl start docker
  docker run -d -p 8081:8081 --name nexus sonatype/nexus3
  http://<ip:8081>
	sometimes this can take sometime. Keep refereshing.
	
  
  click on login/sign in 
  docker ps
  docker exec -it <container id> bash
		get the pwd
		
		disable public access. 
		
  yum update -y
  yum install maven
  mvn archetype:generate   -DgroupId=com.mycompany   -DartifactId=demo   -DarchetypeArtifactId=maven-archetype-quickstart   -DinteractiveMode=false
  cd demo/
  vi pom.xml
  
content of pom.xml


------------------------------------------------------------------------------------------------------------------------------------------------

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.mycompany</groupId>
  <artifactId>demo</artifactId>
  <packaging>jar</packaging>
  
#Change this version 
  <version>1.0</version>
  <name>demo</name>
  <url>http://maven.apache.org</url>

<!-- copy from below-->

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <maven.compiler.source>1.8</maven.compiler.source>
    <maven.compiler.target>1.8</maven.compiler.target>
  </properties>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <build>
    <pluginManagement>
      <plugins>
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-deploy-plugin</artifactId>
          <version>2.8.2</version>
        </plugin>
      </plugins>
    </pluginManagement>
    <plugins>
    <plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-deploy-plugin</artifactId>
   </plugin>
  </plugins>
  </build>


  <distributionManagement>
    <repository>
      <id>nexus-snapshots</id>
      <url>http://18.117.235.153:8081/repository/mymaven-hosted/</url>
    </repository>
  </distributionManagement>
</project>


Change pom.xml version from 1.0SNAPSHOT to 1.0

------------------------------------------------------------------------------------------------------------------------------------------------
  
  
  
  cd ~/.m2/
  vi settings.xml
------------------------------------------------------------------------------------------------------------------------------------------------
<settings>
  <servers>
    <server>
      <id>nexus-snapshots</id>
      <username>deploy</username>
      <password>deploy@123</password>
    </server>
  </servers>
</settings>


------------------------------------------------------------------------------------------------------------------------------------------------

  cd -
  mvn clean deploy

find the url from the console output and try to get there 
http://18.117.235.153:8081/#browse/browse:mymaven-hosted  
	
-------------------------------------------------------------------------------------------------------------------------
				
	5	Continuous Integration & Continuous Delivery (CI/CD) Refresher
-------------------------------------------------------------------------------------------------------------------------

Continuous Integration and Continuous Delivery (CI/CD) are software development practices that aim to automate and streamline the process of building, testing, and delivering software. They play a crucial role in modern software development by improving efficiency, reliability, and speed. Here's a refresher on CI/CD:

Continuous Integration (CI):

    Integration: CI involves integrating code changes from multiple contributors into a shared repository, such as Git, frequently and automatically. This integration happens multiple times a day.

    Automated Build: Whenever code is pushed to the repository, an automated build process is triggered. The build process compiles the code, runs tests, and produces deployable artifacts.

    Automated Testing: CI includes automated testing to ensure that code changes do not introduce new defects or regressions. Common types of tests include unit tests, integration tests, and end-to-end tests.

    Immediate Feedback: Developers receive immediate feedback on the quality of their code. If a build or test fails, the team can quickly identify and address issues.

    Benefits: CI improves code quality, reduces integration problems, and accelerates development by catching and fixing issues early.

Continuous Delivery (CD):

    Automated Deployment: CD extends CI by automating the deployment process. Once code passes CI tests, it is automatically deployed to a staging or pre-production environment.

    Manual Approval: In CD pipelines, there is often a manual approval step before deploying to production. This allows teams to review changes and ensure they are ready for production.

    Deployment to Production: After approval, the code is deployed to the production environment, making it available to end-users.

    Immutable Infrastructure: CD pipelines often use concepts like immutable infrastructure, where new deployments replace existing infrastructure rather than modifying it.

    Rollback: CD pipelines are designed to facilitate easy rollbacks in case of issues in the production environment.

    Benefits: CD reduces the risk associated with manual deployments, shortens the time to market, and ensures that software is always in a deployable state.

CI/CD Pipeline:

A CI/CD pipeline is a series of automated steps that code changes  through, from integration to deployment. Key components of a CI/CD pipeline include:

    Source Control: Developers use version control systems like Git to manage code changes.

    Build: Automated builds compile code, run tests, and produce artifacts.

    Test: Automated testing, including unit tests, integration tests, and other forms of testing.

    Artifact Repository: Artifacts generated in the build phase are stored in an artifact repository for use in deployment.

    Deployment: Automated deployment to staging and production environments.

    Monitoring and Feedback: Continuous monitoring of production environments to detect issues and provide feedback for further improvements.

DevOps and CI/CD:

DevOps is a cultural and organizational movement that emphasizes collaboration and automation between development and IT operations. CI/CD is a key DevOps practice, as it automates many tasks involved in software delivery. Together, they enable organizations to achieve faster, more reliable, and more frequent software releases.

In summary, CI/CD practices are fundamental to modern software development, helping teams deliver high-quality software faster and with fewer manual interventions. They are essential components of a DevOps culture that values collaboration, automation, and continuous improvement.


-------------------------------------------------------------------------------------------------------------------------
				
		CI/CD concepts, benefits, challenges.
-------------------------------------------------------------------------------------------------------------------------

Continuous Integration and Continuous Delivery (CI/CD) are essential practices in modern software development that focus on automating and streamlining the process of building, testing, and deploying software. Here are the key concepts, benefits, and challenges associated with CI/CD:

Concepts:

    Continuous Integration (CI):
        CI involves frequently integrating code changes into a shared repository (usually a version control system like Git).
        Developers commit their code changes multiple times a day.
        Automated build and test processes are triggered for each code commit.
        The al is to detect integration issues and defects early in the development cycle.

    Continuous Delivery (CD):
        CD extends CI by automating the deployment process.
        After passing CI tests, code changes are automatically deployed to a staging or pre-production environment.
        Manual approval gates may be included before deploying to production.
        The al is to ensure that code is always in a deployable state and can be released to production at any time.

    CI/CD Pipeline:
        A CI/CD pipeline is a series of automated steps that code changes  through, from integration to deployment.
        It typically includes stages for code compilation, testing, artifact generation, deployment to different environments, and monitoring.

Benefits:

    Faster Time to Market: CI/CD reduces manual processes and automates repetitive tasks, enabling faster delivery of new features and bug fixes to end-users.

    Higher Code Quality: Automated testing and code reviews in CI/CD pipelines catch defects early, leading to higher code quality and fewer production issues.

    Reduced Manual Errors: Manual deployments are error-prone. CI/CD minimizes human errors by automating the deployment process.

    Consistency: CI/CD promotes consistency in the deployment process, ensuring that each deployment is done the same way every time.

    Improved Collaboration: CI/CD encourages collaboration between development, testing, and operations teams, fostering a DevOps culture.

    Efficiency: CI/CD reduces the time and effort required for manual testing, validation, and deployment, making development teams more efficient.

    Rapid Feedback: Developers receive rapid feedback on code changes, enabling them to fix issues quickly.

Challenges:

    Complex Setup: Implementing CI/CD can be complex, especially for organizations with legacy systems or monolithic applications.

    Testing Challenges: Writing comprehensive automated tests can be time-consuming, and maintaining them as the codebase evolves is a challenge.

    Security Concerns: Automating deployments can raise security concerns, especially if vulnerabilities are introduced into the pipeline.

    Integration Points: CI/CD pipelines often involve integrating with various tools and systems, which can lead to compatibility and integration challenges.

    Cultural Resistance: CI/CD requires cultural changes within organizations, and some teams may resist the shift to automation and collaboration.

    Pipeline Maintenance: As projects grow and evolve, CI/CD pipelines require oning maintenance and adjustments.

    Monitoring and Feedback: Ensuring comprehensive monitoring and feedback loops to detect issues in production can be challenging.

    Cost: Setting up and maintaining CI/CD infrastructure and tools can be costly, particularly for large-scale projects.

In conclusion, CI/CD practices offer significant advantages in terms of speed, quality, and efficiency in software development. However, organizations need to carefully plan, invest in the right tools and processes, and address the associated challenges to fully realize the benefits of CI/CD. Successful implementation often requires a commitment to cultural and process changes as well as oning improvement efforts.





-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Jenkins and GitHub Actions.
-------------------------------------------------------------------------------------------------------------------------
Jenkins
-------
	Jenkins is an open-source automation server that facilitates continuous integration and continuous delivery (CI/CD) in software development. It is one of the most widely used and extensible automation platforms available today. Jenkins is primarily used to automate repetitive tasks, build, test, and deploy code changes, making it an essential tool for DevOps and software development teams.

	
	Key Features:

		Automation: Jenkins automates various tasks in the software development lifecycle, such as code builds, testing, and deployment, by defining and executing custom workflows.

		Integration: Jenkins integrates with a wide range of tools, version control systems (e.g., Git, SVN), build tools (e.g., Maven, Gradle), testing frameworks, and deployment platforms. This extensibility allows it to fit seamlessly into various development environments.

		Plugin Ecosystem: Jenkins offers a vast collection of plugins that extend its functionality. You can find plugins for source code management, build triggers, deployment, notifications, and more.

		Distributed Builds: Jenkins can distribute build and test tasks across multiple machines, improving efficiency and speeding up the CI/CD pipeline.

		Scalability: It scales easily to accommodate growing software development needs, whether you have a small team or a large enterprise.

		Security: Jenkins provides security features, including role-based access control (RBAC) and user authentication, to ensure that sensitive build and deployment processes are protected.

		Monitoring and Logging: Jenkins offers monitoring and logging capabilities to track the status of builds, deployments, and system health.

	Core Concepts:

		Jobs and Pipelines: In Jenkins, you create jobs or pipelines to define and automate tasks. Jobs can be simple, like running a single command, or complex pipelines that include multiple stages and steps.

		Builds: Jenkins handles builds, which involve compiling code, running tests, and producing deployable artifacts (e.g., JAR files, Docker images).

		Plugins: Plugins extend Jenkins' functionality. You can install and configure plugins to integrate with various tools and services.

		Workspace: Each job or pipeline has its own workspace, where files and dependencies are stored for the duration of the build.

	Benefits:

		Automation: Jenkins automates repetitive and time-consuming tasks, reducing manual effort and human error.

		Fast Feedback: With CI, developers receive quick feedback on the quality of their code changes, enabling them to fix issues early in the development cycle.

		Consistency: Jenkins ensures that builds and deployments are consistent and repeatable, reducing discrepancies between development, testing, and production environments.

		Improved Collaboration: Jenkins promotes collaboration among development, testing, and operations teams by providing a central platform for automated processes.

		Efficiency: By automating builds and tests, Jenkins improves the efficiency of the development and deployment process.

	Challenges:

		Complexity: Setting up and configuring Jenkins, especially for complex projects, can be challenging.

		Maintenance: Jenkins requires oning maintenance, including updates, plugin management, and job/pipeline adjustments.

		Resource Management: Allocating sufficient hardware resources for Jenkins and managing distributed builds can be complex.

		Security: Ensuring the security of Jenkins and its integrations is crucial, especially in enterprise environments.

	Jenkins is a versatile and widely adopted tool in the world of CI/CD and DevOps. Its rich feature set, large community, and plugin ecosystem make it a powerful choice for automating various aspects of software development and deployment.


GitHub Actions
--------------

GitHub Actions is a powerful and flexible continuous integration and continuous delivery (CI/CD) platform offered by GitHub, a leading web-based platform for version control and collaboration. GitHub Actions allows developers to automate various software development workflows directly within their GitHub repositories. It is tightly integrated with GitHub repositories and provides a wide range of pre-built actions and workflows to streamline software development and deployment processes.


	Key Concepts:

		Workflows: Workflows in GitHub Actions are sets of automated steps that are executed in response to specific events, such as code pushes, pull requests, or issue updates. Workflows are defined using YAML files placed in the .github/workflows directory of a repository.

		Actions: Actions are individual tasks or steps within a workflow. GitHub provides a marketplace of pre-built actions that cover a wide range of tasks, from building and testing code to deploying applications and sending notifications. You can also create custom actions to suit your specific needs.

		Events: Workflows are triggered by events, which can be actions like code pushes, pull request creation or updates, issue comments, and more. You can configure workflows to run on specific events or schedules.

		Runners: GitHub Actions uses runners to execute workflows. Runners can be hosted by GitHub (GitHub-hosted runners) or self-hosted on your own infrastructure. GitHub-hosted runners are available for free and support various platforms, including Linux, macOS, and Windows.

	Key Features:

		Automated CI/CD: GitHub Actions enables the automation of CI/CD pipelines, allowing you to build, test, and deploy code changes automatically.

		YAML Configuration: Workflows are defined using human-readable YAML configuration files, making it easy to understand and version control your CI/CD process alongside your codebase.

		Versatile Workflows: You can create workflows tailored to your specific needs, whether it's building and testing a web application, deploying a containerized service, or automating code reviews.

		Integration with GitHub: GitHub Actions is deeply integrated with GitHub repositories, making it easy to set up and configure CI/CD workflows without leaving the GitHub platform.

		Custom Actions: GitHub Actions allows you to create custom actions, reusable across different workflows, to extend the functionality of your CI/CD pipeline.

		Matrix Builds: You can set up matrix builds to test code across multiple versions of programming languages, platforms, or dependencies in parallel.

	Benefits:

		Streamlined Development: GitHub Actions automates repetitive tasks, reducing manual effort and ensuring that code changes are continuously tested and deployed.

		Enhanced Collaboration: Teams can collaborate more effectively by automating code reviews, running tests on pull requests, and deploying to staging environments.

		Improved Code Quality: Automated testing and code validation in CI workflows help catch issues early in the development process, leading to higher code quality.

		Faster Release Cycles: CI/CD pipelines speed up the delivery of new features and bug fixes to production, accelerating the release cycle.

		Community Contributions: GitHub Actions benefits from a growing library of community-contributed actions, allowing you to leverage existing solutions.

	Use Cases:

		Continuous Integration (CI): Automate code compilation, testing, and code quality checks with every push to a repository.

		Continuous Deployment (CD): Deploy applications to staging and production environments automatically when code changes are merged into the main branch.

		Code Scanning: Automatically run security scans, code analysis, and vulnerability assessments on your codebase.

		Issue and Pull Request Workflows: Automate issue triage, labeling, and notifications based on predefined rules.

		Release Management: Automate versioning, changelog generation, and release notes creation.

	GitHub Actions is a versatile CI/CD platform that empowers development teams to automate and streamline their software development processes, enabling faster, more reliable, and efficient software delivery.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up a basic CI/CD pipeline using Jenkins.
-------------------------------------------------------------------------------------------------------------------------




Setting up a basic CI/CD pipeline using Jenkins involves several steps. In this example, I'll walk you through the process of creating a simple pipeline that builds and deploys a web application from a Git repository. Before you begin, make sure you have Jenkins installed and configured on your server. Here's a step-by-step guide:

Step 1: Create a New Jenkins Job

    Log in to your Jenkins server.

    Click on "New Item" to create a new Jenkins job.

    Enter a name for your job (e.g., "MyWebAppCI/CD") and select the "Pipeline" project type. Click "OK" to create the job.

Step 2: Configure the Pipeline

    In the job configuration page, scroll down to the "Pipeline" section.

    Under "Definition," select "Pipeline script from SCM" as the pipeline script source.

    Choose your version control system (e.g., Git) and provide the repository URL.

    In the "Branch Specifier" field, specify the branch you want to build (e.g., "main" or "master").

    Optionally, you can add credentials if your repository requires authentication.

Step 3: Define the Pipeline Script

    Scroll down to the "Pipeline" section, and under "Pipeline script," you can either write your pipeline script directly in the Jenkins job configuration or reference a Jenkinsfile from your repository. For simplicity, let's use a scripted pipeline directly in the job configuration.

    Here's a basic scripted pipeline script that builds and deploys a web application. Modify it according to your project's needs:

    groovy

    node {
        stage('Checkout') {
            // Checkout the code from the repository
            checkout scm
        }

        stage('Build') {
            // Replace with your build commands (e.g., npm install, npm run build)
            sh 'npm install'
            sh 'npm run build'
        }

        stage('Deploy') {
            // Replace with your deployment commands (e.g., copy files to a server)
            sh 'rsync -avz --delete dist/ user@your-server:/path/to/deploy'
        }
    }

    This script does the following:
        Checks out the code from your Git repository.
        Builds the web application (you can replace the build commands).
        Deploys the built application (you can replace the deployment commands).

    Click "Save" to save your job configuration.

Step 4: Build and Deploy

    Trigger the pipeline by clicking "Build Now" in the job's main page.

    Jenkins will start building and deploying your web application based on the defined pipeline script.

    You can monitor the progress by clicking on the build in the Jenkins dashboard.

Step 5: Configure Post-Build Actions

Depending on your project, you may want to configure post-build actions such as email notifications, test result reporting, or Slack notifications. You can do this by adding additional stages to your pipeline script or configuring post-build actions in the Jenkins job configuration.

This example provides a basic setup for a CI/CD pipeline using Jenkins. Depending on your project's complexity and requirements, you can extend and customize your pipeline to include testing, quality checks, and more advanced deployment strategies.

-------------------------------------------------------------------------------------------------------------------------
				
Week 3		Advanced DevOps Practices & Automation
-------------------------------------------------------------------------------------------------------------------------

Advanced DevOps practices and automation are key to achieving greater efficiency, reliability, and speed in software development and IT operations. These practices  beyond the basics and often involve more sophisticated automation, orchestration, and collaboration techniques. Here are some advanced DevOps practices and automation strategies:

    Infrastructure as Code (IaC):
        IaC involves managing and provisioning infrastructure resources (e.g., servers, networks, databases) using code. Tools like Terraform, AWS CloudFormation, and Ansible enable teams to define infrastructure as code, allowing for consistent and repeatable deployments.

    Immutable Infrastructure:
        Immutable infrastructure is the practice of replacing the entire infrastructure (e.g., virtual machines or containers) instead of making changes to existing instances. This approach enhances consistency, security, and reliability.

    GitOps:
        GitOps is a practice where all infrastructure and application configurations are stored in a version-controlled repository (e.g., Git). Changes are automatically applied to the environment when changes are committed to the repository. Tools like ArCD and Flux enable GitOps workflows.

    Multi-Cloud and Hybrid Cloud Management:
        Organizations increasingly adopt multi-cloud and hybrid cloud strategies. Advanced DevOps practices involve managing resources and workloads across different cloud providers and on-premises environments using unified tooling and automation.

    Continuous Verification and Testing:
        Beyond basic unit and integration tests, advanced DevOps practices include continuous verification of infrastructure and applications using techniques like chaos engineering (e.g., Chaos Monkey) and security testing (e.g., static analysis, penetration testing).

    Self-Service Portals:
        Implementing self-service portals or catalogs that allow teams to provision their own resources and environments based on predefined templates and policies. This reduces friction and accelerates development.

    AIOps (Artificial Intelligence for IT Operations):
        AIOps leverages machine learning and AI to automate monitoring, anomaly detection, incident resolution, and capacity planning. It helps teams proactively manage complex systems and reduce downtime.

    Container Orchestration:
        Advanced DevOps practices often involve container orchestration platforms like Kubernetes for automating the deployment, scaling, and management of containerized applications. Tools like Helm and operators enhance Kubernetes-based workflows.

    Event-Driven Automation:
        Leveraging event-driven architectures and tools (e.g., Apache Kafka, AWS EventBridge) to automate workflows and processes based on real-time events and triggers.

    Compliance as Code:
        Applying compliance policies and checks as code to ensure that infrastructure and applications meet regulatory and security requirements. Tools like Open Policy Agent (OPA) are used for policy enforcement.

    Observability and Monitoring:
        Implementing advanced monitoring and observability practices, including distributed tracing, metrics collection, and log analysis, to gain deep insights into application and system behavior.

    Site Reliability Engineering (SRE):
        Adopting SRE principles to manage and operate services at scale, including service level objectives (SLOs), error budgets, and automated incident management.

    ChatOps:
        Integrating chat and collaboration tools (e.g., Slack, Microsoft Teams) with automation bots to facilitate real-time communication, incident response, and task execution within chat channels.

    Security Automation:
        Incorporating security into DevOps by automating security checks and scans throughout the software development lifecycle (SDLC) using tools like OWASP ZAP, SonarQube, and vulnerability scanning.

    Continuous Compliance and Auditing:
        Automating compliance checks and audits to ensure that infrastructure and applications adhere to regulatory requirements and industry standards.

Advanced DevOps practices and automation are essential for organizations looking to deliver software faster, with higher quality, and in a more secure and compliant manner. However, implementing these practices requires careful planning, expertise, and a commitment to continuous improvement. It's important to assess your organization's needs and maturity level to determine which advanced practices are most beneficial.


-------------------------------------------------------------------------------------------------------------------------
				
	1	Automation Tests
-------------------------------------------------------------------------------------------------------------------------

Automated testing is a fundamental practice in software development and quality assurance that involves using tools and scripts to execute tests and verify the correctness and functionality of a software application. Automated tests are crucial for maintaining software quality, catching defects early in the development process, and ensuring that changes or updates to the codebase do not introduce regressions. Here's an overview of various types of automated tests:

    Unit Tests:
        Unit tests focus on testing individual components or functions (units) of the software in isolation.
        They are typically written and executed by developers during the development phase.
        Unit tests help verify that each unit of code works as expected and serves as documentation for how the code is intended to function.

    Integration Tests:
        Integration tests verify interactions between different components or modules within the software.
        They ensure that integrated parts of the system work correctly together.
        Integration tests may involve testing how components communicate, exchange data, and function as a cohesive whole.

    Functional Tests:
        Functional tests evaluate the overall functionality of a software application from an end-user perspective.
        These tests typically focus on specific features or user scenarios.
        Functional tests help ensure that the software meets the requirements and functions as expected by the user.

    End-to-End Tests (E2E Tests):
        End-to-end tests evaluate the entire application's behavior and functionality, including all components and dependencies.
        They simulate user interactions with the application and verify that it behaves correctly in real-world scenarios.
        E2E tests help ensure that the application works as a complete system.

    Regression Tests:
        Regression tests are automated tests that verify that recent code changes have not introduced new defects or broken existing functionality.
        They are essential for maintaining software quality as the codebase evolves.
        Continuous integration (CI) pipelines often include regression tests to catch issues early.

    Load and Performance Tests:
        Load and performance tests assess how the software performs under various conditions, such as high user loads or heavy data processing.
        These tests help identify bottlenecks, scalability issues, and performance optimizations.
        Tools like Apache JMeter and Gatling are commonly used for load testing.

    Security Tests:
        Security tests, including vulnerability scanning and penetration testing, assess the application's security posture.
        They help identify and mitigate security vulnerabilities, such as SQL injection or cross-site scripting (XSS) attacks.
        Tools like OWASP ZAP and Nessus can assist in security testing.

    Acceptance Tests:
        Acceptance tests are used to determine if a software application meets the acceptance criteria defined by stakeholders.
        These tests validate that the software fulfills the specified business requirements.
        Behavior-Driven Development (BDD) tools like Cucumber and SpecFlow are often used for defining and executing acceptance tests.

    Continuous Testing:
        Continuous testing is the practice of integrating automated tests into the continuous integration/continuous delivery (CI/CD) pipeline.
        It involves running automated tests automatically whenever code changes are committed, providing rapid feedback to developers.

    UI (User Interface) Tests:
        UI tests automate interactions with the graphical user interface (GUI) of the application.
        They verify that the user interface elements function correctly and that user interactions produce the expected results.
        UI testing frameworks like Selenium and Cypress are widely used for this purpose.

Automated tests should be an integral part of the software development process, supporting agile development practices, code refactoring, and continuous improvement. Well-designed and maintained automated tests significantly contribute to software quality and help ensure that applications are reliable and bug-free

-------------------------------------------------------------------------------------------------------------------------
				
		Importance of automated testing in DevOps.
-------------------------------------------------------------------------------------------------------------------------

Automated testing plays a pivotal role in DevOps by contributing to the overall success and efficiency of the development and deployment pipeline. Here are some of the key reasons highlighting the importance of automated testing in DevOps:

    Speed and Efficiency:
        Automated tests execute much faster than manual tests, allowing for quicker feedback on code changes.
        Tests can be integrated into the CI/CD pipeline, enabling automatic testing of every code commit, which accelerates the development process.

    Continuous Integration (CI):
        Automated testing is a fundamental component of CI, ensuring that code changes are tested thoroughly as soon as they are committed.
        CI systems can automatically trigger tests, providing rapid feedback to developers about the quality of their code.

    Consistency and Reliability:
        Automated tests provide consistent and reproducible results, reducing human errors associated with manual testing.
        Test scripts execute the same steps and checks every time, leading to more reliable and predictable outcomes.

    Regression Testing:
        Automated tests excel at regression testing, where they verify that recent code changes do not introduce new defects or break existing functionality.
        Continuous execution of regression tests helps maintain software quality as the codebase evolves.

    Early Defect Detection:
        Automated tests identify defects and issues early in the development process, allowing developers to address them while the code is fresh in their minds and the changes are still recent.

    Continuous Feedback:
        Automated tests provide continuous feedback to developers about the health of the codebase.
        Failures are reported immediately, enabling quick diagnosis and resolution of issues.

    Parallel Testing:
        Automated tests can be executed in parallel on various configurations, platforms, or browsers, enabling comprehensive testing and reducing test execution time.

    Scalability:
        As DevOps practices promote scalability and elasticity, automated tests can scale with the application, ensuring that testing keeps pace with development.

    Documentation:
        Automated tests serve as living documentation for the application's behavior. They describe how various parts of the software should function, making it easier for new team members to understand and maintain the code.

    Shift-Left Testing:
        Automated testing supports the "shift-left" approach, where testing activities are performed earlier in the development cycle.
        This approach helps identify and address issues at a stage when they are less costly to fix.

    Improved Collaboration:
        Automated tests encourage collaboration between development, testing, and operations teams by providing a shared understanding of the application's quality and behavior.

    DevOps Culture:
        Automated testing is a fundamental component of a DevOps culture that emphasizes collaboration, automation, and continuous improvement.

In summary, automated testing is an essential enabler of DevOps practices, promoting speed, quality, and reliability in the software development and delivery process. By automating testing activities, teams can reduce manual effort, catch defects early, and ensure that software is continuously tested, leading to faster and more reliable releases.


----------------

Rapid Feedback and Continuous Integration: Automated testing enables developers to receive immediate feedback on the code changes they make. This feedback loop is essential in a DevOps environment where continuous integration (CI) is practiced. Developers can quickly identify and fix issues, leading to faster development cycles.

Quality Assurance: Automated tests help ensure the quality of the software by verifying that it functions correctly and does not introduce new defects. This reduces the likelihood of releasing buggy or unstable software into production.

Regression Testing: As software evolves and new features are added, it's essential to verify that existing functionality remains intact. Automated regression tests can quickly identify any regressions, saving time and effort compared to manual testing.

Consistency: Automated tests are executed in a consistent and repeatable manner, reducing the risk of human error in testing. Manual testing can be prone to inconsistencies, especially when tests need to be executed frequently.

Scalability: DevOps often involves deploying software to different environments (e.g., development, staging, production). Automated tests can be easily scaled to run in multiple environments, ensuring that the software behaves consistently across them.

Efficiency: Automated tests can execute a large number of test cases much faster than manual testing. This efficiency is especially valuable when dealing with complex applications or large codebases.

Parallel Testing: Many automated testing frameworks allow tests to run in parallel, which can significantly reduce the overall test execution time. This is important when striving for fast feedback in a DevOps pipeline.

Continuous Delivery and Deployment: In a DevOps pipeline, automated tests are an integral part of the process that determines whether a build is suitable for deployment. They act as gatekeepers, ensuring that only high-quality code is promoted to production.

Cost Savings: While there is an initial investment in setting up automated tests, they ultimately save time and resources in the long run. The cost of fixing defects discovered in production is typically higher than catching them early in the development cycle.

Documentation: Automated tests serve as documentation for the expected behavior of the software. They provide a clear and executable specification of the system's functionality.

Continuous Improvement: Automated testing can provide valuable metrics and insights into the quality and performance of the software. This data can be used to identify areas for improvement and optimization.

In summary, automated testing is a fundamental component of the DevOps process, promoting speed, quality, consistency, and efficiency throughout the software development lifecycle. It allows organizations to deliver reliable software more rapidly and adapt to changing requirements and market demands.

-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Selenium for web testing.
-------------------------------------------------------------------------------------------------------------------------

Selenium is a widely-used open-source framework for automating web browsers. It provides a platform-independent way to automate and control web browsers through programs and perform various actions on web applications. Selenium is primarily used for web testing, including functional testing, regression testing, and browser compatibility testing. Here's an introduction to Selenium for web testing:

Key Components of Selenium:

    Selenium WebDriver: WebDriver is the core component of Selenium that allows interaction with web browsers. It provides a programming interface for controlling browser actions and navigating web pages.

    Selenium IDE: Selenium IDE is a record-and-playback tool that enables users to create test scripts by recording their interactions with a web application. It's a browser extension available for Chrome and Firefox.

    Selenium Grid: Selenium Grid is a component that allows parallel execution of tests across multiple browsers and platforms simultaneously. It helps distribute test execution to reduce testing time.

Supported Browsers:

Selenium supports various web browsers, including:

    ogle Chrome
    Mozilla Firefox
    Microsoft Edge
    Apple Safari
    Opera
    Internet Explorer (though it's deprecated in favor of Edge)

Supported Programming Languages:

Selenium supports multiple programming languages for writing test scripts, including:

    Java
    Python
    C#
    Ruby
    JavaScript (Node.js)

1. What is Selenium?

Selenium is a suite of tools and libraries designed for automating web browser actions.
It supports multiple programming languages, including Java, Python, C#, and more.
Selenium can be used with various web browsers, such as Chrome, Firefox, Safari, and Edge.


Key Features and Capabilities:

	Selenium WebDriver: The core component for interacting with web browsers. WebDriver provides a programming interface for controlling browsers and automating actions.

    Web Automation: Selenium can simulate user interactions with a web application, such as clicking buttons, filling out forms, navigating pages, and extracting data.

    Cross-Browser Testing: Selenium allows you to test web applications on various browsers and platforms, ensuring compatibility and consistency.

    Parallel Execution: 
		Not used much 
		Selenium Grid enables concurrent execution of test cases across multiple browsers, reducing test execution time.

    Headless Browsing: Selenium can run tests in headless mode, which means running tests without displaying the browser GUI. This is useful for faster and more efficient testing.

    Data-Driven Testing: Selenium supports data-driven testing, where test cases can be executed with multiple sets of data to validate different scenarios.

    Integration with Test Frameworks: Selenium can be integrated with popular testing frameworks such as JUnit, TestNG, NUnit, and PyTest.

    Continuous Integration: Selenium can be seamlessly integrated into CI/CD pipelines to automate testing as part of the software delivery process.

	Platform Independence: Selenium is available for various programming languages, making it suitable for different development environments.
	Open Source: Selenium is open-source software, which means it's free to use and has a large community of users and contributors.
	Wide Adoption: Selenium is widely adopted in the industry, making it a valuable skill for testers and developers.


Common Use Cases:

    Functional Testing: Selenium is primarily used for functional testing to verify that a web application behaves as expected by automating user interactions.

    Regression Testing: Automated tests can be created to ensure that new code changes do not break existing functionality.

    Performance Testing: Selenium can be used to simulate user interactions and measure the performance and load-handling capacity of a web application.

    Browser Compatibility Testing: Testers can use Selenium to verify that a web application works correctly on different browsers and versions.

    Scraping and Data Extraction: Selenium can be employed for web scraping to extract data from websites and web applications.

    Automated Form Filling: Selenium can automate the process of filling out and submitting web forms, which is useful for web application testing.

Selenium is a powerful tool for web testing, and its versatility, extensive community support, and compatibility with multiple programming languages make it a popular choice among developers and QA professionals for automating web testing processes.




-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Writing and executing a basic Selenium test.
	-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
			
	2	Monitoring
-------------------------------------------------------------------------------------------------------------------------


Monitoring in the context of IT and software refers to the process of systematically observing and measuring the performance, health, and behavior of various components within a system, application, network, or infrastructure. It aims to ensure that everything is functioning as expected and to identify and address issues proactively. Monitoring is a critical practice in maintaining the reliability, availability, and performance of systems and applications. Here is a detailed overview of monitoring:

Key Concepts in Monitoring:

    Metrics: Metrics are quantitative data points that represent specific aspects of the system's performance or state. Examples include CPU utilization, memory usage, response time, error rates, and network traffic.

    Logs: Logs are textual records generated by systems, applications, and services. They provide a detailed historical account of events, errors, and activities within a system. Monitoring tools often analyze logs for anomalies.

    Alerts: Alerts are notifications triggered by monitoring systems when predefined conditions or thresholds are met or violated. Alerts notify operators or automated systems of potential issues that require attention.

    Dashboards: Dashboards are visual representations of monitoring data, providing real-time or historical insights into the system's health and performance. They often include charts, graphs, and status indicators.

    Incident Management: Monitoring is closely linked to incident management. When issues are detected, an incident management process is initiated to investigate, diagnose, and resolve the problem.

    Data Collection: Monitoring systems collect data from various sources, including agents running on servers, APIs, logs, and external sensors. The collected data is then processed and analyzed.

Types of Monitoring:

    Performance Monitoring: This involves measuring the performance of various system components, including CPU, memory, disk I/O, and network throughput. Performance monitoring helps identify bottlenecks and optimize resource usage.

    Availability Monitoring: Availability monitoring checks if a system or service is accessible and responsive. It often includes monitoring for downtime, uptime, and response times.

    Error Monitoring: Error monitoring focuses on capturing and analyzing errors, exceptions, and unexpected behaviors in applications and systems. It helps identify and resolve issues that impact user experience.

    Security Monitoring: Security monitoring involves the detection of security threats, vulnerabilities, and unauthorized activities. It helps protect against cyberattacks and data breaches.

    Application Monitoring: Application monitoring assesses the performance and behavior of software applications, including response times, transaction rates, and user experience metrics.

    Network Monitoring: Network monitoring tracks the health and performance of network infrastructure, including routers, switches, and network services. It helps ensure optimal network connectivity.

    Infrastructure Monitoring: Infrastructure monitoring monitors the underlying hardware and virtual resources, including servers, storage, and cloud services. It ensures the availability and health of these resources.

    End-User Monitoring (EUM): EUM measures the experience of end-users interacting with web applications. It tracks load times, page rendering, and user interactions to assess application performance from the user's perspective.

Monitoring Tools:

Various monitoring tools and platforms are available to assist organizations in implementing effective monitoring strategies. Some popular monitoring tools include:

    Prometheus: An open-source monitoring and alerting toolkit known for its scalability and flexibility.

    Grafana: A visualization and dashboarding platform commonly used with Prometheus.

    Nagios: An open-source monitoring system that can monitor hosts, services, and network devices.

    ELK Stack (Elasticsearch, Logstash, Kibana): A combination of tools used for log and data analytics.

    New Relic: A cloud-based observability platform that provides application performance monitoring (APM) and infrastructure monitoring.

    Datadog: A cloud-based monitoring and analytics platform for infrastructure, applications, and logs.

Challenges in Monitoring:

Monitoring can be challenging due to the following factors:

    Data Overload: Monitoring generates vast amounts of data. Filtering, analyzing, and making sense of this data can be overwhelming.

    Alert Fatigue: Excessive or irrelevant alerts can lead to alert fatigue, where operators become desensitized to alerts, potentially causing critical issues to be overlooked.

    Complex Environments: Modern IT environments are complex, with hybrid cloud setups, microservices, and containerized applications, making monitoring more intricate.

    Cost: Implementing a comprehensive monitoring system can be costly in terms of tools, infrastructure, and personnel.

    Security and Privacy: Monitoring often involves collecting sensitive data, which must be handled securely to comply with privacy regulations.

Effective monitoring requires a well-planned strategy, the right tools, and a focus on relevant metrics. It is an oning process that evolves with the changing needs of the organization and the technology landscape. Proper monitoring is essential for maintaining system reliability, optimizing performance, and delivering a positive user experience.



-------------------------------------------------------------------------------------------------------------------------
				
		Importance of monitoring in DevOps.
-------------------------------------------------------------------------------------------------------------------------

Monitoring is of paramount importance in DevOps for several compelling reasons:

    Early Issue Detection: Monitoring allows DevOps teams to detect issues, anomalies, and errors in real-time or near-real-time. Early detection is crucial for addressing problems before they escalate into major incidents or outages.

    Proactive Problem Resolution: By continuously monitoring key performance indicators (KPIs) and system metrics, DevOps teams can proactively identify and address potential issues, reducing downtime and minimizing the impact on users.

    Improved Reliability: Monitoring helps maintain the reliability and availability of systems and applications. It ensures that services are up and running as expected, reducing the risk of service disruptions.

    Performance Optimization: Monitoring provides insights into system performance, allowing for the identification of bottlenecks, resource constraints, and other performance-related issues. This information helps in optimizing system performance and resource utilization.

    Resource Efficiency: Monitoring enables efficient resource management by identifying underutilized or overutilized resources. DevOps teams can allocate resources effectively and save costs by optimizing resource usage.

    Enhanced User Experience: Monitoring helps track user-centric metrics, such as response times and error rates. Ensuring a positive user experience is critical for customer satisfaction and retention.

    Automated Responses: DevOps teams can implement automated responses to certain types of issues, reducing the need for manual intervention. For example, auto-scaling can automatically adjust resource allocation in response to increased traffic.

    Continuous Improvement: Monitoring data provides valuable feedback for continuous improvement. DevOps teams can use historical data to make informed decisions about system architecture, code changes, and infrastructure upgrades.

    Security and Compliance: Monitoring helps detect security threats, vulnerabilities, and unauthorized access attempts. It plays a crucial role in ensuring compliance with security standards and regulations.

    Capacity Planning: By analyzing resource utilization trends, monitoring assists in capacity planning. DevOps teams can anticipate future resource requirements and scale infrastructure accordingly.

    Documentation and Reporting: Monitoring tools often provide reporting and documentation features. This documentation serves as a record of system behavior, making it easier to troubleshoot issues and conduct post-incident analysis.

    Feedback Loop: Monitoring creates a feedback loop that connects development, operations, and other stakeholders. Developers receive insights into how their code performs in production, fostering collaboration and accountability.

    Change Management: Monitoring helps assess the impact of code changes and new releases on system behavior. DevOps teams can roll back changes quickly if monitoring reveals adverse effects.

    Effective Incident Management: In the event of incidents or outages, monitoring data is invaluable for diagnosing the root cause and facilitating rapid incident resolution.

    Alignment with DevOps Principles: Monitoring aligns with core DevOps principles, such as automation, collaboration, and continuous improvement. It supports the DevOps al of delivering high-quality software at speed.

In summary, monitoring is an integral part of the DevOps lifecycle, enabling teams to maintain system health, improve performance, enhance security, and deliver a seamless user experience. It fosters a culture of proactive problem-solving, automation, and continuous improvement, all of which are central to the DevOps philosophy.

-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Nagios and ELK stack.
-------------------------------------------------------------------------------------------------------------------------

Nagios and the ELK Stack are two popular and powerful tools used in the realm of IT operations, monitoring, and log analysis. Let's explore each of them:
Nagios:

Nagios, originally known as Nagios Core, is an open-source monitoring and alerting system used to monitor the availability, performance, and status of various network resources, hosts, and services. It is highly extensible and has a vast community-driven ecosystem of plugins and extensions. Nagios is particularly valuable for system administrators and DevOps teams for ensuring the reliability and health of IT infrastructure.

Key Features and Components of Nagios:

    Core Engine: 
		Responsible for 
			scheduling and executing checks
			processing the results
			generating alerts based 
				on predefined thresholds and configurations.

    Plugins: 
		Nagios relies on plugins 
			to perform specific checks 
				on hosts and services. 
		can be custom-built 
		or 
		obtained from the Nagios Plugin Exchange and other sources.

    Configuration Files: 
		Nagios is highly configurable 
			through text-based configuration files. 
		Users can define 
			hosts, 
			services, 
			notification settings, and 
			more in these files.

    Web Interface: 
		user interface for 
			monitoring, 
			reporting, and 
			managing hosts and services. 
		The interface offers real-time status updates and history.

    Alerting: 
		Nagios can send alerts 
			via various methods, 
			including 
				email, 
				SMS, and 
				custom scripts. 
			It supports escalations and acknowledges alerts.

    Dashboards: 
		Nagios can be extended with 
			third-party dashboards 
			and 
			visualization tools 
				to provide more advanced reporting and data presentation.

    Scalability: 
		Nagios can be scaled horizontally and vertically 
			to monitor large and complex infrastructures. 
		Tools like Nagios XI provide enterprise-level scalability and features.

	NagiOS Architecture
	NagiOS Terminologies
		Plugins:
			Nagios plugins are small programs or scripts that perform the actual checks on hosts and services. These plugins are responsible for collecting information and reporting it back to Nagios. Common plugins include checking server uptime, disk space, or network connectivity.
		Host:
			In Nagios, a host represents a physical or virtual device that you want to monitor, such as a server, router, or switch.
		Service:
			A service is something running on a host that you want to monitor, such as a specific application, a web server, or a database service.
		Users:
			Nagios users are individuals or entities with permission to access the Nagios web interface and view monitoring data.
		Contact:
			A contact in Nagios is a person or entity that can be notified when a problem is detected with a host or service.
		Contact Group:
			A contact group is a collection of contacts. It allows you to group related contacts together for easier management of notifications.
		Acknowledgement:
			Acknowledgment is the process of recognizing and accepting a host or service problem. Acknowledging a problem indicates that the issue is known and being addressed.
		Downtime:
			Downtime is a period during which a host or service is temporarily not monitored. It's used to prevent unnecessary notifications during maintenance or planned outages.
		Latency:
			Latency refers to the delay between initiating a check on a host or service and receiving a response. Minimizing latency is important to ensure timely monitoring.
		Statement:
			This term isn't commonly associated with Nagios. It's possible that you are referring to configuration files or comments in the Nagios configuration.
		Agent:
			An agent in Nagios can refer to software installed on a remote host that collects and sends monitoring data back to the Nagios server. It's also known as a "NRPE (Nagios Remote Plugin Executor) agent."
		Host Groups:
			Host groups are used to organize and group related hosts together, making it easier to manage configuration and apply checks to multiple hosts at once.
		Service Group:
			Similar to host groups, service groups allow you to group related services together. This can simplify configuration and status monitoring.
		Commands:
			Nagios uses commands to execute plugins and define how checks are performed. Commands specify the plugins to run, parameters, and other details.
		Templates:
			Templates in Nagios are used to define common settings and attributes for hosts, services, and other objects. You can create templates to avoid repetitive configuration.

		Active vs. Passive Check:
			Active checks are initiated by the Nagios server, which actively sends requests to hosts and services to collect data. Passive checks, on the other hand, involve the hosts and services sending data to the Nagios server without being actively polled.

		Soft vs. Hard State:
			In Nagios, soft states typically indicate transient issues, while hard states represent persistent problems. Soft states can change to hard states if a problem persists for a defined period or reaches a certain threshold.

	Use Cases and features of Nagios:

		Network Monitoring: Nagios can monitor network devices, such as routers, switches, and firewalls, to ensure they are functioning correctly.

		Server Monitoring: Nagios can monitor server health, resource utilization (CPU, memory, disk space), and services running on servers.

		Environmental Monitoring: Nagios can monitor environmental conditions such as temperature, humidity, and power supply in data centers.

		Custom Monitoring: Users can create custom checks to monitor any aspect of their infrastructure or applications.

		Host and Service Monitoring: 
			Nagios can monitor the status and performance of hosts (servers) and services (applications) using checks and plugins.
		Alerting: 
			Nagios can send alerts (via email, SMS, or other methods) when it detects issues, allowing administrators to take timely actions.
		Distributed Monitoring: 
			It supports distributed monitoring setups, making it suitable for large and complex environments.
		Extensibility: 
			Nagios is highly extensible, allowing users to create custom checks and plugins to monitor specific services or devices.
		Historical Data: 
			Nagios can store historical performance and event data for reporting and analysis.

	Components:
		Nagios Core: 
			The core monitoring engine that executes checks and handles alerting.
		Nagios Plugins: 
			External executables or scripts used to perform checks.
		NRPE (Nagios Remote Plugin Executor): 
			Allows remote execution of Nagios plugins on monitored hosts.
		Nagios XI: 
			A commercial version of Nagios with additional features and a web-based user interface.



ELK Stack (Elasticsearch, Logstash, and Kibana):
ELK Stack:

The ELK Stack is a set of three open-source tools used for log management and log analysis. The acronym "ELK" stands for:

    Elasticsearch: 
		distributed
		NOSQL database 
		based on lucene search engine 
			designed to store logs 
		RESTful 
			search and analytics engine. 
		can 
			store, 
			process
			search, and 
			analyze 
				large volumes of data
				including logs and 
				event data.
				

    Logstash: 
		Logstash is 
			data processing pipeline 
				ingests, 
				processes, and 
				transforms 
					log data from various sources and 
				sends it to Elasticsearch for storage and analysis.
		pipeline tool 
		accepts input from various sources 
		exports data to various targets 

    Kibana: 
		visualization and exploration tool 
		provides a web interface for 
			searching, 
			analyzing, and 
			visualizing 
				log data stored in Elasticsearch. 
		It allows users to create custom dashboards and reports.

Key Features and Components of the ELK Stack:

    Log Collection: Logstash collects log data from various sources, including log files, syslog, and network flows.

    Data Transformation: Logstash can filter, parse, and enrich log data, making it more structured and suitable for analysis.

    Data Storage: Elasticsearch stores the processed log data in a highly scalable and distributed manner, allowing for fast and efficient searching and querying.

    Data Visualization: Kibana provides a user-friendly interface for exploring log data, creating visualizations, and generating reports and dashboards.

    Real-Time Analysis: The ELK Stack supports real-time log analysis, making it valuable for monitoring and troubleshooting applications and infrastructure.

	Log Centralization: ELK enables organizations to centralize log data from various sources, making it easier to search and analyze logs in one place.
	Real-time Monitoring: Users can monitor log data in real-time, allowing for immediate detection and response to issues.
	Search and Analysis: Elasticsearch provides powerful search capabilities, and Kibana offers customizable dashboards and visualizations.
	Scalability: ELK is horizontally scalable, allowing it to handle large volumes of log data.
	Alerting: Users can set up alerts and notifications based on log data trends and patterns.


Use Cases of the ELK Stack:

    Log Management: The ELK Stack is widely used for centralized log management, where logs from various sources are aggregated, indexed, and made searchable.

    Application Monitoring: It helps monitor the performance and behavior of applications by analyzing logs generated by application servers and services.

    Security Information and Event Management (SIEM): The ELK Stack is used for security monitoring, threat detection, and incident response by analyzing security logs and events.

    DevOps and Infrastructure Monitoring: It assists DevOps teams in monitoring infrastructure, containers, and microservices by collecting and analyzing log data from various components.

    Business Intelligence: The ELK Stack can be used for business intelligence and reporting by analyzing logs containing business metrics and transactions.

Both Nagios and the ELK Stack are valuable tools in the IT operations and monitoring toolkit. Nagios excels in real-time monitoring and alerting, while the ELK Stack is ideal for log management, analysis, and visualization. Many organizations use both tools in tandem to ensure comprehensive monitoring and visibility into their IT environments.


-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up basic monitoring using Nagios, visualizing logs with ELK.
-------------------------------------------------------------------------------------------------------------------------
Nagios using docker 
	Install docker, portainer 
	Create nagios and nagvis container refer link below 
	Add host on nagios monitoring server 
	Create a map where you would monitoring host
	https://blog.itsupport.com.bd/2023/02/step-by-step-to-build-network.html
	https://mpolinowski.github.io/docs/DevOps/Zabbix/2020-07-12--nagios-docker-install/2020-07-12/
	
	
	cd /usr/local/nagios/
	cd etc
	ls
	cp nagios.cfg nagios.cfg.backup 
	vi nagios.cfg
	uncomment #monitoring a router/switch 
	mkdir switches
	ls 
	cd switches 
	vi setlrouter.cfg
	
	
Adding a linux host 
	https://www.tutorialspoint.com/how-to-add-linux-host-to-nagios-monitoring-server-using-nrpe-plugin

https://www.tutorialspoint.com/nagios/index.htm	

https://www.tutorialspoint.com/logstash/logstash_elk_stack.htm
https://phoenixnap.com/kb/elk-stack-docker
https://logz.io/learn/docker-monitoring-elk-stack/
	
echo "define host{ \
   use                     linux-server \
   host_name               linux-host \
   alias                   Linux Host \
   address                 Linux-Host-IP \
} " >> hosts.cfg
	

Nagios
	Setting up basic monitoring using Nagios involves several steps, including installing Nagios, configuring hosts and services to monitor, and setting up alerts. In this hands-on guide, I'll provide an overview of the process.

	Note: This is a simplified guide to help you get started with basic monitoring using Nagios. For a production environment, consider security, scalability, and best practices.

	Prerequisites:

		A Linux-based server (Ubuntu or CentOS is commonly used).
		Root or sudo access to the server.

	Step 1: Install Nagios Core
https://support.nagios.com/kb/article/nagios-core-installing-nagios-core-from-source-96.html#Ubuntu

		Update your package list:

		sql

	sudo apt update

	Install necessary packages:

	For Ubuntu:

	sudo apt install -y autoconf gcc libc6 make wget unzip apache2 php libapache2-mod-php7.4 libgd-dev

	For CentOS:

	

	sudo yum install -y httpd php gcc glibc glibc-common gd gd-devel make net-snmp

	Download Nagios Core and plugins:


	wget https://assets.nagios.com/downloads/nagioscore/releases/nagios-4.4.6.tar.gz
	wget https://nagios-plugins.org/download/nagios-plugins-2.3.3.tar.gz

	Extract the archives:

	tar -zxvf nagios-4.4.6.tar.gz
	tar -zxvf nagios-plugins-2.3.3.tar.gz

	Compile and install Nagios Core:

	cd nagios-4.4.6
	./configure --with-command-group=nagcmd
	make all
	sudo make install
	sudo make install-init
	sudo make install-config
	sudo make install-commandmode
	sudo make install-webconf

	Create a Nagios admin user:

	sudo htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin

	Start Apache web server:

	For Ubuntu:

	

	sudo systemctl enable apache2
	sudo systemctl start apache2

	For CentOS:

	
	sudo systemctl enable httpd
	sudo systemctl start httpd

	Start Nagios and enable it to start on boot:

	

		sudo systemctl enable nagios
		sudo systemctl start nagios

		Access Nagios web interface by ing to http://your-server-IP/nagios in your web browser. Log in with the nagiosadmin credentials.

	Step 2: Configure Hosts and Services

		Define hosts and services you want to monitor in the Nagios configuration files located in /usr/local/nagios/etc/objects/.

		Example configuration:

			Define a host in localhost.cfg:

			sql

	define host {
	  use       linux-server
	  host_name localhost
	  alias    My Local Machine
	  address  127.0.0.1
	}

	Define a service in commands.cfg:

	perl

		define service {
		  use                 local-service
		  host_name           localhost
		  service_description PING
		  check_command       check_ping!100.0,20%!500.0,60%
		}

	Verify your configurations:

	

	sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg

	If there are no errors, restart Nagios:

		sudo systemctl restart nagios

	Step 3: Set Up Alerts

		Configure Nagios to send email notifications by editing /usr/local/nagios/etc/objects/contacts.cfg. Add your email address to the email parameter.

		Configure the contact group and define how notifications should be sent in contactgroups.cfg.

		Define commands for notifications in commands.cfg.

		Link your contacts and contact groups to hosts and services in the appropriate configuration files.

		Verify the configuration:

		

	sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg

	Restart Nagios for the changes to take effect:

		sudo systemctl restart nagios

	You have now set up basic monitoring using Nagios. You can continue to define more hosts and services, configure alerting rules, and customize your monitoring setup according to your specific requirements.

	
nagios
https://assets.nagios.com/downloads/nagiosxi/docs/How-to-Monitor-Docker-Containers-with-Nagios-XI.pdf
https://sysadminxpert.com/step-by-step-method-for-installing-nagios-in-amazon-linux/


official elastic search install 
-------------------------------
https://www.elastic.co/guide/en/elastic-stack/current/installing-elastic-stack.html
https://www.elastic.co/guide/en/elasticsearch/reference/8.10/install-elasticsearch.html


docker network create elastic
docker pull docker.elastic.co/elasticsearch/elasticsearch:8.10.3

sudo sysctl -w vm.max_map_count=262144
docker run -d --name es01 --net elastic -p 9200:9200 -it docker.elastic.co/elasticsearch/elasticsearch:8.10.3

docker logs continer-id
--------------
Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  U=z=CXwJdc*AH9M01v23

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  ce21d45b0c77d51e925b15f93f0b5a7cf05d6d819e2850d5979b75a9e13b5787

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjEwLjMiLCJhZHIiOlsiMTcyLjE4LjAuMjo5MjAwIl0sImZnciI6ImNlMjFkNDViMGM3N2Q1MWU5MjViMTVmOTNmMGI1YTdjZjA1ZDZkODE5ZTI4NTBkNTk3OWI3NWE5ZTEzYjU3ODciLCJrZXkiOiItOVJZSDRzQmFLM1JGS0F5TDhOZzp4d1I5QzJDTVJ2YW5MdFZPbXVpcy1nIn0=

ℹ️ Configure other nodes to join this cluster:
• Copy the following enrollment token and start new Elasticsearch nodes with `bin/elasticsearch --enrollment-token <token>` (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjEwLjMiLCJhZHIiOlsiMTcyLjE4LjAuMjo5MjAwIl0sImZnciI6ImNlMjFkNDViMGM3N2Q1MWU5MjViMTVmOTNmMGI1YTdjZjA1ZDZkODE5ZTI4NTBkNTk3OWI3NWE5ZTEzYjU3ODciLCJrZXkiOiItZFJZSDRzQmFLM1JGS0F5TDhNdTo4WFBhR0U3WFQ2R0xFSjZJZHNSUkJBIn0=

  If you're running in Docker, copy the enrollment token and run:
  `docker run -e "ENROLLMENT_TOKEN=eyJ2ZXIiOiI4LjEwLjMiLCJhZHIiOlsiMTcyLjE4LjAuMjo5MjAwIl0sImZnciI6ImNlMjFkNDViMGM3N2Q1MWU5MjViMTVmOTNmMGI1YTdjZjA1ZDZkODE5ZTI4NTBkNTk3OWI3NWE5ZTEzYjU3ODciLCJrZXkiOiItZFJZSDRzQmFLM1JGS0F5TDhNdTo4WFBhR0U3WFQ2R0xFSjZJZHNSUkJBIn0=" docker.elastic.co/elasticsearch/elasticsearch:8.10.3`

-----------------



export ELASTIC_PASSWORD="U=z=CXwJdc*AH9M01v23"
docker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .
curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200

To add more elasticsearch node refer documentation 
-----------------

Kibana
------


docker pull docker.elastic.co/kibana/kibana:8.10.3

docker run -d --name kib01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.10.3
access 5601 port 
	provide token 
	bin\kibana-verification-code


	if no response
		check docker logs
		refresh if required
Logstash 		
https://www.elastic.co/guide/en/logstash/8.10/docker.html



docker pull docker.elastic.co/logstash/logstash:8.10.3
https://salithachathuranga94.medium.com/integrate-elk-stack-into-spring-boot-application-ae38a6371f86
	docker run -d --rm --link your_elasticsearch_container_id:elasticsearch docker.elastic.co/logstash/logstash:8.10.3

References:
https://stackoverflow.com/questions/60120210/kibana-not-connecting-to-elasticsearch-warning-elasticsearch-admin-pid/60122043#60122043

	Create a spring boot application 
		use logger to log item

spring:
  application:
    name: vilas
server:
  port: 
  
logging:
  file: /c/abc.txt
  +ive and -ive should be loged to log file
  

elk-stack.log 
create logstash.conf in logstash/bin 
	mention the logfile location 
	logstash -f logstash/conf 
	run on port 9600



-------------------------------------------------------------------------------------------------------------------------
				
	3	Security in DevOps
-------------------------------------------------------------------------------------------------------------------------


Security in DevOps is a critical aspect of modern software development practices. It involves integrating security practices and measures into the entire software development lifecycle, from planning and design to development, testing, deployment, and operations. The al is to ensure that security is not a separate or isolated concern but an integral part of the DevOps process. Here are key principles, practices, and considerations for implementing security in DevOps:

1. Shift-Left Security:

    Shift-left security means moving security activities and testing as early as possible in the software development process.
    This approach helps identify and address security vulnerabilities and issues in the early stages, reducing the cost and effort required to fix them.

2. Collaboration and Communication:

    Foster collaboration and communication between development, operations, and security teams.
    Ensure that security requirements and best practices are clearly communicated and understood by all team members.

3. Threat Modeling:

    Conduct threat modeling exercises to identify potential security threats and vulnerabilities in the design and architecture of the application.
    Prioritize and address these threats based on their potential impact and likelihood.

4. Secure Coding Practices:

    Train developers in secure coding practices to write code that is resistant to common security vulnerabilities such as SQL injection, XSS (Cross-Site Scripting), and CSRF (Cross-Site Request Forgery).
    Use static analysis tools to automatically detect and flag security issues in the codebase.

5. Continuous Security Testing:

    Implement automated security testing tools and practices throughout the development pipeline.
    Include static code analysis, dynamic application security testing (DAST), and software composition analysis (SCA) in the CI/CD process.

6. Vulnerability Management:

    Establish processes for tracking, prioritizing, and remedying security vulnerabilities in third-party libraries and components (known as Software Composition Analysis).
    Ensure that patches and updates are applied promptly.

7. Infrastructure as Code (IaC) Security:

    Implement security practices for managing infrastructure as code templates.
    Regularly audit and scan infrastructure code for security vulnerabilities.

8. Access Control and Identity Management:

    Implement strong authentication and authorization mechanisms.
    Follow the principle of least privilege to restrict access based on roles and responsibilities.

9. Container Security:

    Apply security best practices to container images.
    Use container scanning tools to identify vulnerabilities in containerized applications.

10. Secrets Management:
- Safely store and manage sensitive information such as API keys, passwords, and certificates.
- Utilize secret management tools and practices to protect these credentials.

11. Security Monitoring and Incident Response:
- Implement real-time security monitoring to detect and respond to security incidents.
- Develop incident response plans and conduct tabletop exercises to prepare for security breaches.

12. Compliance and Auditing:
- Ensure compliance with industry regulations and security standards (e.g., GDPR, HIPAA, PCI DSS).
- Conduct regular security audits and assessments to maintain compliance.

13. Cloud Security:
- Apply cloud-specific security best practices when using cloud services and infrastructure.
- Utilize cloud-native security tools and services.

14. Security Culture:
- Promote a security-aware culture within the organization.
- Educate all team members about security risks and best practices.

15. Continuous Improvement:
- Regularly review and update security practices and measures to adapt to evolving threats and technologies.

Security in DevOps is an oning effort that requires continuous monitoring, assessment, and adaptation. It's essential to strike a balance between security and agility to deliver secure, high-quality software at the speed of DevOps.


-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to SAST/DAST tools.
-------------------------------------------------------------------------------------------------------------------------

Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) are two distinct but complementary approaches to identifying and mitigating security vulnerabilities in software applications. Here's an introduction to SAST and DAST tools:

Static Application Security Testing (SAST):

SAST is a white-box testing method that analyzes the source code, bytecode, or binary code of an application to identify security vulnerabilities without executing the application. It examines the codebase for potential issues based on coding rules, security standards, and known vulnerability patterns. SAST tools are typically used during the development phase to catch issues early in the software development lifecycle.

Key features and characteristics of SAST tools include:

    Source Code Analysis: SAST tools analyze the application's source code to identify security vulnerabilities, coding errors, and potential weaknesses.

    Early Detection: SAST can identify issues in the code as soon as it is written, enabling developers to address them before they become ingrained in the application.

    High False Positives: SAST tools may produce false positive results, requiring manual review and validation to determine the true positives.

    Language and Platform Support: SAST tools support various programming languages and development platforms, making them versatile for different projects.

    Integration with IDEs: Some SAST tools integrate with integrated development environments (IDEs) to provide real-time feedback to developers as they write code.

Examples of SAST tools include Checkmarx, Fortify (HP Fortify), Veracode, and SonarQube (which also provides SAST capabilities).

Dynamic Application Security Testing (DAST):

DAST is a black-box testing method that assesses a running application from the outside to identify security vulnerabilities and weaknesses. Unlike SAST, DAST tools do not require access to the application's source code. Instead, they interact with the application in its deployed state, simulating attacks and analyzing the responses to identify potential vulnerabilities.

Key features and characteristics of DAST tools include:

    Realistic Testing: DAST tools provide a real-world perspective by interacting with the application as an external attacker would.

    Runtime Analysis: DAST tools analyze the application's behavior during runtime, which is valuable for identifying vulnerabilities that may not be evident in the source code.

    Low False Positives: DAST tools typically produce fewer false positives compared to SAST, as they assess the application's actual behavior.

    Scanning Web Applications: DAST is commonly used for web application security testing, including testing web services, APIs, and web interfaces.

    Authentication Support: DAST tools can handle authentication mechanisms, such as login forms and session management, to assess authenticated areas of the application.

Examples of DAST tools include OWASP ZAP (Zed Attack Proxy), Burp Suite, Nessus, and Qualys.

Complementary Use:

While SAST and DAST are distinct approaches, they are often used together to provide comprehensive security testing coverage:

    SAST identifies vulnerabilities in the source code and helps developers fix them during development.

    DAST complements SAST by assessing the runtime behavior of the application, identifying vulnerabilities that may not be apparent in the source code, and simulating real-world attack scenarios.

Using both SAST and DAST tools in combination, organizations can enhance their overall application security posture by addressing vulnerabilities at different stages of the software development lifecycle and under different testing conditions.



	Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) are two essential cateries of tools used in the field of application security to identify and mitigate security vulnerabilities. Each type of tool serves a specific purpose in securing software applications.

Static Application Security Testing (SAST):

Introduction:

SAST, also known as "white-box testing," is a type of security testing that analyzes the source code, bytecode, or binary code of an application to identify vulnerabilities, coding errors, and security issues.
SAST tools scan the application's codebase without executing the application itself.
Key Features:

Source Code Analysis: SAST tools analyze the source code, including comments, to identify security vulnerabilities.
Early Detection: SAST identifies vulnerabilities early in the development process, helping developers fix issues before deployment.
Static Analysis: SAST is a static analysis technique that doesn't require running the application, making it suitable for early-stage security assessments.
Integration: SAST tools can be integrated into the development environment, such as IDEs or CI/CD pipelines.
Use Cases:

Identifying vulnerabilities like SQL injection, Cross-Site Scripting (XSS), and insecure authentication mechanisms.
Ensuring code complies with security coding standards and best practices.
Providing developers with actionable security feedback during code development.
Dynamic Application Security Testing (DAST):

Introduction:

DAST, also known as "black-box testing," is a type of security testing that analyzes a running application to identify vulnerabilities and security weaknesses from an external perspective.
DAST tools interact with the application as a user would, sending requests and analyzing responses.
Key Features:

Runtime Testing: DAST tools assess the application during runtime, simulating real-world attack scenarios.
External Perspective: DAST tools examine the application from the outside, which helps identify vulnerabilities that may not be apparent from the source code alone.
Security Scanning: DAST tools perform security scans by sending various requests and payloads to the application.
Use Cases:

Identifying vulnerabilities like input validation flaws, security misconfigurations, and vulnerabilities that manifest only during runtime.
Assessing the security posture of deployed applications, including web applications and APIs.
Providing a hacker's perspective on an application's security.
Comparison:

SAST is used early in the development lifecycle and focuses on source code and static analysis. It provides detailed insights into code-level vulnerabilities and helps developers write more secure code.

DAST is used to test applications in their running state and assesses security from an external perspective. It's typically used in later stages of development or after deployment to identify runtime vulnerabilities.

SAST can find issues in the codebase but may produce false positives or false negatives. It requires expertise to interpret results effectively.

DAST evaluates applications as they are deployed and running. It provides more realistic testing but may not uncover all code-level vulnerabilities.

Many organizations use a combination of SAST and DAST tools to achieve comprehensive application security testing. By using both approaches, they can identify and mitigate vulnerabilities at different stages of the software development lifecycle, reducing the risk of security breaches and data breaches.


Best SAST Tools:

Fortify by Micro Focus:

Fortify offers a comprehensive SAST solution that helps identify and remediate security vulnerabilities in source code and binaries.
It supports multiple programming languages and offers integration with various development and CI/CD tools.
Checkmarx:

Checkmarx provides SAST solutions for identifying and managing security vulnerabilities in applications.
It offers integration with popular IDEs and CI/CD pipelines and supports various programming languages.
Veracode:

Veracode offers a cloud-based SAST platform that analyzes code for security vulnerabilities.
It provides extensive reporting and remediation guidance and integrates well with CI/CD processes.
SonarQube:

SonarQube is an open-source platform for continuous inspection of code quality and security.
It provides SAST capabilities alongside code quality and technical debt analysis.
Best DAST Tools:

Nessus:

Nessus is a widely used vulnerability scanner that offers both network and web application scanning capabilities.
It can identify web application vulnerabilities by scanning the application from an external perspective.
Burp Suite:

Burp Suite is a popular security testing tool for web applications. It offers a free and a paid version with more advanced features.
It provides both manual and automated testing of web application security.
OWASP ZAP (Zed Attack Proxy):

ZAP is an open-source security testing tool developed by OWASP.
It is designed for automated and manual testing of web applications for security vulnerabilities.
Acunetix:

Acunetix is a web vulnerability scanner that offers automated DAST capabilities for web application security testing.
It provides detailed reports and integration options with various tools.
Qualys Web Application Scanning (WAS):

Qualys WAS is a cloud-based DAST tool that helps organizations identify and remediate web application vulnerabilities.
It offers scalability and easy integration with other Qualys security solutions.
Remember that the choice of SAST and DAST tools may depend on factors such as the programming languages used, integration requirements, scalability, and budget. Additionally, many organizations use a combination of both SAST and DAST tools to achieve comprehensive security testing coverage. It's essential to evaluate these tools based on your specific needs and requirements to determine which ones are the best fit for your organization's application security testing strategy.



-------------------------------------------------------------------------------------------------------------------------
				
		Importance of security in CI/CD pipelines.
-------------------------------------------------------------------------------------------------------------------------



Security in CI/CD (Continuous Integration and Continuous Deployment/Delivery) pipelines is of paramount importance for several compelling reasons:

    Early Detection of Vulnerabilities: Integrating security into CI/CD allows for the early detection of security vulnerabilities in the code. Automated security scans can identify issues as soon as code changes are committed, enabling developers to address them before they propagate further into the pipeline.

    Reduced Remediation Costs: Addressing security issues early in the development process is significantly less costly and time-consuming than dealing with them after deployment. Quick remediation reduces the risk of security breaches and their associated financial and reputational consequences.

    Alignment with DevOps Principles: CI/CD is a core DevOps practice focused on automation and collaboration. Integrating security seamlessly into CI/CD aligns security practices with DevOps principles, promoting collaboration between development, operations, and security teams.

    Faster Release Cycles: Traditional security testing methods can introduce delays in the release process. Automated security testing in CI/CD ensures that security checks are performed quickly and efficiently, enabling faster and more frequent releases.

    Continuous Compliance: CI/CD pipelines can include security compliance checks to ensure that applications meet regulatory and security standards throughout the development process. This minimizes compliance-related risks.

    Increased Visibility: Security checks in CI/CD provide visibility into the security posture of the codebase and infrastructure at every stage of the pipeline. This transparency enables informed decision-making.

    Automated Remediation: Some CI/CD security tools offer automated remediation suggestions or actions, allowing for immediate fixes or mitigation steps without manual intervention.

    Scalability and Consistency: CI/CD pipelines can scale security testing to match the pace of development, ensuring that security practices are consistent across all projects and releases.

    Security as Code: Treating security as code means defining security policies and checks as code within the CI/CD pipeline. This approach makes it easier to version, track, and manage security configurations.

    Shift-Left Security: Integrating security into CI/CD follows the "shift-left" principle, which emphasizes early and continuous security testing and mitigation throughout the software development lifecycle. This approach identifies and addresses security issues closer to their source.

    Threat Mitigation: Security in CI/CD helps organizations proactively address threats, such as vulnerabilities in third-party dependencies, code injection, authentication issues, and misconfigurations.

    Enhanced Collaboration: Collaboration between security, development, and operations teams is critical for effective security practices. Integrating security into CI/CD encourages this collaboration by making security a shared responsibility.

    Continuous Feedback: CI/CD provides immediate feedback on the security of code changes. Developers can view security reports and alerts in the same pipeline where they see build and test results, fostering a culture of continuous improvement.

In summary, security in CI/CD pipelines is essential for ensuring that applications are developed and deployed with strong security measures. It promotes a proactive approach to security, aligns with DevOps principles, reduces risk, and enhances the overall security posture of organizations, ultimately leading to more secure and resilient software.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Running a basic security scan on a sample application.
-------------------------------------------------------------------------------------------------------------------------


Running a basic security scan on a sample application typically involves using security scanning tools and services to identify vulnerabilities or security issues in the application's code or configuration. Here's a hands-on guide for running a basic security scan on a sample web application:

Prerequisites:

    A sample web application (e.g., a simple web application built with HTML, JavaScript, and a server-side language like Python or Node.js).

    Access to a security scanning tool or service. In this example, we'll use OWASP ZAP (Zed Attack Proxy) for dynamic application security testing (DAST).

Step 1: Install OWASP ZAP

You can download and install OWASP ZAP from the official website: OWASP ZAP Download(https://www.zaproxy.org/download/)

Step 2: Start OWASP ZAP

After installing ZAP, start the application.

Step 3: Configure ZAP for Scanning

    Launch ZAP and  to the "Tools" menu.
    Select "Options" to open the options window.
    In the "Local Proxies" section, specify the listening host and port (e.g., localhost and 8080) for ZAP's proxy.
    Save the changes and close the options window.

Step 4: Configure Your Web Browser

To run the scan, you need to configure your web browser to use ZAP as a proxy:

    Open your web browser's settings.
    Configure the proxy settings to use localhost and the port you specified in ZAP (e.g., 8080) as the HTTP proxy.

Step 5: Navigate the Application

    Start your web application on a local server (e.g., using Python's http.server or Node.js's http-server).
    Access your web application in the browser, making sure the requests  through ZAP.

Step 6: Spider the Application

In ZAP, use the spider tool to crawl your web application:

     to the "Spider" tab in ZAP.
    Enter the URL of your web application (e.g., http://localhost:your-port).
    Click "Start Scanning" to begin the spidering process.
    ZAP will crawl your application, discover links, and build a site map.

Step 7: Perform an Active Scan

After the spidering is complete, perform an active scan to identify vulnerabilities:

     to the "Active Scan" tab in ZAP.
    Select the site you want to scan from the dropdown (your application).
    Click "Start Scan" to begin the active scan process.
    ZAP will send various requests and analyze responses to identify vulnerabilities.

Step 8: View Scan Results

Once the scan is complete, review the scan results in ZAP:

     to the "Alerts" tab to view a list of detected vulnerabilities.
    Click on each vulnerability to get more details, including the affected URL and description.

Step 9: Remediate Vulnerabilities

Review the detected vulnerabilities and work on remediating them in your sample application. This may involve fixing code issues, implementing security controls, or making configuration changes.

Step 10: Re-Scan

After addressing the vulnerabilities, you can repeat the scanning process to ensure that the issues have been resolved.

Running a basic security scan with OWASP ZAP is just one example. Different tools and services may have varying procedures, but the general process involves configuring the tool, scanning the application, reviewing results, and remediating vulnerabilities. For more comprehensive security testing, consider using a combination of static analysis tools (SAST) and dynamic analysis tools (DAST) as part of your CI/CD pipeline.



------------------------
Running a basic security scan on a sample application typically involves using a security testing tool or service to identify common vulnerabilities and weaknesses in the application. Here are the steps to run a basic security scan on a sample web application using an open-source tool called OWASP ZAP (Zed Attack Proxy):

Prerequisites:

Install OWASP ZAP: Download and install OWASP ZAP from the official website (https://www.zaproxy.org/download/).
Steps to Run a Basic Security Scan:

Launch OWASP ZAP:

Open OWASP ZAP after installation.
You may need to configure your browser to use ZAP as a proxy. ZAP will intercept and inspect web traffic to the target application.
Configure ZAP Options:

Go to the "Tools" menu and select "Options."
Set up the local proxy settings and other preferences as needed.
Create a New Session:

In the ZAP workspace, create a new session for your scan. Click on "File" > "New Session."
Configure the Target:

Specify the target URL of the web application you want to scan. This is the application you want to test for security vulnerabilities.
Go to the "Quick Start" tab and enter the target URL in the "URL to attack" field.
Spider the Application:

Use ZAP's spider tool to crawl the web application and discover its pages and functionality.
Go to the "Spider" tab and click the "Start" button.
Active Scan:

Perform an active scan to identify vulnerabilities actively. ZAP will send malicious payloads to the application and analyze the responses.
Go to the "Active Scan" tab and click the "Start Scan" button.
Passive Scan:

ZAP also performs passive scans by analyzing responses from the application for known vulnerabilities and issues.
Passive scanning is ongoing while you use the application.
Review Scan Results:

Once the scans are complete, go to the "Alerts" tab to review the findings.
ZAP will list security vulnerabilities and provide details about each issue, including severity and recommendations for remediation.
Generate Reports:

ZAP allows you to generate reports summarizing the scan results. This is useful for sharing findings with the development team.
Go to the "Report" tab and choose the report format (e.g., HTML, XML) and generate the report.
Remediate Vulnerabilities:

After reviewing the scan results, prioritize and remediate the identified vulnerabilities in your application's code.
Make the necessary code changes and retest the application to ensure the vulnerabilities are resolved.
Re-Scan:

It's essential to re-scan the application after remediation to confirm that the security issues have been successfully addressed.
Continuous Monitoring:

For production applications, consider implementing continuous security monitoring and periodic security scans to detect new vulnerabilities as they arise.
Please note that this is a basic example of how to run a security scan using OWASP ZAP. More advanced tools and services may provide additional features and customization options based on your specific security testing requirements. Additionally, it's essential to follow secure coding practices and perform thorough security testing throughout the software development lifecycle to ensure the security of your applications.





Using sonar 
-----------


Running a security scan using SonarQube involves configuring and executing security-focused code analysis rules on your source code. SonarQube is an open-source platform for continuous code quality inspection that can be extended with security-focused plugins and rules. Here are the general steps to run a security scan using SonarQube:

Prerequisites:

Install and set up SonarQube on a server or as a container. You can download it from the official website (https://www.sonarqube.org/downloads/).
Steps to Run a Security Scan with SonarQube:

Install and Configure SonarQube Scanner:

Download and install the SonarQube Scanner on your local machine or CI/CD server.
Configure the scanner by specifying the SonarQube server URL and other parameters in the sonar-scanner.properties file or by passing them as command-line arguments.
Install SonarQube Security Plugins:

SonarQube supports various security plugins, such as OWASP Dependency-Check and OWASP ESLint. Depending on the programming languages and technologies used in your project, install the relevant security plugins through the SonarQube interface.
Create or Import a Project:

In the SonarQube web interface, create a new project or import an existing one from your version control system (e.g., Git, SVN).
Configure Project Properties:

Specify project-specific properties, such as the project's name, key, and source code location in the SonarQube project settings.
Analyze the Project:

Run the SonarQube Scanner to analyze the project's source code. Execute the following command in your project's root directory:



sonar-scanner
The scanner will analyze the code and send the results to the SonarQube server.

Review Security Issues:

Access the SonarQube web interface and navigate to your project to review the analysis results.
Focus on the "Security Hotspots" and "Vulnerabilities" sections to identify security issues.
Remediate Security Issues:

Review the security issues reported by SonarQube and prioritize them based on severity.
Work with your development team to remediate the identified security vulnerabilities and hotspots.
Re-Scan and Verify Fixes:

After addressing the issues, re-run the SonarQube scan to ensure that the security vulnerabilities have been resolved.
Regularly monitor the codebase for new security issues as you continue development.
Integrate into CI/CD Pipeline:

To ensure that security scans are performed automatically as part of your CI/CD pipeline, integrate the SonarQube analysis into your build and deployment process.
Generate Reports:

SonarQube provides the option to generate detailed reports of the analysis results for sharing with the development team and stakeholders.
Running security scans using SonarQube is an effective way to identify and remediate security vulnerabilities in your code. It provides continuous visibility into the security of your applications and helps you enforce secure coding practices throughout the development lifecycle.

-------------------------------------------------------------------------------------------------------------------------
				
	4	Containerization & Registries
-------------------------------------------------------------------------------------------------------------------------

Containerization is a technology used in modern software development and deployment that allows applications and their dependencies to be packaged together into a single unit called a container. Containers are lightweight, portable, and isolated environments that can run consistently across different infrastructure, including development, testing, and production environments. The most commonly used containerization technology is Docker, although other containerization solutions like containerd and Podman are also used.

Key concepts and benefits of containerization include:

    Isolation: Containers provide process and file system isolation, ensuring that applications and their dependencies do not interfere with each other. This isolation enhances security and stability.

    Portability: Containers encapsulate applications and all their dependencies, making them highly portable. A container can run on any system that supports the containerization technology, regardless of underlying infrastructure differences.

    Consistency: Containers ensure consistent application behavior across various environments, reducing "it works on my machine" issues and streamlining the development and deployment process.

    Resource Efficiency: Containers share the host operating system's kernel, which makes them lightweight and efficient in terms of resource utilization.

    Scalability: Containers can be easily scaled up or down to accommodate varying workloads. Container orchestration platforms like Kubernetes automate this process.

    Version Control: Container images are versioned, making it easy to track changes to applications and roll back to previous versions if needed.

    Continuous Integration/Continuous Deployment (CI/CD): Containers are a fundamental building block for CI/CD pipelines, enabling rapid and consistent application deployment.

Container Registries:

A container registry is a repository for storing and sharing container images. Container registries play a crucial role in the containerization ecosystem, as they provide a central location for developers to publish and distribute container images.

Key characteristics and concepts related to container registries include:

    Image Storage: Container registries store container images, which are usually composed of a base image (e.g., a minimal Linux distribution) and application-specific layers.

    Versioning: Container images are versioned to track changes and ensure consistency. A version tag (e.g., v1.0) is used to identify a specific image version.

    Access Control: Registries often include access control mechanisms to restrict who can push or pull images. This is crucial for security and compliance.

    Public and Private Registries: Some container registries are public and can be accessed by anyone, while others are private and require authentication. Private registries are often used for proprietary or sensitive applications.

    Distributed: Container registries can be distributed to ensure redundancy and high availability, especially in production environments.

    Registry Providers: Several cloud providers offer container registry services, such as Amazon Elastic Container Registry (ECR), ogle Container Registry (GCR), and Azure Container Registry (ACR). These services simplify image storage and distribution.

Popular container registry software includes:

    Docker Hub: A public registry provided by Docker, Inc.
    Quay.io: A cloud-hosted container registry service.
    Harbor: An open-source container registry that can be self-hosted.
    Artifactory: A universal repository manager that supports Docker images.

In summary, containerization is a technology that packages applications and their dependencies for portability and consistency, while container registries are repositories for storing and sharing container images. Together, they enable the efficient deployment, scaling, and management of applications in modern software development and deployment practices.

-------------------------------------------------------------------------------------------------------------------------
				
		Basics of containerization.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Introduction to Docker and Container Registries.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Creating a Docker image, pushing it to a registry.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	5	Advanced CI/CD Concepts
-------------------------------------------------------------------------------------------------------------------------

Advanced CI/CD (Continuous Integration and Continuous Deployment/Delivery) concepts  beyond the basics and involve implementing more sophisticated strategies, tools, and practices to enhance the efficiency, reliability, and automation of the software development and deployment process. Here are some advanced CI/CD concepts:

    Pipeline Orchestration: Advanced CI/CD pipelines may involve complex orchestration of multiple stages, environments, and parallel or sequential execution of tasks. Orchestration tools like Jenkins Pipeline DSL or declarative YAML pipelines in modern CI/CD platforms facilitate this.

    Infrastructure as Code (IaC): Integrating IaC tools like Terraform or AWS CloudFormation into CI/CD pipelines allows for the automated provisioning and management of infrastructure alongside application deployments.

    Immutable Infrastructure: Instead of updating existing servers, advanced CI/CD embraces the concept of immutable infrastructure, where every change results in a new, replaceable instance. Tools like Packer and Docker help create immutable artifacts.

    Blue-Green Deployments: Advanced CI/CD pipelines can implement blue-green deployments, where two identical environments (blue and green) are maintained, with traffic gradually shifted to the new version (green) to minimize downtime and risk.

    Canary Deployments: Similar to blue-green, canary deployments release new versions to a subset of users or servers, allowing for real-time monitoring of performance and issues before a full rollout.

    Feature Toggles (Feature Flags): Advanced CI/CD leverages feature toggles to enable or disable specific features at runtime. This allows for controlled feature releases and rollbacks if issues arise.

    A/B Testing: Implementing A/B testing in CI/CD pipelines enables the testing of different variations of a feature with a subset of users, helping gather data on user preferences and performance.

    Multi-Environment Management: Managing multiple environments (development, testing, staging, production) efficiently using infrastructure as code and automated deployment strategies.

    Release Orchestration: Advanced CI/CD pipelines may integrate release orchestration tools that manage the entire release process, including approvals, change tracking, and rollbacks.

    Container Orchestration: Integrating container orchestration platforms like Kubernetes or Docker Swarm allows for advanced container management, scaling, and automated updates.

    Security Scanning and Compliance Checks: Advanced CI/CD pipelines incorporate security scanning tools (SAST, DAST, SCA) and compliance checks to ensure code and deployments meet security and regulatory standards.

    Auto-scaling: Automatically scaling infrastructure resources based on demand, which can be achieved with tools like Kubernetes Horizontal Pod Autoscaling or AWS Auto Scaling.

    Self-Healing: Implementing self-healing mechanisms that detect and automatically recover from infrastructure or application failures to maintain high availability.

    Deployment Rollback Strategies: Advanced CI/CD pipelines include robust rollback strategies that can quickly revert to a previous version or configuration in case of issues.

    Chaos Engineering: Introducing chaos engineering practices to proactively inject failures and test system resilience, helping identify and address weaknesses in the infrastructure and applications.

    Monitoring and Observability: Integrating advanced monitoring and observability tools to gain deep insights into application performance and troubleshoot issues quickly.

    Machine Learning and AI: Leveraging machine learning and AI in CI/CD for predictive analytics, anomaly detection, and automated decision-making.

    Serverless Deployments: Implementing serverless architectures and deploying functions as a service (FaaS) as part of CI/CD pipelines.

    Compliance as Code: Using compliance as code tools to automate the verification and enforcement of compliance requirements within CI/CD pipelines.

    Dark Launching: Gradually releasing new features to a subset of users in a controlled manner using feature flags and monitoring.

Advanced CI/CD concepts are tailored to organizations with complex deployment needs, larger development teams, and a focus on achieving high levels of automation, reliability, and agility in software development and deployment processes.

-------------------------------------------------------------------------------------------------------------------------
				
		Deep dive into GitHub Actions.
-------------------------------------------------------------------------------------------------------------------------

GitHub Actions is a powerful and versatile automation platform provided by GitHub for building, testing, and deploying software projects. It enables developers to define custom workflows, automate various tasks, and integrate CI/CD (Continuous Integration/Continuous Deployment) directly within their GitHub repositories. Here's a deep dive into GitHub Actions:

Key Concepts and Components:

    Workflow: A GitHub Actions workflow is a customizable set of automation steps defined in a YAML file. Workflows are triggered by events like code pushes, pull requests, or custom events.

    Jobs: Each workflow consists of one or more jobs. Jobs are individual units of work and can run concurrently on different runners (virtual machines).

    Runners: Runners are the execution environments where jobs run. GitHub provides hosted runners for various platforms (Linux, macOS, Windows), and you can also set up self-hosted runners on your own infrastructure.

    Actions: Actions are reusable, shareable units of code that encapsulate a specific task or action. You can use built-in actions from the GitHub Marketplace or create your own custom actions.

    Events: Events trigger workflows. Common events include pushes to branches, pull requests, issue comments, and scheduled events. You can also create custom webhook events.

    Workflow File (YAML): Workflows are defined in YAML files within the .github/workflows directory of your repository. These files specify the workflow's name, on which events it triggers, and the jobs to execute.

Workflow Syntax:

Here's a simplified example of a GitHub Actions workflow:

yaml

name: CI/CD Pipeline

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Build and Test
      run: |
        # Build and test your code here
        npm install
        npm test

    - name: Deploy
      run: |
        # Deploy your application here
        ./deploy.sh

In this example:

    The workflow is triggered on each push to the main branch.
    It has a single job named build, which runs on an ubuntu-latest runner.
    The steps within the job include checking out the code, building, testing, and deploying.

GitHub Actions Use Cases:

GitHub Actions can be used for a wide range of automation tasks, including but not limited to:

    Continuous Integration (CI): Automatically build, test, and validate code changes on every push to ensure code quality.

    Continuous Deployment (CD): Automatically deploy applications to various environments (e.g., staging, production) after successful CI.

    Scheduled Tasks: Execute tasks on a schedule, such as database backups, cleanup jobs, or periodic reports.

    Issue and Pull Request Automation: Automate actions like labeling issues, assigning reviewers, or notifying team members when pull requests are created.

    Dependency Management: Automatically update dependencies and run tests to detect compatibility issues.

    Security Scanning: Run security scans and vulnerability checks on code and dependencies.

    Release Management: Automate the process of creating releases, generating changelogs, and notifying users.

    Documentation Generation: Automatically generate documentation from code comments or other sources.

GitHub Actions Benefits:

    Native Integration: GitHub Actions is tightly integrated with GitHub repositories, making it easy to set up and manage CI/CD pipelines.
    Customization: Workflows can be customized to meet specific project requirements using YAML configuration.
    Extensibility: You can use a wide range of built-in actions, create custom actions, or leverage actions from the GitHub Marketplace.
    Parallel Execution: Multiple jobs within a workflow can run in parallel, saving time and resources.
    Secrets Management: GitHub provides a secure way to store and use secrets (e.g., API keys, access tokens) in workflows.
    Community and Ecosystem: Benefit from a growing community of users and a rich ecosystem of actions and integrations.

GitHub Actions simplifies and automates many aspects of the software development lifecycle, helping teams build, test, and deploy applications more efficiently and reliably. It's a versatile platform suitable for projects of all sizes and complexity levels.

-------------------------------------------------------------------------------------------------------------------------
				
		Integrating security, testing, and deployment into CI/CD.
-------------------------------------------------------------------------------------------------------------------------

Integrating security, testing, and deployment into your CI/CD (Continuous Integration and Continuous Deployment/Delivery) pipeline is crucial for delivering secure, high-quality software efficiently. Below, I'll outline the steps to integrate these elements effectively:

1. Security Integration:

    Static Application Security Testing (SAST): Incorporate SAST tools (e.g., Checkmarx, SonarQube) into your CI/CD pipeline. Scan your source code for security vulnerabilities during the build process. Fail the build if critical vulnerabilities are detected.

    Dynamic Application Security Testing (DAST): Use DAST tools (e.g., OWASP ZAP, Burp Suite) to scan your running application for security vulnerabilities. This can be part of a pre-deployment or post-deployment step.

    Software Composition Analysis (SCA): Integrate SCA tools (e.g., OWASP Dependency-Check) to identify vulnerabilities in third-party libraries and components. This is especially important for open-source dependencies.

    Secrets Management: Safely manage secrets like API keys, passwords, and certificates using tools like HashiCorp Vault or Git-crypt. Never store secrets in your version control system.

    Vulnerability Scanning: Regularly scan your infrastructure and containers for known vulnerabilities using tools like Clair for containers or Nessus for infrastructure.

2. Testing Integration:

    Unit Testing: Run unit tests as part of your CI pipeline. Use testing frameworks relevant to your programming language (e.g., JUnit for Java, Pytest for Python).

    Integration Testing: Automate integration tests to ensure that different components of your application work together correctly.

    End-to-End (E2E) Testing: Implement E2E tests using tools like Selenium or Cypress to validate the functionality of your application from a user's perspective.

    Performance Testing: Incorporate performance testing tools like Apache JMeter or Gatling to assess your application's scalability and responsiveness.

    Code Quality Analysis: Utilize code quality analysis tools (e.g., SonarQube) to maintain code quality standards and enforce coding best practices.

    Test Automation: Automate your test suites to run as part of the CI/CD pipeline for quick feedback and validation.

3. Deployment Integration:

    Infrastructure as Code (IaC): Define your infrastructure using IaC tools like Terraform or AWS CloudFormation to ensure consistent and repeatable deployments.

    Deployment Automation: Use deployment automation tools like Ansible, Puppet, or Chef to automate the configuration of servers and services.

    Containerization: Package your application and its dependencies into containers (e.g., Docker) to ensure consistent deployment across different environments.

    Container Orchestration: If using containers, consider container orchestration platforms like Kubernetes for managing containerized applications in production.

    Blue-Green Deployments: Implement blue-green or canary deployments to minimize downtime and reduce the risk of introducing issues into the production environment.

    Rollback Strategy: Develop a robust rollback strategy to quickly revert to a previous version in case of deployment failures or issues in production.

    Continuous Monitoring: Set up monitoring and observability tools (e.g., Prometheus, ELK Stack) to continuously monitor your application's performance and health in production.

4. Continuous Improvement:

    Feedback Loop: Establish a feedback loop by collecting and analyzing data from production, tests, and security scans to identify areas for improvement.

    Incident Response: Have an incident response plan in place to address security incidents or production issues promptly.

    Post-Mortems: Conduct post-mortems or retrospectives after incidents or failures to learn from them and prevent future occurrences.

    Iterative Development: Embrace an iterative development approach, where you continuously refine your CI/CD pipeline and processes based on lessons learned and changing requirements.

By integrating security, testing, and deployment into your CI/CD pipeline and following best practices, you can ensure that your software is both secure and of high quality while also achieving efficient and reliable deployments. This integration helps you catch issues early in the development process and ensures that you can deliver features to production with confidence.

-------------------------------------------------------------------------------------------------------------------------
				
		Hands-on: Setting up a comprehensive CI/CD pipeline using GitHub Actions.
-------------------------------------------------------------------------------------------------------------------------


Setting up a comprehensive CI/CD pipeline using GitHub Actions involves several steps, including building, testing, and deploying your application. Here's a hands-on guide to get you started with a basic example:

Prerequisites:

    A GitHub repository with your application code.
    A GitHub account with appropriate permissions to create and modify GitHub Actions workflows.

Step 1: Create a GitHub Actions Workflow File

In your GitHub repository, create a directory named .github/workflows. This is where you will store your workflow files.

Inside the workflows directory, create a YAML file (e.g., ci-cd.yml) to define your CI/CD pipeline. Here's a simple example that covers building and testing a Node.js application and deploying it to a web server:

yaml

name: CI/CD Pipeline

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Setup Node.js
        uses: actions/setup-node@v2
        with:
          node-version: 14

      - name: Install dependencies
        run: npm install

      - name: Run tests
        run: npm test

  deploy:
    needs: build
    runs-on: ubuntu-latest

    steps:
      - name: Deploy to server
        run: |
          ssh user@your-server 'cd /path/to/your/app && git pull && npm install && pm2 restart your-app'
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}

In this example:

    The workflow is triggered on every push to the main branch.
    It has two jobs: build and deploy. The deploy job depends on the successful completion of the build job.
    The build job checks out the code, sets up Node.js, installs dependencies, and runs tests.
    The deploy job deploys the code to a server using SSH, assuming you have set up the necessary environment variables for SSH.

Step 2: Set Up Secrets

To securely store sensitive information like SSH keys or API tokens, you can use GitHub Secrets. In your repository,  to Settings > Secrets, and add a secret called SSH_PRIVATE_KEY containing your SSH private key for deployment.

Step 3: Push Changes

Commit the .github/workflows/ci-cd.yml file to your repository and push it to the main branch. This will trigger the CI/CD pipeline.

Step 4: Monitor Workflow Runs

 to the "Actions" tab in your GitHub repository to monitor the progress of your workflow runs. You can view logs and details for each step.

Step 5: Customize and Expand

Customize the workflow file to suit your specific application and deployment needs. You can add more jobs, integrate additional testing tools, implement deployment strategies like blue-green or canary deployments, and set up notifications or alerts.

This hands-on guide provides a basic example of setting up a CI/CD pipeline using GitHub Actions. In practice, the complexity of your pipeline will depend on your application and infrastructure requirements. As your project evolves, you can continually refine and expand your CI/CD setup to meet your specific needs.

-------------------------------------------------------------------------------------------------------------------------
				
Week 4		Hands-on Project & Recap
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	1 - 3	Project
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Participants will work on an end-to-end project that encompasses all the concepts learned.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Setting up cloud infrastructure, building a sample application, automating tests, setting up CI/CD, monitoring
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	4	Recap & Review
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Review of key concepts learned throughout the month.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Q&A session, addressing any doubts or clarifications.
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
	5	Feedback & Fixing gaps
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		Gathering feedback from participants
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
				
		
---------------------------

What is load balancing?
	• Load Balances 
		servers that forward traffic to multiple
			servers (e.g., EC2 instances) downstream		
Why use a load balancer?
	• Spread load across multiple downstream instances
	• Expose a single point of access (DNS) to your application
	• Seamlessly handle failures of downstream instances
	• Do regular health checks to your instances
	• Provide SSL termination (HTTPS) for your websites
	• Enforce stickiness with cookies
	• High availability across zones
	• Separate public traffic from private traffic	
	
	
Why use an Elastic Load Balancer?
	• An Elastic Load Balancer is a managed load balancer
	• Highly available
	• AWS takes care of 
		upgrades, 
		maintenance, 
		high availability
	• AWS provides only a few configuration knobs
	• More costly
	• It costs less to setup your own load balancer 
		but it will be a lot more effort on your end
	• Integrated with many AWS offerings / services
		• EC2, 
			EC2 Auto Scaling Groups, 
			Amazon ECS
		• AWS Certificate Manager (ACM), 
			CloudWatch
		• Route 53, 
			AWS WAF, 
			AWS Global Accelerator	
			
			
Health Checks
	• Health Checks are crucial for Load Balancers
	• They enable the load balancer to know if instances it forwards traffic to	are available to reply to requests
	• The health check is done on a port and a route (/health is common)
	• If the response is not 200 (OK), then the instance is unhealthy		
	
	
Types of load balancer on AWS
	• AWS has 4 kinds of managed Load Balancers
		• Classic Load Balancer (v1 - old generation) – 2009 – CLB
			• HTTP, HTTPS, TCP, SSL (secure TCP)
		• Application Load Balancer (v2 - new generation) – 2016 – ALB
			• HTTP, HTTPS, WebSocket
		• Network Load Balancer (v2 - new generation) – 2017 – NLB
			• TCP, TLS (secure TCP), UDP
		• Gateway Load Balancer – 2020 – GWLB
			• Operates at layer 3 (Network layer) – IP Protocol
	• recommended to use the newer generation load balancers as they provide more features
	• Some load balancers can be setup as 
		internal (private) or 
		external (public) ELBs	
		
Classic Load Balancers (v1)
	Client CLB EC2
	listener internal
	• Supports TCP (Layer 4), HTTP &
	HTTPS (Layer 7)
	• Health checks are TCP or HTTP
	based
	• Fixed hostname
	XXX.region.elb.amazonaws.com

Application Load Balancer (v2)
	• Application load balancers is Layer 7 (HTTP)
	• Load balancing to multiple HTTP applications across machines
	(target groups)
	• Load balancing to multiple applications on the same machine
	(ex: containers)
	• Support for HTTP/2 and WebSocket
	• Support redirects (from HTTP to HTTPS for example)		
	
	
Application Load Balancer (v2)
	Path based Query Strings/Parameters Routing
	
	• Routing tables to different target groups:
	• Routing based on path in URL (example.com/users & example.com/posts)
	• Routing based on hostname in URL (one.example.com & other.example.com)
	• Routing based on Query String, Headers
	(example.com/users?id=123&order=false)
	• ALB are a great fit for micro services & container-based application
	(example: Docker & Amazon ECS)
	• Has a port mapping feature to redirect to a dynamic port in ECS
	• In comparison, we’d need multiple Classic Load Balancer per application
	• Fixed hostname (XXX.region.elb.amazonaws.com)
	• The application servers don’t see the IP of the client directly
	• The true IP of the client is inserted in the header X-Forwarded-For
	• We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)


Application Load Balancer (v2)	Target Groups

	• EC2 instances (can be managed by an Auto Scaling Group) – HTTP
	• ECS tasks (managed by ECS itself) – HTTP
	• Lambda functions – HTTP request is translated into a JSON event
	• IP Addresses – must be private IPs
	• ALB can route to multiple target groups
	• Health checks are at the target group level

	
	
Network Load Balancer (v2)
	• Network load balancers (Layer 4) allow to:
	• Forward TCP & UDP traffic to your instances
	• Handle millions of request per seconds
	• Less latency ~100 ms (vs 400 ms for ALB)
	• NLB has one static IP per AZ, and supports assigning Elastic IP
	(helpful for whitelisting specific IP)
	• NLB are used for extreme performance, TCP or UDP traffic
	• Not included in the AWS free tier	
	
Network Load Balancer (v2)
	• Network load balancers (Layer 4) allow to:
	• Forward TCP & UDP traffic to your instances
	• Handle millions of request per seconds
	• Less latency ~100 ms (vs 400 ms for ALB)
	• NLB has one static IP per AZ, and supports assigning Elastic IP
	(helpful for whitelisting specific IP)
	• NLB are used for extreme performance, TCP or UDP traffic


Sticky Sessions (Session Affinity)
	• It is possible to implement stickiness so that the
	same client is always redirected to the same
	instance behind a load balancer
	• This works for Classic Load Balancers &
	Application Load Balancers
	• The “cookie” used for stickiness has an
	expiration date you control	
	

Auto Scaling Group Attributes
	• A Launch Template (older “Launch Configurations” are deprecated)
	• AMI + Instance Type
	• EC2 User Data
	• EBS Volumes
	• Security Groups
	• SSH Key Pair
	• IAM Roles for your EC2 Instances
	• Network + Subnets Information
	• Load Balancer Information
	• Min Size / Max Size / Initial Capacity
	• Scaling Policies


	